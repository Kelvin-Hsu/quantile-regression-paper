\documentclass[twoside]{article} \usepackage{aistats2017}

% If your paper is accepted, change the options for the package
% aistats2017 as follows:
%
%\usepackage[accepted]{aistats2017}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathbbol}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{vector}
\usepackage{color}
\usepackage{cleveref}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{identity}[theorem]{Identity}

%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]
%\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}[section]
%\theoremstyle{definition}
%\newtheorem{lemma}{Lemma}[section]
%\theoremstyle{remark}
%\newtheorem*{remark}{Remark}

\newcommand{\eq}[1]{\begin{equation} \begin{aligned} #1 \end{aligned} \end{equation}}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
%\newcommand{\rv}[1]{\underline{\overline{#1}}}
\newcommand{\rv}[1]{\underline{#1}}
\newcommand{\ds}[1]{{#1}}
\newcommand{\warn}[1]{{\color{red} #1}}

\newcommand{\expect}[1]{{\mathbb{E}[#1]}}
\newcommand{\inner}[2]{{\langle #1, #2 \rangle}}

\newcommand{\Hk}{\mathcal{H}_{k}}
\newcommand{\Hl}{\mathcal{H}_{l}}
\newcommand{\muX}{\mu_{\rv{X}}}
\newcommand{\muY}{\mu_{\rv{Y}}}
\newcommand{\phiX}{\phi_{\rv{X}}}
\newcommand{\psiY}{\psi_{\rv{Y}}}
\newcommand{\Cxy}{C_{\rv{X} \rv{Y}}}
\newcommand{\Cyx}{C_{\rv{Y} \rv{X}}}
\newcommand{\Cxx}{C_{\rv{X} \rv{X}}}
\newcommand{\Cyy}{C_{\rv{Y} \rv{Y}}}
\newcommand{\Cylx}{C_{\rv{Y} | \rv{X}}}
\newcommand{\Cxly}{C_{\rv{X} | \rv{Y}}}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Quantile Regression with Kernel Embeddings : \\ Supplementary Material}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}

\end{abstract}

\section{Notation}

	\textbf{Vectors.} The only quantities that are bold in notation are vectors. Scalars, matrices, and functions are not denoted in bold. Keep in mind that this convention is chosen even after considering the fact that functions operate as uncountably infinite dimensional vectors in many settings within this paper.
	
	A vector $\bvec{v}$ with components $v_{1}, \dots, v_{n}$ can be written in multiple ways.
	
	\begin{equation}
		\bvec{v} = {\begin{bmatrix} v_{1} & \dots & v_{n} \end{bmatrix}}^{T} = \begin{bmatrix} v_{1} \\ \vdots \\ v_{n} \end{bmatrix} = \{v_{i}\}_{i = 1}^{n}
	\end{equation}
	
	\textbf{Indices.} The $j$-th element of a vector $\bvec{x}$ is denoted as $x_{j}$. The $(i, j)$-th element of a matrix $X$ is denoted as $x_{i, j}$.
	
	\textbf{Random Variables and Vectors.} A random variable (RV) is denoted with an upper case letter with an underline, as in $\rv{X}$. A random vector (also RV) is then denoted in bold, as in $\bvec{\rv{X}}$. Semantically, while random variables and vectors are so named, they are formally a mapping from the sample space $\Omega$ to some other Borel measurable space, such as $\mathcal{X} = \mathbb{R}$ or $\mathcal{X} = \mathbb{R}^{d}$. This is where the notations in probability theory and statistics differ slightly from those in measure theory. In general, while the notations in measure theory are more precise (making clear that RVs are mappings), the notations in probability theory and statistics are easier to read and interpret (treating RVs as what it sounds like - random variables and vectors).
	
	For events, a common shorthand used in both probability theory and measure theory is
	
	\begin{equation}
		\{\rv{X} \in A\} := \rv{X}^{-1}[A] := \{\omega \in \Omega : \rv{X}(\omega) \in A\} \in \mathcal{W}
	\end{equation}
	
	\textbf{Measures, Functions, and Functionals.} While both measures and regular functions are mappings, this paper makes a distinction between mappings whose domain is a $\sigma$-algebra (measures) and mappings whose domain is not (other functions). This distinction is emphasised by the notation of how a measure $\nu$ or a regular function $f$ takes its arguments. A measure takes its arguments with square brackets, as in $\nu[\cdot]$. A regular function takes its arguments with round brackets, as in $f(\cdot)$. For functionals $\mathbb{F}$, or mappings that take regular functions as an input, square brackets are also used, such as $\mathbb{F}[\cdot]$.
	
	Typical examples of measures are the probability measure $\mathbb{P} : \mathcal{W}_{\Omega} \mapsto [0, 1]$ on the sample space $\Omega$ and the Lebesgue measure $m_{N} : \mathcal{A}_{\mathbb{R}^{N}} \mapsto [0, \infty)$ on Euclidean space $\mathbb{R}^{N}$, where $\mathcal{W}_{\Omega}$ and $\mathcal{A}_{\mathbb{R}^{N}}$ are $\sigma$-algebras of their respective subscripts.
	
	Typical examples of regular functions involve functions $f : \mathcal{X} \mapsto \mathbb{R}$ that form a Hilbert space (HS) or reproducing kernel Hilbert space (RKHS) and random variables $\rv{X} : \Omega \mapsto \mathcal{X}$.
	
	Typical examples of functionals are the expectance operator $\mathbb{E}$ and variance operator $\mathbb{V}$ that take in a function or function composition whose domain is a $\sigma$-algebra of the sample space $\Omega$. For example, it takes in a function that maps from $\Omega$ to $\mathbb{R}$ (for instance, a random variable), and outputs a real number in $\mathbb{R}$ (see also the next point). An inner product on an uncountably infinite dimensional Hilbert space $\langle \cdot, \cdot, \rangle$ is a functional of two arguments, recalling that a function can be seen as an uncountably infinite dimensional vector. A Lebesgue integral $\int_{B} \cdot d\nu$ (as opposed to a Riemann integral) with respect to a measure $\nu$ on some set $B$ is also a functional.
	
	\textbf{Expectance and Variance Operators.} For a random variable $\rv{X} : \Omega \mapsto \mathcal{X}$ and a function $f : \mathcal{X} \mapsto \mathbb{R}$. The notation in probability theory and statistics for the expectance of a function of a random variable would be $\mathbb{E}[f(\rv{X})]$, as if $\rv{X}$ is a variable and not a function, while in measure theory the more precise notation is $\mathbb{E}[f \circ \rv{X}]$, as $\mathbb{E}$ is a functional and should therefore take a function as the input. In practice, $\mathbb{E}[f(\rv{X})]$ becomes a shorthand for $\mathbb{E}[f \circ \rv{X}]$, and thus the former is used more often.
	
	\textbf{Indicator Measures and Functions.} Another example of a measure is the indicator measure $\mathbb{1} : \mathcal{W} \mapsto \{0, 1\}$. Using the definition $\mathbb{1}_{A}(x) := \mathbb{1}[x \in A]$, the indicator function is then a regular function $\mathbb{1}_{A} : \mathcal{X} \mapsto \{0, 1\}$. Note that the latter is an abuse of notation since the shorthand for $x \in A$ is only defined if $x$ is a random variable (and thus a mapping). Formally, we define the indicator function as below \eqref{eq:indicator_function}.
	
	\begin{equation}
		\mathbb{1}_{A}(x) := \begin{cases}
			1 & : x \in A \\
			0 & : x \notin A
		\end{cases}
	\label{eq:indicator_function}
	\end{equation}
	
	\textbf{Datasets.} A dataset consisting of $n$ observations of $d$-dimensional vector quantities will be denoted using an upper case, such as $\ds{X}$, as it is also a matrix of size $n \times d$ so that $\ds{X} \in \mathbb{R}^{n \times d}$. We call the dimension from $1$ to $n$ the observation dimension, and the dimension from $1$ to $d$ the covariate dimension, noting that the covariates themselves are not always the same as the features. As such, we have $\ds{X} = \{x_{i, j}\}_{i = 1, j = 1}^{n, d}$, where $x_{i, j}$ is the $j$-th covariate of the $i$-th observation. We also denote those observations equivalently by

	\begin{equation}
	\bvec{x}_{i} = \{x_{i, j}\}_{j = 1}^{d} \equiv \{x_{i, 1}, \dots, x_{i, d}\} \equiv \begin{bmatrix}
	x_{i, 1} & \cdots & x_{i, d}
	\end{bmatrix}^{T}
	\label{eq:data_vector_notation}
	\end{equation}
	
	for all $i \in \{1, \dots, n\}$, and thus

	\begin{equation}
		\ds{X} = \{\bvec{x}_{i}\}_{i = 1}^{n} \equiv \{\bvec{x}_{1}, \dots, \bvec{x}_{n}\} \equiv \begin{bmatrix}
			\bvec{x}_{1} & \cdots & \bvec{x}_{n}
		\end{bmatrix}^{T}
	\label{eq:data_matrix_notation}
	\end{equation}
		
	Since $\{ \cdot \}$ stacks the components in the first axis, stacking $d$ scalars would turn it into a $\mathbb{R}^{d}$ vector \eqref{eq:data_vector_notation}, while stacking $n$ $\mathbb{R}^{d}$ vectors will turn it into a $\mathbb{R}^{n \times d}$ matrix \eqref{eq:data_matrix_notation}.
	
	\textbf{Vectorisation.} For a function $f : \mathcal{X} \mapsto \mathbb{R}$ for some Borel measurable space $\mathcal{X}$. Suppose $x_{i} \in \mathcal{X}$, $i \in \{1, \dots, n\}$, and the function $f$ is to be applied on each of those elements. Suppose further, as \eqref{eq:data_matrix_notation} we denote $X = \{x_{i}\}_{i = 1}^{n}$. Note that here we do not bold the elements $x_{i}$ as they may or may not be vectors, or even elements of a Euclidean space, in order to illustrate the general case. We define the following shorthand notation for the vector containing $f(x_{1}), \dots, f(x_{n})$. We choose the curly brackets \eqref{eq:function_vectorisation} to remind readers of the shorthand notation for the vector $\{f(x_{i})\}_{i = 1}^{n}$.
	
	\begin{equation}
		f\{\ds{X}\} := \{f(x_{i})\}_{i = 1}^{n} = {\begin{bmatrix} f(x_{1}) & \dots & f(x_{n}) \end{bmatrix}}^{T}
	\label{eq:function_vectorisation}
	\end{equation}
	
	\textbf{Kernel Embeddings.} In the kernel embeddings setting, the partially applied kernel function, $k(x, \cdot)$, acts like a feature vector for the RKHS it spans over $x \in \mathcal{X}$. However, instead of an usual finite dimensional vector, this is an (usually uncountably, depending on the cardinality of $\mathcal{X}$) infinite dimensional vector as it is a function of one variable in $\mathcal{X}$. In many literature, the notation $\phi(x) := k(x, \cdot)$ is used. However, the author prefers $\phi_{x} := k(x, \cdot)$ so that it is clearer that $\phi_{x}$ is a function of one variable such that $\phi_{x}(x') = k(x, x')$. It would be awkward to write $\phi(x)(x')$ otherwise. Also, as an infinite dimensional vector, when an appropriate Hilbert-Schmidt operator comes, say $C_{\rv{Y} | \rv{X}}$, it would look less awkward if we write $C_{\rv{Y} | \rv{X}} \phi_{x}$ as in $\phi_{x}$ is an element of some vector space where linear operations can be applied onto, instead of $C_{\rv{Y} | \rv{X}} \phi(x)$ where it looks like this is $C_{\rv{Y} | \rv{X}}$ multiples of some functional value $\phi(x)$.
	
	In this way, we emphasise that $\phi_{x}$ is akin to an infinite dimensional vector, or a matrix with size $\infty \times 1$. We can then write tensor products and inner products with matrix notation as usual.
	
\section{Kernel Embeddings}

	As always, let $\rv{X} : \Omega \mapsto \mathcal{X}$ be a random variable taking values in $\mathcal{X}$. Similarly, let $\rv{Y} : \Omega \mapsto \mathcal{Y}$ be a random variable taking values in $\mathcal{Y}$. Let $k$ and $l$ be characteristic positive definite kernels defined on $\mathcal{X}$ and $\mathcal{Y}$ respectively. Let $f \in \Hk$ and $g \in \Hl$.
		

	\begin{theorem} \label{thm:feature_functions}
		(Feature Functions)
		The feature functions that form the basis for the full RKHS are obtained by partially applying the respective kernel of that RKHS. \eqref{eq:feature_functions}.
		\begin{equation}
		\begin{aligned}
			\phi_{x} &:= k(x, \cdot) \in \Hk \\
			\psi_{y} &:= l(y, \cdot) \in \Hl
		\label{eq:feature_functions}
		\end{aligned}
		\end{equation}
	\end{theorem}

	\begin{theorem} \label{thm:reproducing_property}
		(Reproducing Property)
		The evaluation operator of a function within the full RKHS is the inner product with the feature function in that RKHS \eqref{eq:reproducing_property}.
		\begin{equation}
		\begin{aligned}
			\inner{\phi_{x}}{f} &= f(x) \\
			\inner{\psi_{y}}{g} &= g(y)
		\label{eq:reproducing_property}
		\end{aligned}
		\end{equation}	
	\end{theorem}

	\begin{definition} \label{def:kernel_embedding}
		(Kernel Embedding)
		The kernel embedding (or mean embedding) of a distribution $\mathbb{P}_{\rv{X}}$ is defined as the expectation of the corresponding feature function in that RKHS \eqref{eq:kernel_embedding}.
		
		\begin{equation}
		\begin{aligned}
			\muX &:= \expect{\phiX} = \expect{k(\rv{X}, \cdot)} \in \Hk \\
			\muY &:= \expect{\psiY} = \expect{l(\rv{Y}, \cdot)} \in \Hl
		\label{eq:kernel_embedding}
		\end{aligned}
		\end{equation}	
	\end{definition}

	\begin{definition} \label{def:tensor_operator}
		(Tensor Product as an Operator)
		Suppose $a \in \mathcal{H}_{1}$ and $b \in \mathcal{H}_{2}$. Applying the tensor product $(a \otimes b) \in \mathcal{H}_{1} \otimes \mathcal{H}_{2}$ on $c \in \mathcal{H}_{2}$ results in an element in $\mathcal{H}_{1}$ according to \eqref{eq:tensor_operator}. Note that sometimes we may use $\cdot$ to emphasise this operation so that it is not confused with scalar multiplication. Also note that the result of the dot product $\langle b, c \rangle$ is a scalar.
		\begin{equation}
			(a \otimes b) c \equiv (a \otimes b) \cdot c := a \langle b, c \rangle = \langle b, c \rangle a \in \mathcal{H}_{1}
		\label{eq:tensor_operator}
		\end{equation}
	\end{definition}
	
	\begin{definition} \label{def:inner_of_tensor}
		(Inner Products of Tensor Products)
		Suppose $a, c \in \mathcal{H}_{1}$ and $b, d \in \mathcal{H}_{2}$. The inner product of the two tensor products are defined to be the product of the corresponding component inner products \eqref{eq:inner_of_tensor}.
		\begin{equation}
			\inner{a \otimes b}{c \otimes d} := \inner{a}{c} \inner{b}{d}
		\label{eq:inner_of_tensor}
		\end{equation}
	\end{definition}
	
	\begin{definition} \label{def:cross_cov}
		(Uncentred Cross-Covariance Operators)
		The uncentred cross-covariance operators are defined as the kernel embedding of the corresponding tensor-joint RKHS. \eqref{eq:cross_cov}.
		\begin{equation}
		\begin{aligned}
			\Cxx &:= \expect{\phiX \otimes \psiY} \in \Hk \otimes \Hk \\
			\Cxy &:= \expect{\phiX \otimes \psiY} \in \Hk \otimes \Hl \\
			\Cyx &:= \expect{\psiY \otimes \phiX} \in \Hl \otimes \Hk \\
			\Cyy &:= \expect{\psiY \otimes \psiY} \in \Hl \otimes \Hl 
		\label{eq:cross_cov}
		\end{aligned}
		\end{equation}		
	\end{definition}

	\begin{lemma} \label{thm:cxy_g}
		$\Cxy \in \mathcal{H}_{k} \otimes \mathcal{H}_{l}$ is also an operator from $\Hl$ to $\Hk$, such that $\Cxy : \Hl \mapsto \Hk$, with its operation given by the following identity \eqref{eq:cxy_g}.
		\begin{equation}
			\Cxy \cdot g = \expect{g(\rv{Y}) \phiX}
		\label{eq:cxy_g}
		\end{equation}
	
		\begin{proof}
		\begin{align*}
			\Cxy \cdot g &=  (\expect{\phiX \otimes \psiY}) \cdot g \tag{\cref{def:cross_cov}} \\
			&= \expect{(\phiX \otimes \psiY) \cdot g} \tag{linearity of $\expect{\cdot}$} \\
			&= \expect{\phiX \inner{\psiY}{g}} \tag{\cref{def:tensor_operator}} \\
			&= \expect{g(\rv{Y}) \phiX} \tag{\cref{thm:reproducing_property}}
		\end{align*}
		\end{proof}
	\end{lemma}
		
	\begin{lemma} \label{thm:cxy_g_f}
		Being the expectation of a scalar multiple of $\phiX$, this shows that $\Cxy \cdot g \in \mathcal{H}_{k}$, so that the inner product with $f \in \mathcal{H}_{k}$ can also be applied, resulting in the following identity \eqref{eq:cxy_g_f}.
		\begin{equation}
			\inner{\Cxy \cdot g}{f} = \expect{f(\rv{X}) g(\rv{Y})}
		\label{eq:cxy_g_f}
		\end{equation}
		\begin{proof}
		\begin{align*}
			\inner{\Cxy \cdot g}{f} &= \inner{\expect{g(\rv{Y}) \phiX}}{f} \tag{\cref{thm:cxy_g}}\\
			&= \expect{g(\rv{Y}) \inner{\phiX}{f}} \tag{bilinearity of $\inner{\cdot}{\cdot}$} \\
			&= \expect{f(\rv{X}) g(\rv{Y})} \tag{\cref{thm:reproducing_property}}
		\end{align*}
		\end{proof}
	\end{lemma}
		

	\begin{lemma} \label{thm:cxy_f_g}
		Alternatively, since $\Cxy \in \mathcal{H}_{k} \otimes \mathcal{H}_{l}$, the inner product of $\Cxy$ and a function $f \otimes g \in \mathcal{H}_{k} \otimes \mathcal{H}_{l}$ in the same space results in another identity \eqref{eq:cxy_f_g}.
		\begin{equation}
			\inner{\Cxy}{f \otimes g} = \expect{f(X) g(Y)}
		\label{eq:cxy_f_g}
		\end{equation}
		
		\begin{proof}
		\begin{align*}
			\inner{\Cxy}{f \otimes g} &= \inner{\expect{\phiX \otimes \psiY}}{f \otimes g} \tag{\cref{def:cross_cov}} \\
			&= \expect{\inner{\phiX \otimes \psiY}{f \otimes g}} \tag{bilinearity} \\
			&= \expect{\inner{\phiX}{f} \inner{\psiY}{g}} \tag{\cref{def:inner_of_tensor}}\\
			&= \expect{f(\rv{X}) g(\rv{Y})} \tag{\cref{thm:reproducing_property}}
		\end{align*}
		\end{proof}			
	\end{lemma}
	
	\begin{theorem}
		Together, \cref{thm:cxy_g_f} and \cref{thm:cxy_f_g} reveals an important identity for the cross-covariance operator $\Cxy$ \eqref{eq:cxy}.
		
		\begin{equation}
		\begin{aligned}
			 \inner{\Cxy g}{f} &= \inner{\Cxy}{f \otimes g} = \inner{\Cyx f}{g} \\
			 &= \expect{f(\rv{X}) g(\rv{Y})}
			 \label{eq:cxy}			
		\end{aligned}
		\end{equation}
		
		where the second equality is obtained through symmetry. \\
	\end{theorem}

	\begin{theorem} \label{thm:cxx_f_cxy_g}
		Define $f$ in relation to $g$ as follows \eqref{eq:f_def_by_g}.
		\begin{equation}
			f := \expect{g(Y) | X = \cdot}
		\label{eq:f_def_by_g}
		\end{equation}
		
		Then, the following result hold \eqref{eq:cxx_f_cxy_g}.
		\begin{equation}
			\Cxx \cdot f = \Cxy \cdot g
		\label{eq:cxx_f_cxy_g}
		\end{equation}
		
		Further, if the inverse of the operator $\Cxx \equiv \Cxx \cdot$ exists, then $f$ is uniquely defined by \eqref{eq:f_cxxinv_cxy_g}.
		
		\begin{equation}
			f := \expect{g(Y) | X = \cdot} = \Cxx^{-1} \Cxy g
		\label{eq:f_cxxinv_cxy_g}
		\end{equation}

		\begin{proof}
		Notice that $f$ is a function in $\mathcal{H}_{k}$ and the domain of $f$ is $\mathcal{X}$ such that $f(x) = \expect{g(\rv{Y}) | \rv{X} = x}$.
		
		\begin{align*}
			\Cxx \cdot f &= \expect{\phiX \otimes \phiX} \cdot f \tag{\cref{def:cross_cov}} \\
			&= \expect{(\phiX \otimes \phiX) \cdot f} \tag{linearity of $\expect{\cdot}$} \\
			&= \expect{\phiX \inner{\phiX}{f}} \tag{\cref{def:tensor_operator}} \\
			&= \expect{\phiX f(\rv{X})} \tag{\cref{thm:reproducing_property}} \\
			&= \expect{\phiX \expect{g(\rv{Y}) | \rv{X}}} \tag{equation \eqref{eq:f_def_by_g}} \\
			&= \expect{\expect{\phiX g(\rv{Y}) | \rv{X}}} \tag{$\rv{X}$ is given} \\
			&= \expect{\phiX g(\rv{Y})} \tag{tower property} \\
			&= \Cxy \cdot g \tag{\cref{thm:cxy_g}}
		\end{align*}
		
		If the inverse of the operator $\Cxx \equiv \Cxx \cdot$ exists, then we can write the following.
		
		\begin{align*}
			\Cxx f &= \Cxy g \tag{equation \eqref{eq:cxx_f_cxy_g}} \\
			f &= \Cxx^{-1} \Cxy g \tag{$\Cxx^{-1}$ exists} \\
			\expect{g(Y) | X = \cdot} &= \Cxx^{-1} \Cxy g \tag{equation \eqref{eq:f_def_by_g}}
		\end{align*}
		
		\end{proof}
	\end{theorem}
	
	\subsection{Kernel Sum Rule}
	
	\subsection{Kernel Chain Rule}
	
\section{Optimal Approximate Embedding Derivations}

		\begin{equation}
		\begin{aligned}
			\Vert \hat{f} - f \Vert &= \langle \hat{f} - f, \hat{f} - f \rangle \\
			&= \langle \hat{f}, \hat{f} \rangle - 2 \langle \hat{f}, f \rangle + \langle f, f \rangle \\
			&=  \hat{f}^{T} \hat{f} - 2 \hat{f}^{T} f + f^{T} f \\
			&= (\Phi_{\ds{X}} \bm{\alpha})^{T} \Phi_{\ds{X}}  \bm{\alpha} - 2 (\Phi_{\ds{X}}  \bm{\alpha})^{T} f + f^{T} f \\
			&= \bm{\alpha}^{T} \Phi_{\ds{X}} ^{T} \Phi_{\ds{X}}  \bm{\alpha} - 2 \bm{\alpha}^{T} \Phi_{\ds{X}} ^{T} f + f^{T} f \\
			&= \bm{\alpha}^{T} K_{\ds{X} \ds{X}}  \bm{\alpha} - 2 \bm{\alpha}^{T} \bvec{f}_{\ds{X}} + f^{T} f \\
			\end{aligned}
		\label{eq:approximate_embedding_derivation}
		\end{equation}

\end{document}
