\documentclass[twoside]{article} \usepackage{aistats2017}

% If your paper is accepted, change the options for the package
% aistats2017 as follows:
%
%\usepackage[accepted]{aistats2017}
%

\bibliographystyle{apalike}  % Use the "unsrtnat" BibTeX style for formatting the Bibliography
\usepackage[square, authoryear, comma, sort&compress]{natbib}  % Use the "Natbib" style for the references in the Bibliography

% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathbbol}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{vector}
\usepackage[usenames, dvipsnames]{color}
\usepackage{cleveref}
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\eq}[1]{\begin{equation} \begin{aligned} #1 \end{aligned} \end{equation}}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\rv}[1]{{#1}}
\newcommand{\ds}[1]{\tilde{#1}}
\newcommand{\warn}[1]{{\color{red} #1}}
\newcommand{\extra}[1]{{\color{ForestGreen} #1}}
\newcommand{\qpi}{QPI }

\newcommand{\expect}[1]{{\mathbb{E}[#1]}}
\newcommand{\inner}[2]{{\langle #1, #2 \rangle}}

\newcommand{\Hk}{\mathcal{H}_{k}}
\newcommand{\Hl}{\mathcal{H}_{l}}
\newcommand{\muX}{\mu_{\rv{X}}}
\newcommand{\muY}{\mu_{\rv{Y}}}
\newcommand{\muYx}{\mu_{\rv{Y} | \rv{X} = x}}
\newcommand{\muXy}{\mu_{\rv{X} | \rv{Y} = y}}
\newcommand{\phiX}{\phi_{\rv{X}}}
\newcommand{\psiY}{\psi_{\rv{Y}}}
\newcommand{\Cxy}{C_{\rv{X} \rv{Y}}}
\newcommand{\Cyx}{C_{\rv{Y} \rv{X}}}
\newcommand{\Cxx}{C_{\rv{X} \rv{X}}}
\newcommand{\Cyy}{C_{\rv{Y} \rv{Y}}}
\newcommand{\Cylx}{C_{\rv{Y} | \rv{X}}}
\newcommand{\Cxly}{C_{\rv{X} | \rv{Y}}}

\newcommand{\hatmuX}{\hat{\mu}_{\rv{X}}}
\newcommand{\hatmuY}{\hat{\mu}_{\rv{Y}}}
\newcommand{\hatmuYx}{\hat{\mu}_{\rv{Y} | \rv{X} = x}}
\newcommand{\hatmuXy}{\hat{\mu}_{\rv{X} | \rv{Y} = y}}
\newcommand{\hatCxy}{\hat{C}_{\rv{X} \rv{Y}}}
\newcommand{\hatCyx}{\hat{C}_{\rv{Y} \rv{X}}}
\newcommand{\hatCxx}{\hat{C}_{\rv{X} \rv{X}}}
\newcommand{\hatCyy}{\hat{C}_{\rv{Y} \rv{Y}}}
\newcommand{\hatCylx}{\hat{C}_{\rv{Y} | \rv{X}}}
\newcommand{\hatCxly}{\hat{C}_{\rv{X} | \rv{Y}}}
\newcommand{\cardX}{\Vert \mathcal{X} \Vert}
\newcommand{\cardY}{\Vert \mathcal{Y} \Vert}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Nonparametric Quantile Regression with Kernel Embeddings: \\ Supplementary Material}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
	
	This document is the supplementary material for the paper on `Nonparametric Quantile Regression with Kernel Embeddings'. Each section in this document provides the supplementary material for the correspondingly named section in the original paper. The main purpose of this document is to provide derivations for the results claimed in the original paper, and to supply the experimental results used to produce its supporting figures. It is not a stand-alone document that is intended to be read by itself. 
	
\end{abstract}

\section{DIRECT QUANTILE REGRESSION}
\label{sec:direct_quantile_regression}

	Direct approaches to quantile regression refer to methods that estimate the cumulative distribution function (CDF) directly, without requiring the probability density function (PDF) to be estimated first as with pre-image approaches. Once the CDF is obtained, the corresponding quantiles are then computed from a root finding procedure. By omitting the need for PDF estimation, direct approaches are faster than pre-image approaches by a factor proportional to the square of the number of observations $n$. However, the computational efficiency comes at a trade off of estimation accuracy.
	
	Below we provide the corresponding derivations for the theoretical results introduced in the corresponding section of the paper.

	\subsection{Optimal Function Projection}
	\label{sec:direct_quantile_regression:optimal_function_projection}

		A function $f \in \mathcal{H}_{k}$ in the full RKHS can be approximated with $\hat{f} \in \mathcal{H}_{k, \ds{X}}$ in the RKHS spanned by the feature functions centered at the observations \eqref{eq:function_projection},
		\begin{equation}
			\hat{f} = \sum_{i = 1}^{n} \alpha_{i} k(\bvec{x}_{i}, \cdot) = \sum_{i = 1}^{n} \alpha_{i} \phi_{\bvec{x}_{i}} = \Phi_{\ds{X}} \bm{\alpha}.
		\label{eq:function_projection}
		\end{equation}
		By tuning the embedding weights $\bm{\alpha} = \{\alpha_{i}\}_{i = 1}^{n}$, the optimal approximation, or projection, can be obtained by minimising the distance between $f$ and $\hat{f}$ as measured by the norm induced by the full RKHS $\mathcal{H}_{k}$ \eqref{eq:function_projection_optimisation},
		\begin{equation}
			\bm{\alpha}^{\star} = \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert.
		\label{eq:function_projection_optimisation}
		\end{equation}

		\begin{theorem} \label{thm:function_projection_solution}
			(Optimal Function Projection)
			The optimisation problem \eqref{eq:function_projection_optimisation} has a closed form solution \eqref{eq:function_projection_solution},
			\begin{equation}
				\begin{aligned}
					\bm{\alpha}^{\star} &= K^{-1} \bvec{f}, \\
					\hat{f} &= \Phi \bm{\alpha}^{\star} = \Phi K^{-1} \bvec{f}.
				\end{aligned}
			\label{eq:function_projection_solution}
			\end{equation}
			where $K \equiv K_{\ds{X} \ds{X}} := \{k(\bvec{x}_{i}, \bvec{x}_{j})\}_{i = 1, j = 1}^{n, n}$ is the gram matrix and $\bvec{f} := f\{\ds{X}\} := \{f(\bvec{x}_{i})\}_{i = 1}^{n}$ is the function evaluated at the observed points.
		\end{theorem}
			
		\begin{proof}
			Since the square is a monotonic function for positive inputs, the objective function can be rewritten as the squared normed difference between the function approximation and the actual function,
			\begin{align*}
				\bm{\alpha}^{\star} &= \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert \tag{\cref{eq:function_projection_optimisation}} \\
				&= \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert^{2} \tag{monotonic transformation}.
			\end{align*}
			Since this is expressed in a positive quadratic form, it is known from quadratic optimisation that this optimisation has one solution that is obtained by setting the objective's gradient to zero \citep{bertsekas1999nonlinear}. There is no need to check for the Hessian to distinguish between minimum and maximum, as the quadratic is convex and not concave. By expanding the norm as an inner product through its definition in the Hilbert space, the objective to be optimised reveals a quadratic form,
			\begin{align*}
				\Vert \hat{f} - f \Vert^{2} &= \langle \hat{f} - f, \hat{f} - f \rangle \tag{Hilbert norm definition} \\
				&= \langle \hat{f}, \hat{f} \rangle - 2 \langle \hat{f}, f \rangle + \langle f, f \rangle \tag{bilinearity of $\inner{\cdot}{\cdot}$} \\
				&=  \hat{f}^{T} \hat{f} - 2 \hat{f}^{T} f + f^{T} f \tag{linear algebra notation} \\
				&= (\Phi_{\ds{X}} \bm{\alpha})^{T} \Phi_{\ds{X}}  \bm{\alpha} - 2 (\Phi_{\ds{X}}  \bm{\alpha})^{T} f + f^{T} f \tag{\cref{eq:function_projection}} \\
				&= \bm{\alpha}^{T} \Phi_{\ds{X}} ^{T} \Phi_{\ds{X}}  \bm{\alpha} - 2 \bm{\alpha}^{T} \Phi_{\ds{X}} ^{T} f + f^{T} f \tag{linear algebra manipulation}\\
				&= \bm{\alpha}^{T} K_{\ds{X} \ds{X}}  \bm{\alpha} - 2 \bm{\alpha}^{T} f\{\ds{X}\} + f^{T} f \tag{reproducing property}.
			\end{align*}
			The quadratic objective is then differentiated,
			\begin{align*}
				\frac{d}{d \bm{\alpha}} \Vert \hat{f} - f \Vert^{2} &= \frac{d}{d \bm{\alpha}} 
				\bigg[ \bm{\alpha}^{T} K_{\ds{X} \ds{X}}  \bm{\alpha} - 2 \bm{\alpha}^{T} f\{\ds{X}\} + f^{T} f \bigg]\\
				&= 2 K_{\ds{X} \ds{X}}  \bm{\alpha} - 2 f\{\ds{X}\}.
			\end{align*}
			Setting the derivative to zero reveals the solution \eqref{eq:function_projection_solution} to the optimisation problem \eqref{eq:function_projection_optimisation},
			\begin{align*}
				\frac{d}{d \bm{\alpha}} \Vert \hat{f} - f \Vert^{2} &= 0 \\
				2 K_{\ds{X} \ds{X}}  \bm{\alpha}^{\star} - 2 f\{\ds{X}\} &= 0 \\
				K_{\ds{X} \ds{X}}  \bm{\alpha}^{\star} = f\{\ds{X}\} \\
				\bm{\alpha}^{\star} = K_{\ds{X} \ds{X}}^{-1} f\{\ds{X}\}.
			\end{align*}
		\end{proof}
		
		In practice, it is possible that the kernel gram matrix has no inverse. In that case, we instead use the regularised inverse of the gram matrix \eqref{eq:regularised_function_projection_solution} so that the problem is well posed \citep{muandet2016kernel}, where $I_{n}$ is the identity matrix with size $n$,
		\begin{equation}
			\bm{\alpha}^{\star} = (K_{\ds{X} \ds{X}} + \lambda^{2} I_{n})^{-1} f\{\ds{X}\}.
		\label{eq:regularised_function_projection_solution}
		\end{equation}
		The regularisation parameter $\lambda$ is treated as a jitter quantity which should diminish as the number of observations increases. As such, it is customary to choose the smallest $\lambda^{2}$ such that the inverse exists, with the ideal situation being $\lambda = 0$ if the gram matrix is already invertible. 

	\subsection{Direct Empirical Distribution}
	\label{sec:direct_quantile_regression:direct_empirical_distribution}

		With \cref{thm:function_projection_solution}, any function $f \in \mathcal{H}_{k}$ in the full RKHS can be projected into the empirical RKHS as $\hat{f} \in \mathcal{H}_{k, \ds{X}}$.
		
		If $\mathbb{1}_{A} \in \mathcal{H}_{k}$, we can let $f = \mathbb{1}_{A}$ to obtain the projection of an indicator $\hat{\mathbb{1}}_{A} = \Phi K^{-1} \mathbb{1}_{A}\{\ds{X}\}$ into $\mathcal{H}_{k, \ds{X}}$ using \cref{thm:function_projection_solution}.

		Recall that expectations of a function in the RKHS can be evaluated as the inner product \eqref{eq:function_expectation} between the kernel embedding and the function \citep{muandet2016kernel},
		\begin{equation}
			\inner{\muX}{f} = \expect{f(\rv{X})}.
		\label{eq:function_expectation}
		\end{equation}
		Hence, motivated by the fact that $\mathbb{P}_{\bvec{\rv{X}}}[A] = \expect{\mathbb{1}_{A}(\bvec{\rv{X}})} = \inner{\mu_{\bvec{\rv{X}}}}{\mathbb{1}_{A}}$, we define the following direct estimate for the distribution and CDF, and derive a closed form expression for that estimate.
		
		\begin{theorem} \label{thm:empirical_distribution_and_cdf}
			(Direct Empirical Distribution)
			Assume that a kernel $k$ is chosen such that the indicator function is in $\mathcal{H}_{k}$. The direct empirical distribution measured on a set is simply the sum of weights on that set \eqref{eq:empirical_distribution},
			\begin{equation}
				\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] := \inner{\hat{\mu}_{\bvec{\rv{X}}}}{ \hat{\mathbb{1}}_{A}} = \sum_{i : \bvec{x}_{i} \in A} \beta_{i}.
			\label{eq:empirical_distribution}
			\end{equation}
			By the definition of the CDF, $P_{\bvec{\rv{X}}}(\bvec{x}) := \mathbb{P}_{\bvec{\rv{X}}}[(-\bm{\infty}, \bvec{x}]]$, the CDF shows a similar form \eqref{eq:empirical_cdf},
			\begin{equation}
				\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) := \hat{\mathbb{P}}_{\bvec{\rv{X}}}[(-\bm{\infty}, \bvec{x}]] = \sum_{i : \bvec{x}_{i} \leq \bvec{x}} \beta_{i}.
			\label{eq:empirical_cdf}
			\end{equation}
			Note that the notation $\bvec{a} \leq \bvec{b}$ implies that $a_{i} \leq b_{i}$ for all $i \in \{1, \dots, d\}$.
		\end{theorem}
			
		\begin{proof}
			Given that the indicator function $\mathbb{1}_{A} \in \mathcal{H}_{k}$ is in the RKHS, we can derive \eqref{eq:empirical_distribution} by projecting the indicator function into the relevant vector subspace $\mathcal{H}_{k, \ds{X}}$ using \cref{thm:function_projection_solution}, and taking the inner product between the empirical embedding and the resulting projection. 

			\begin{align*}
				\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] &= \inner{\hat{\mu}_{\bvec{\rv{X}}}}{ \hat{\mathbb{1}}_{A}} \tag{by construction} \\
				&= \hat{\mu}_{\bvec{\rv{X}}}^{T}\hat{\mathbb{1}}_{A} \tag{linear algebra notation} \\
				&= (\Phi \bm{\beta})^{T} \Phi K^{-1} \mathbb{1}_{A}\{\ds{X}\} \tag{\cref{thm:function_projection_solution}} \\
				&= \bm{\beta}^{T} \Phi^{T} \Phi K^{-1} \mathbb{1}_{A}\{\ds{X}\} \tag{matrix transpose} \\
				&= \bm{\beta}^{T} K K^{-1} \mathbb{1}_{A}\{\ds{X}\} \tag{reproducing property} \\
				&= \bm{\beta}^{T} \mathbb{1}_{A}\{\ds{X}\} \tag{gram matrices cancel out} \\
				&= \sum_{i = 1}^{n} \beta_{i} \mathbb{1}_{A}(\bvec{x}_{i}) \tag{expanding the sum} \\
				&= \sum_{i : \bvec{x}_{i} \in A} \beta_{i} \tag{definition of indicator function}.
			\end{align*}

			The empirical estimate for the cumulative distribution function is then defined according to its theoretical definition, resulting in \eqref{eq:empirical_cdf},

			\begin{align*}
				\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) &= \hat{\mathbb{P}}_{\bvec{\rv{X}}}[(-\bm{\infty}, \bvec{x}]] \tag{definition of CDF} \\
				&= \sum_{i : \bvec{x}_{i} \in (-\bm{\infty}, \bvec{x}]} \beta_{i} \tag{derived result \eqref{eq:empirical_distribution}}\\
				&= \sum_{i : \bvec{x}_{i} \leq \bvec{x}} \beta_{i} \tag{equivalent notation}.
			\end{align*}
		\end{proof}
		
	\subsection{Smooth Empirical Distribution}
	\label{sec:direct_quantile_regression:smooth_empirical_distribution}
		
		For the purpose of distribution estimation, we are interested in taking the inner product with some appropriate representation of the indicator function. If we restrict that representation to be in the space of $\mathcal{H}_{k, \ds{X}}$, then \cref{thm:empirical_distribution_and_cdf} gives the best estimator under this restriction. However, this does not result in a smooth approximation to the distribution. It is then natural to consider the case where the indicator representation is not restricted to $\mathcal{H}_{k, \ds{X}}$, but to the full reproducing kernel Hilbert space $\mathcal{H}_{k}$.

		Now, the norm induced by the reproducing kernel Hilbert space $\mathcal{H}_{k}$ is defined for all functions within that Hilbert space. Therefore, we are not restricted to taking the inner product between the empirical mean embedding and a function in $\mathcal{H}_{k, \ds{X}}$. We can instead take the inner product between the empirical mean embedding and a function in $\mathcal{H}_{k}$.
		
		Motivated by this idea, instead of projecting the indicator function into the subspace $\mathcal{H}_{k, \ds{X}}$, we can instead project the indicator into the full RKHS. Of course, if the indicator function is already in the full RKHS, then this projection operation is simply the identity operation, where the indicator projection is simply the indicator function itself. However, in general, this is not the case, which is also a disadvantage for the estimator given in \cref{thm:empirical_distribution_and_cdf}, and is the major reason for its inferior performance as compared to pre-image methods.
		
		In the following approach, we do not assume that the indicator function is in the full RKHS $\mathcal{H}_{k}$. We instead project it into the full RKHS with respect to the standard Lebesgue measure in Euclidean space.
		
%		Note that indicator functions $\mathbb{1}_{A}$ are simply $1$ on its support $A$, so that it is continuous on its support. However, $A$ may not be compact. However, the space of continuous and compactly supported functions is dense in a Lebesgue measurable space \citep{bauer1981probability}. This means that even if the indicator is not Lebesgue measurable, because Lebesgue spaces are dense, there exists a sequence of Lebesgue measurable functions that converges to the indicator function. 
		
		That is, we take the inner product between the indicator function and the basis functions point-wise using the Lebesgue measure \eqref{eq:indicator_smooth_original},
		\begin{equation}
			\tilde{\mathbb{1}}_{A}(\bvec{x}') := \int_{\mathcal{X}} \mathbb{1}_{A}(\bvec{x}) k(\bvec{x}, \bvec{x}') d\bvec{x}.
		\label{eq:indicator_smooth_original}
		\end{equation}
		This does not require the indicator to be in the full RKHS $\mathcal{H}_{k}$, as we are simply defining an RKHS representation of the indicator function \eqref{eq:indicator_smooth},
		\begin{equation}
			\tilde{\mathbb{1}}_{A} := \int_{\mathcal{X}} \mathbb{1}_{A}(\bvec{x}) k(\bvec{x}, \cdot) d\bvec{x} = \int_{A} k(\bvec{x}, \cdot) d\bvec{x}.
		\label{eq:indicator_smooth}
		\end{equation}
		That is, by construction, $\tilde{\mathbb{1}}_{A} \in \mathcal{H}_{k}$. With this indicator representation, we derive the following smooth distribution and CDF estimate.
		\begin{theorem} \label{thm:smooth_empirical_distribution_and_cdf}
			The smooth distribution estimate \eqref{eq:smooth_empirical_distribution},
			\begin{equation}
				\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] := \inner{\hat{\mu}_{\bvec{\rv{X}}}}{\tilde{\mathbb{1}}_{A}} = \sum_{i = 1}^{n} \beta_{i} \int_{A}  k(\bvec{x}_{i}, \bvec{x}) d\bvec{x},
			\label{eq:smooth_empirical_distribution}
			\end{equation}
			and the smooth CDF estimate \eqref{eq:smooth_empirical_cdf},
			\begin{equation}
				\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) = \sum_{i = 1}^{n} \beta_{i} \int_{-\bm{\infty}}^{\bvec{x}}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' = \bm{\beta}^{T} \bvec{K}(\bvec{x}),
			\label{eq:smooth_empirical_cdf}
			\end{equation}
			can be obtained by integrating the corresponding mean embedding, resulting in a linear combination of kernel integrals. The vector of kernel integrals is notated by
			\begin{equation}
				\bvec{K}(\bvec{x}) := \Bigg\{ \int_{-\bm{\infty}}^{\bvec{x}}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' \Bigg\}_{i = 1}^{n}.
			\label{eq:kernel_integral}
			\end{equation}
		\end{theorem}
			
		\begin{proof}
			As an integral of the feature functions $k(\bvec{x}, \cdot)$, which are the basis functions for the RKHS $\mathcal{H}_{k}$, over some set $A$, the smooth indicator approximation $\tilde{\mathbb{1}}_{A} \in \mathcal{H}_{k}$ is also within the same RKHS. The derivation of \eqref{eq:smooth_empirical_distribution} then follows by similarly taking the inner product between the empirical embedding and this smooth indicator approximation.	
			\begin{align*}
					\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] &= \inner{\hat{\mu}_{\bvec{\rv{X}}}}{\tilde{\mathbb{1}}_{A}} \tag{by construction} \\
					&= \hat{\mu}_{\bvec{\rv{X}}}^{T} \tilde{\mathbb{1}}_{A} \tag{linear algebra notation} \\
					&= (\Phi_{\ds{X}} \bm{\beta})^{T} \int_{A} \phi_{\bvec{x}} d\bvec{x} \tag{\cref{eq:indicator_smooth}} \\
					&= \bm{\beta}^{T} \Phi_{\ds{X}}^{T} \int_{A} \phi_{\bvec{x}} d\bvec{x} \tag{matrix transpose}\\
					&= \int_{A} \bm{\beta}^{T} \Phi_{\ds{X}}^{T} \phi_{\bvec{x}} d\bvec{x} \tag{linearity of integral} \\
					&= \int_{A} \bm{\beta}^{T} K_{\ds{X} \bvec{x}} d\bvec{x} \tag{reproducing property} \\
					&= \int_{A} \sum_{i = 1}^{n} \beta_{i} k(\bvec{x}_{i}, \bvec{x}) d\bvec{x} \tag{expanding the sum} \\
					&= \sum_{i = 1}^{n} \beta_{i} \int_{A}  k(\bvec{x}_{i}, \bvec{x}) d\bvec{x} \tag{linearity of integrals}.
			\end{align*}
			The CDF estimate \eqref{eq:smooth_empirical_cdf} then follows directly from the definition of of a CDF, where we defined \eqref{eq:kernel_integral} for concise notation.
		\end{proof}
		
		While this does not require the indicator function to be in the full RKHS, if it is not, then the projection is not the indicator function itself. This means that the resulting estimate will not always satisfy the basic properties of a CDF. In this case, while $\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) \to 0$ as $\bvec{x} \to -\bm{\infty}$, it is not necessarily true that $\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) \to 1$ as $\bvec{x} \to +\bm{\infty}$. It is also not necessarily true that the CDF estimate is always non-decreasing. However, experiments show that the magnitude of such violations are usually very small. Nevertheless, it can be helpful to scale the CDF estimate such that the condition $\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) \to 1$ as $\bvec{x} \to +\bm{\infty}$ always holds. In the next section, we derive the normalisation constants which would guarantee this property.

	\subsection{Normalisation Constants}
	\label{sec:direct_quantile_regression:normalisation_constants}
	
		Suppose some data $\{(\bvec{y}_{i}, x_{i})\}_{i = 1}^{n}$ is collected from a joint distribution $\mathbb{P}_{\rv{Y} \rv{X}}$. We can either find the standard empirical embedding $\hat{\mu}_{\rv{X}}$ or empirical conditional embedding $\hat{\mu}_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}$, both of which is of the form $\Phi \bm{\beta}$ for some embedding weights $\bm{\beta}$. This is also true for posterior embeddings obtained from kernel Bayes' Rule (KBR) \citep{fukumizu2013kernel}.
		
		Direct quantile regression methods rely on estimating the CDF directly from the dataset, without the need to find the pre-image embedding weights first. While efficient and effective, it does not guarantee a CDF estimate that satisfies the desirable properties discussed in the previous section. However, in the limit of infinite data and vanishing bandwidth \citep{kanagawa2016filtering}, the empirical embedding converges to the true PDF, so that the CDF estimates based on the embedding weights will also converge to the true CDF.
		
		Nevertheless, for a finite number of observations, it can be beneficial to enforce some of these constraints for better numerical performance. In practice, while both the range criteria and non-decreasing criteria can be enforced, it is usually sufficient and sometimes beneficial to only enforce the fact that the CDF must end with $1$ as its inputs tends to infinity. This removes the possibility of having an quantile problem with no solution (for example, there are no inputs such that the CDF is 0.99), although it is possible that quantile problem may have multiple solutions unless the other criteria are enforced too. Rather than being an issue however, this usually indicates a lack of observation at those candidate quantile locations, and therefore can be very useful in active sampling or Bayesian optimisation applications.
		
		Therefore, we choose to retain the shape of the direct CDF estimate, so that the CDF is merely scaled uniformly through a multiplicative constant. We first derive the form of the scaled CDF estimate for general multivariate distribution, then we restrict it to a univariate distribution for defining the quantile estimate.
		
		For the direct CDF estimate \eqref{eq:empirical_cdf} from \cref{thm:empirical_distribution_and_cdf}, we require that the scaled CDF estimate $\tilde{P}_{\bvec{\rv{X}}}(\bvec{x})$ approach $1$ as $\bvec{x} \to \bm{\infty}$, which is a shorthand for $x_{j} \to \infty \; \forall j \in \{1, \cdots, d\}$. To do this, we normalise the CDF estimate $\hat{P}_{\bvec{\rv{X}}}(\bvec{x})$ by a fixed constant $z_{\mathrm{DR}}$ to a new CDF $\tilde{P}_{\bvec{\rv{X}}}(\bvec{x})$ estimate \eqref{eq:modified_empirical_cdf},
		\begin{equation}
			\tilde{P}_{\bvec{\rv{X}}}(\bvec{x}) := \frac{1}{z_{\mathrm{DR}}} \hat{P}_{\bvec{\rv{X}}}(\bvec{x}),
		\label{eq:modified_empirical_cdf}
		\end{equation}
		where the constant is derived as follows \eqref{eq:normalisation_constant_dr},
		\begin{equation}
			\begin{aligned}
				1 &= \lim_{\bvec{x} \to \infty} \tilde{P}_{\bvec{\rv{X}}}(\bvec{x}) \\
				&= \lim_{\bvec{x} \to \infty} \frac{1}{z_{\mathrm{DR}}} \sum_{i : \bvec{x}_{i} \leq \bvec{x}} \beta_{i} \\
				&= \frac{1}{z_{\mathrm{DR}}} \lim_{\bvec{x} \to \infty} \sum_{i : \bvec{x}_{i} \leq \bvec{x}} \beta_{i} \\
				&= \frac{1}{z_{\mathrm{DS}}} \sum_{i = 1}^{n} \beta_{i} \\
				z_{\mathrm{DR}} &= \sum_{i = 1}^{n} \beta_{i}.
			\end{aligned}
		\label{eq:normalisation_constant_dr}
		\end{equation}
		Similarly, for the smooth CDF estimate \eqref{eq:smooth_empirical_cdf} from \cref{thm:smooth_empirical_distribution_and_cdf}, we normalise the CDF estimate $\hat{P}_{\bvec{\rv{X}}}(\bvec{x})$ by a fixed constant $z_{\mathrm{DS}}$ to a new CDF $\tilde{P}_{\bvec{\rv{X}}}(\bvec{x})$ estimate \eqref{eq:modified_smooth_cdf},
		\begin{equation}
			\tilde{P}_{\bvec{\rv{X}}}(\bvec{x}) := \frac{1}{z_{\mathrm{DS}}} \hat{P}_{\bvec{\rv{X}}}(\bvec{x}),
		\label{eq:modified_smooth_cdf}
		\end{equation}
		where the constant is derived as follows \eqref{eq:normalisation_constant_ds},
		\begin{equation}
			\begin{aligned}
				1 &= \lim_{\bvec{x} \to \infty} \tilde{P}_{\bvec{\rv{X}}}(\bvec{x}) \\
				&= \lim_{\bvec{x} \to \infty} \frac{1}{z_{\mathrm{DS}}} \sum_{i = 1}^{n} \beta_{i} \int_{-\bm{\infty}}^{\bvec{x}}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' \\
				&= \frac{1}{z_{\mathrm{DS}}} \lim_{\bvec{x} \to \infty} \sum_{i = 1}^{n} \beta_{i} \int_{-\bm{\infty}}^{\bvec{x}}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' \\
				&= \frac{1}{z_{\mathrm{DS}}} \sum_{i = 1}^{n} \beta_{i} \lim_{\bvec{x} \to \infty} \int_{-\bm{\infty}}^{\bvec{x}}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' \\
				&= \frac{1}{z_{\mathrm{DS}}} \sum_{i = 1}^{n} \beta_{i} \int_{-\bm{\infty}}^{\bm{\infty}}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' \\
				&= \frac{1}{z_{\mathrm{DS}}} \sum_{i = 1}^{n} \beta_{i} c \\
				&= \frac{c}{z_{\mathrm{DS}}} \sum_{i = 1}^{n} \beta_{i} \\
				z_{\mathrm{DR}} &= c \sum_{i = 1}^{n} \beta_{i}.
			\end{aligned}
		\label{eq:normalisation_constant_ds}
		\end{equation}
		
		In this paper, all kernels are assumed to be stationary. Each integral integrates to $c > 0$ \eqref{eq:constant_definition} for stationary kernels, so that $\int_{\mathbb{R}^{d}} k(\bvec{x}^{(1)}, \bvec{x}) d\bvec{x} = \int_{\mathbb{R}^{d}} k(\bvec{x}^{(2)}, \bvec{x}) d\bvec{x}$ for any $\bvec{x}^{(1)}, \bvec{x}^{(2)} \in \mathbb{R}^{d}$,
		\begin{equation}
			c := \int_{\mathbb{R}^{d}} k(\bvec{0}, \bvec{x}) d\bvec{x}.
		\label{eq:constant_definition}
		\end{equation}
		This constant is usually analytically computable from the form of the stationary kernel, such as the Gaussian kernel.
		
		With the normalisation constants, we now focus on obtaining quantiles for a univariate distribution.

		\theoremstyle{definition}
		\begin{definition}
			Motivated by \cref{thm:empirical_distribution_and_cdf}, the \textit{direct embedding $\tau$-quantile estimate} (DR) is
			\begin{equation}
			q_{\rv{X}}(\tau) = \min\{x \in \mathbb{R} : \frac{1}{\sum_{i = 1}^{n} \beta_{i}} \sum_{i : x_{i} \leq x} \beta_{i} \geq \tau\}.
			\end{equation}	
			The normalisation constant $\sum_{i = 1}^{n} \beta_{i}$ ensures that the CDF approaches one as its inputs approaches infinity. The hyperparameter learning is done by minimising the pinball loss of the training data using cross-validation.
		\end{definition}
		
		\theoremstyle{definition}
		\begin{definition}
			Motivated by \cref{thm:smooth_empirical_distribution_and_cdf}, the \textit{smooth embedding $\tau$-quantile estimate} (DS) is
			\begin{equation}
			q_{\rv{X}}(\tau) = x : \frac{1}{\int_{\mathbb{R}} k(0, x) dx \sum_{i = 1}^{n} \beta_{i}} \bm{\beta}^{T} \bvec{K}(x) = \tau.
			\end{equation}	
			Similarly, we apply a normalisation constant $\int_{\mathbb{R}} k(0, x) dx \sum_{i = 1}^{n} \beta_{i}$, and pinball learning is used to learn the hyperparameters.
		\end{definition}
	
\section{PRE-IMAGE QUANTILE REGRESSION}
	\label{sec:pre_image_quantile_regression}

	The \qpi-based method for computing the pre-image density from a kernel embedding can be very computationally expensive.
	Below we introduce the efficient, but less accurate, clip-normalisation technique for density recovery.
	
	\subsection{Clip-Normalise Density Recovery}
	\label{sec:pre_image_quantile_regression:density_recovery}
	
		The pre-image technique involves approximating the density with the same mixture components as the embedding, but with different weights. That is, suppose that the conditional embedding has the form \eqref{eq:general_conditional_embedding},
		\begin{equation}
			\hat{\mu}_{\bvec{X} | \bvec{Y} = y}= \sum_{i = 1}^{n} \beta_{i} k(\bvec{x}_{i}, \cdot),
		\label{eq:general_conditional_embedding}
		\end{equation}		
		where $\bm{\beta}$ is to be computed with the dataset $\ds{Y} := \{\bvec{y}_{i}\}_{i = 1}^{n}$ either through the conditional embedding operator or through kernel Bayes' rule, depending on the application. The pre-imaged PDF would exhibit the form \eqref{eq:general_conditional_density},
		\begin{equation}
		\hat{p}_{\bvec{X} | \bvec{Y}}(\cdot | \bvec{y}) = \sum_{i = 1}^{n} w_{i} k(\bvec{x}_{i}, \cdot),
		\label{eq:general_conditional_density}
		\end{equation}
		where $\bvec{w}$ is obtained from $\bm{\beta}$. For the \qpi method introduced by \cite{mccalman2013multi}, $\bm{\beta}$ is obtained through quadratic programming.
		
		Because $\hat{p}_{\bvec{X} | \bvec{Y}}(\cdot | \bvec{y})$ is a density, it must integrate to one throughout its domain \eqref{eq:normalisation},
		
		\begin{equation}
			\begin{aligned}
				1 &= \int_{\mathbb{R}^{d}} \hat{p}_{\bvec{X} | \bvec{Y}}(\bvec{x} | \bvec{y}) d\bvec{x} \\
				&= \int_{\mathbb{R}^{d}} \sum_{i = 1}^{n} w_{i} k(\bvec{x}_{i}, \bvec{x}) d\bvec{x} \\
				&= \sum_{i = 1}^{n} w_{i} \int_{\mathbb{R}^{d}} k(\bvec{x}_{i}, \bvec{x}) d\bvec{x} \\
				&= c \sum_{i = 1}^{n} w_{i}.
			\end{aligned}
		\label{eq:normalisation}
		\end{equation}

		Computing the density pre-image density weights through quadratic programming requires a separate optimisation loop, increasing the time complexity of the algorithm significantly. At the expense of accuracy, the density weights can be instead obtained through clipping the embedding weights at zero and normalising them such that the resulting weights $\{\tilde{w}_{i}\}_{i = 1}^{n}$ are valid PDF weights.

		\begin{definition} \label{def:clip_normalise}
			(Clip-Normalised Weights and Density Recovery)
			Given the weights $\bm{\beta}$ for a general conditional embedding \eqref{eq:general_conditional_embedding}, the \textit{clip-normalised weights} \eqref{eq:clip_normalise_weights},
			\begin{equation}
				\tilde{w}_{i} = \frac{\max\{\beta_{i}, 0\}}{c \sum_{j = 1}^{n} \max\{\beta_{j}, 0\}},
				\label{eq:clip_normalise_weights}
			\end{equation}
			can be obtained through clipping the embedding weights $\bm{\beta}$ at zero and normalising them so that they sum up to $\frac{1}{c}$ such that the resulting PDF \eqref{eq:clip_normalise_density} is a valid PDF,
			\begin{equation}
				\hat{p}_{\bvec{X} | \bvec{Y}}(\cdot | \bvec{y}) = \sum_{i = 1}^{n} \tilde{w}_{i} k(\bvec{x}_{i}, \cdot).
			\label{eq:clip_normalise_density}
			\end{equation}
		\end{definition}
			
%\section{Computational Complexity}
%\label{sec:computational_complexity}
%
%	\begin{table}[t!]
%		% * <lachlan@mccalman.info> 2016-10-06T00:07:58.096Z:
%		% 
%		% If you want to double check these complexity bounds I'd appreciate it  =)
%		% They make sense to me too
%		% 
%		% ^.
%		\begin{center}
%			\begin{tabular}{l|cccc}
%				Algorithm & S & ND &   NC & Complexity \\ \hline
%				DR  &              &                &                & $O(n \log(n))$    \\
%				DS  & $\checkmark$ &                &                &
%				$O(n \log(n))$  \\
%				NB  & $\checkmark$ & $\checkmark$   &                &
%				$O(n \log(n))$ \\
%				JB  & $\checkmark$ & $\checkmark$   &                &
%				$O(n^{3} \log(n))$ \\
%				NL  & $\checkmark$ & $\checkmark$   & $\checkmark$   &
%				$O(n \log(n))$ \\
%				JL  & $\checkmark$ & $\checkmark$   & $\checkmark$   &   $O(n^{3} \log(n))$ 
%			\end{tabular}
%		\end{center}
%		\caption{\small Comparison of Quantile estimation techniques. S stands for Smooth, ND for Non-Decreasing, NC for Non-Crossing, and $n$ is the number of training points.}
%		\label{table:quantile_regression_methods}
%	\end{table}
%	
%	The cheapest cumulative evaluate is the direct embedding estimator (DR) --- this is simply a (conditional) sum of the mixture weights and is therefore $O(n)$ in the number of training points. The smooth embedding estimator (DS) and the clip-normalised estimator (NL and NB) are similarly inexpensive to compute, once the kernel integral \eqref{eq:kernel_integral} is pre-computed. They are also $O(n)$, but requires pre-computation of the kernel integral. The most expensive are the \qpi-based pre-image estimators (JL and JB), which require solving a quadratic program to determine the mixture weights and therefore introduces a further factor of $O(n^2)$. However, if a pre-image estimate is also required for other tasks and is therefore already available, then JL and JB also only introduces a further factor of $O(n)$.

\section{EXPERIMENTS}
\label{sec:experiments}

	Below we provide a description of our experimental setup for testing and comparing our algorithms against state-of-the-art algorithms in quantile regression.
		\begin{table*}
			\vspace*{5mm}
			\begin{center}
				\resizebox{0.48\textwidth}{!}{
					\begin{tabular}{l|ccc}
						Alg. & $\tau = 0.1$     & $\tau = 0.5$  & $\tau = 0.9$      \\ \hline
						DR & $0.125 \pm 0.033$ & $0.270 \pm 0.063$ & $0.152 \pm 0.022$ \\
						DS & $\mathbf{0.109 \pm 0.022}$ & $0.267 \pm 0.059$ & $0.141 \pm 0.031$ \\
						NB & $0.117 \pm 0.021$ & $0.275 \pm 0.065$ & $0.139 \pm 0.017$ \\
						JB & $0.118 \pm 0.031$ & $0.256 \pm 0.044$ & $0.142 \pm 0.039$ \\
						NL & $0.136 \pm 0.035$ & $0.294 \pm 0.030$ & $0.141 \pm 0.014$ \\
						JL & $0.124 \pm 0.017$ & $0.260 \pm 0.033$ & $0.132 \pm 0.022$ \\
						A & $0.239 \pm 0.105$ & $0.264 \pm 0.050$ & $0.292 \pm 0.087$ \\
						B & $0.123 \pm 0.033$ & $\mathbf{0.249 \pm 0.033}$ & $0.128 \pm 0.018$ \\
						C & $0.122 \pm 0.031$ & $0.266 \pm 0.021$ & $0.131 \pm 0.015$ \\
						D & $0.166 \pm 0.021$ & $0.255 \pm 0.028$ & $\mathbf{0.126 \pm 0.015}$
					\end{tabular}}
					\resizebox{0.48\textwidth}{!}{
						\begin{tabular}{l|ccc}
							Alg. & $\tau = 0.1$     & $\tau = 0.5$  & $\tau = 0.9$      \\ \hline
							DR & $0.169 \pm 0.098$ & $0.353 \pm 0.117$ & $0.086 \pm 0.015$ \\
							DS & $0.106 \pm 0.064$ & $0.169 \pm 0.129$ & $0.073 \pm 0.009$ \\
							NB & $\mathbf{0.062 \pm 0.012}$ & $0.115 \pm 0.010$ & $0.076 \pm 0.016$ \\
							JB & $0.055 \pm 0.008$ & $0.112 \pm 0.020$ & $0.072 \pm 0.009$ \\
							NL & $0.063 \pm 0.008$ & $0.143 \pm 0.029$ & $0.087 \pm 0.016$ \\
							JL & $0.077 \pm 0.012$ & $0.136 \pm 0.034$ & $0.094 \pm 0.016$ \\
							A & $0.291 \pm 0.034$ & $0.293 \pm 0.024$ & $0.301 \pm 0.045$ \\
							B & $0.067 \pm 0.015$ & $0.218 \pm 0.034$ & $0.118 \pm 0.013$ \\
							C & $0.075 \pm 0.011$ & $0.176 \pm 0.028$ & $0.123 \pm 0.011$ \\
							D & $0.057 \pm 0.010$ & $\mathbf{0.097 \pm 0.015}$ & $\mathbf{0.068 \pm 0.017}$
						\end{tabular}}
						\resizebox{0.48\textwidth}{!}{
							\begin{tabular}{l|ccc}
								Alg. & $\tau = 0.1$     & $\tau = 0.5$  & $\tau = 0.9$      \\ \hline
								DR & $0.163 \pm 0.060$ & $0.240 \pm 0.037$ & $0.096 \pm 0.024$ \\
								DS & $0.180 \pm 0.075$ & $0.465 \pm 0.181$ & $0.120 \pm 0.022$ \\
								NB & $0.096 \pm 0.019$ & $0.194 \pm 0.033$ & $0.092 \pm 0.015$ \\
								JB & $0.101 \pm 0.030$ & $0.204 \pm 0.022$ & $0.107 \pm 0.041$ \\
								NL & $0.097 \pm 0.016$ & $0.210 \pm 0.034$ & $0.092 \pm 0.016$ \\
								JL & $0.091 \pm 0.007$ & $0.199 \pm 0.032$ & $0.091 \pm 0.010$ \\
								A & $0.396 \pm 0.080$ & $0.389 \pm 0.019$ & $0.387 \pm 0.056$ \\
								B & $0.090 \pm 0.012$ & $0.202 \pm 0.019$ & $0.085 \pm 0.008$ \\
								C & $0.094 \pm 0.011$ & $0.190 \pm 0.015$ & $0.083 \pm 0.010$ \\
								D & $0.092 \pm 0.025$ & $\mathbf{0.186 \pm 0.018}$ & $0.089 \pm 0.010$ \\
								E & $\mathbf{0.079 \pm 0.019}$ & $0.187 \pm 0.021$ & $\mathbf{0.070 \pm
									0.016}$
							\end{tabular}}
							\resizebox{0.48\textwidth}{!}{
								\begin{tabular}{l|ccc}
									Alg. & $\tau = 0.1$     & $\tau = 0.5$  & $\tau = 0.9$      \\ \hline
									DR & $0.125 \pm 0.008$ & $0.308 \pm 0.013$ & $0.162 \pm 0.028$ \\
									DS & $\mathbf{0.120 \pm 0.007}$ & $0.307 \pm 0.014$ & $0.157 \pm 0.011$ \\
									NB & $0.122 \pm 0.007$ & $\mathbf{0.306 \pm 0.017}$ & $\mathbf{0.151 \pm
										0.014}$ \\
									JB & $0.121 \pm 0.006$ & $0.307 \pm 0.016$ & $0.152 \pm 0.014$ \\
									NL & $0.121 \pm 0.016$ & $0.308 \pm 0.013$ & $0.153 \pm 0.011$ \\
									JL & $0.123 \pm 0.014$ & $\mathbf{0.306 \pm 0.012}$ & $0.153 \pm 0.013$ \\
									A & $0.328 \pm 0.028$ & $0.325 \pm 0.034$ & $0.324 \pm 0.073$ \\
									B & $0.122 \pm 0.017$ & $\mathbf{0.306 \pm 0.039}$ & $0.152 \pm 0.025$ \\
									C & $0.121 \pm 0.020$ & $0.311 \pm 0.041$ & $0.154 \pm 0.027$ \\
									D & $0.135 \pm 0.014$ & $0.310 \pm 0.045$ & $0.168 \pm 0.030$ \\
									E & $0.123 \pm 0.017$ & $0.309 \pm 0.045$ & $0.153 \pm 0.027$
								\end{tabular}}
							\end{center}
							
							\caption{Top: Antigen and Weather results. Bottom: Motorcycle and Bone Mineral Density results.}
							
							\label{table:results}
						\end{table*}
						
						%		\begin{table*}
						%			\begin{center}
						%				\resizebox{0.24\textwidth}{!}{
						%				\begin{tabular}{l|ccc}
						%					Alg. & $\tau = 0.1$     & $\tau = 0.5$  & $\tau = 0.9$      \\ \hline
						%					DR & $0.169 \pm 0.098$ & $0.353 \pm 0.117$ & $0.086 \pm 0.015$ \\
						%					DS & $0.106 \pm 0.064$ & $0.169 \pm 0.129$ & $0.073 \pm 0.009$ \\
						%					NB & $\mathbf{0.062 \pm 0.012}$ & $0.115 \pm 0.010$ & $0.076 \pm 0.016$ \\
						%					JB & $0.055 \pm 0.008$ & $0.112 \pm 0.020$ & $0.072 \pm 0.009$ \\
						%					NL & $0.063 \pm 0.008$ & $0.143 \pm 0.029$ & $0.087 \pm 0.016$ \\
						%					JL & $0.077 \pm 0.012$ & $0.136 \pm 0.034$ & $0.094 \pm 0.016$ \\
						%					A & $0.291 \pm 0.034$ & $0.293 \pm 0.024$ & $0.301 \pm 0.045$ \\
						%					B & $0.067 \pm 0.015$ & $0.218 \pm 0.034$ & $0.118 \pm 0.013$ \\
						%					C & $0.075 \pm 0.011$ & $0.176 \pm 0.028$ & $0.123 \pm 0.011$ \\
						%					D & $0.057 \pm 0.010$ & $\mathbf{0.097 \pm 0.015}$ & $\mathbf{0.068 \pm 0.017}$
						%				\end{tabular}}
						%			\end{center}
						%			\caption{Weather results.}
						%			\label{table:weather}
						%		\end{table*}
						
						
						%		\begin{table*}
						%			\begin{center}
						%				\resizebox{0.24\textwidth}{!}{
						%				\begin{tabular}{l|ccc}
						%					Alg. & $\tau = 0.1$     & $\tau = 0.5$  & $\tau = 0.9$      \\ \hline
						%					DR & $0.163 \pm 0.060$ & $0.240 \pm 0.037$ & $0.096 \pm 0.024$ \\
						%					DS & $0.180 \pm 0.075$ & $0.465 \pm 0.181$ & $0.120 \pm 0.022$ \\
						%					NB & $0.096 \pm 0.019$ & $0.194 \pm 0.033$ & $0.092 \pm 0.015$ \\
						%					JB & $0.101 \pm 0.030$ & $0.204 \pm 0.022$ & $0.107 \pm 0.041$ \\
						%					NL & $0.097 \pm 0.016$ & $0.210 \pm 0.034$ & $0.092 \pm 0.016$ \\
						%					JL & $0.091 \pm 0.007$ & $0.199 \pm 0.032$ & $0.091 \pm 0.010$ \\
						%					A & $0.396 \pm 0.080$ & $0.389 \pm 0.019$ & $0.387 \pm 0.056$ \\
						%					B & $0.090 \pm 0.012$ & $0.202 \pm 0.019$ & $0.085 \pm 0.008$ \\
						%					C & $0.094 \pm 0.011$ & $0.190 \pm 0.015$ & $0.083 \pm 0.010$ \\
						%					D & $0.092 \pm 0.025$ & $\mathbf{0.186 \pm 0.018}$ & $0.089 \pm 0.010$ \\
						%					E & $\mathbf{0.079 \pm 0.019}$ & $0.187 \pm 0.021$ & $\mathbf{0.070 \pm
						%						0.016}$
						%				\end{tabular}}
						%			\end{center}
						%			\caption{Motorcycle results.}
						%			\label{table:motorcycle}
						%		\end{table*}
						%		
						%		
						%		\begin{table*}
						%			\begin{center}
						%				\resizebox{0.24\textwidth}{!}{
						%				\begin{tabular}{l|ccc}
						%					Alg. & $\tau = 0.1$     & $\tau = 0.5$  & $\tau = 0.9$      \\ \hline
						%					DR & $0.125 \pm 0.008$ & $0.308 \pm 0.013$ & $0.162 \pm 0.028$ \\
						%					DS & $\mathbf{0.120 \pm 0.007}$ & $0.307 \pm 0.014$ & $0.157 \pm 0.011$ \\
						%					NB & $0.122 \pm 0.007$ & $\mathbf{0.306 \pm 0.017}$ & $\mathbf{0.151 \pm
						%						0.014}$ \\
						%					JB & $0.121 \pm 0.006$ & $0.307 \pm 0.016$ & $0.152 \pm 0.014$ \\
						%					NL & $0.121 \pm 0.016$ & $0.308 \pm 0.013$ & $0.153 \pm 0.011$ \\
						%					JL & $0.123 \pm 0.014$ & $\mathbf{0.306 \pm 0.012}$ & $0.153 \pm 0.013$ \\
						%					A & $0.328 \pm 0.028$ & $0.325 \pm 0.034$ & $0.324 \pm 0.073$ \\
						%					B & $0.122 \pm 0.017$ & $\mathbf{0.306 \pm 0.039}$ & $0.152 \pm 0.025$ \\
						%					C & $0.121 \pm 0.020$ & $0.311 \pm 0.041$ & $0.154 \pm 0.027$ \\
						%					D & $0.135 \pm 0.014$ & $0.310 \pm 0.045$ & $0.168 \pm 0.030$ \\
						%					E & $0.123 \pm 0.017$ & $0.309 \pm 0.045$ & $0.153 \pm 0.027$
						%				\end{tabular}}
						%			\end{center}
						%			\caption{Bone Mineral Density results.}
						%			\label{table:bmd}
						%		\end{table*}
						
	\subsection{Datasets}
	\label{sec:experiments:datasets}
	
		There are four standard datasets used in this experiment for comparisons of algorithm performance.
		
		The Antigen dataset samples the concentration of various molecules in a blood sample \citep{quadrianto2009kernel, lichman2013uci}. The dataset contains 97 points, with eight-dimensional observations $\bvec{y}$ and a one-dimensional state $x$.
		
		The Weather dataset measures a meteorological variable distributed over the surface of the Earth \citep{lichman2013uci}. It has two-dimensional observations $\bvec{y}$ and a one-dimensional state $x$, and contains 238 points.
		
		The Motorcycle dataset is a record of measurements taken from an accelerometer during a front-on motorcycle collision \citep{lichman2013uci}, and is often used because it contains heteroskedastic, non-Gaussian behaviour. The dataset contains 137 points, with a single input dimension $y$ (time), and a single output dimension $x$.
		
		Finally, the Bone Mineral Density dataset contains relative spinal bone mineral density measurements from North American adolescents of various ages \citep{hastie2005the}. It has a single input variable $y$ (age), a single output variable $x$ (bone mineral density), and 485 points. The original data were recorded by \cite{bachrach1999bone}.


	\subsection{Comparisons}
	\label{sec:experiments:comparison}
	
		There are five state-of-the-art algorithms from the literature that are used in the experiment. These algorithms are compared with our algorithms in terms of performance in producing quantile estimates.
		
		\textbf{Linear Quantile Estimation}. (Algorithm A) This is the linear direct quantile estimator described in \cite{koenker1978regression}.
		
		\textbf{Quantile SVM}. (Algorithm B) This is a nonparametric quantile estimator based on dual optimisation of the loss function through a SVM, as documented in \cite{takeuchi2006nonparametric}. A Gaussian kernel was used for this algorithm, with kernel width and regularisation fitted as described in \cite{quadrianto2009kernel}.
		
		\textbf{Reduction to Classification}. (Algorithm C) This is the algorithm for reducing a quantile estimation problem to a series of classifiers \citep{langford2012predicting}. The results from this algorithm are taken from \cite{quadrianto2009kernel}, which used 100 GP classifiers trained with expectation propagation and an RBF kernel.
		
		\textbf{Quantile GP}. (Algorithm D) This algorithm is the Gaussian-processed based direct CDF estimate \cite{quadrianto2009kernel}. An Gaussian kernel was used, with the hyper-parameters learned by optimising the marginal log-likelihood.
		
		\textbf{Heteroskedastic Quantile GP}. (Algorithm E) This algorithm is the same as the Quantile GP, except that the kernel is a heteroskedastic Gaussian with 10 latent pseudo-observations controlling the bandwidth along the length of the input dimension \citep{quadrianto2009kernel}. 
		
	\subsection{Results}
	\label{sec:experiments:results}
	
		The full results for each individual experiment and quantile are tabulated in \cref{table:results}. Curiously, the three top performers for the quantiles were each from a totally different family of algorithms. Our smooth direct embedding estimator (DS), the quantile SVM (B), and the quantile GP (D).
		
		From \cref{table:results}, we can see that algorithm E matched or exceeded all other algorithms in Motorcycle dataset, and was competitive but did not beat our algorithms in the Bone Mineral Density dataset. The likely explanation is the strong heteroskedasicity of the Motorcycle dataset benefited from a modelling approach that explicitly took it into account. This raises a possible avenue of future work--- including heteroskedastic kernel in our methods to improve performance.

\bibliography{kernel_embeddings}

\end{document}
