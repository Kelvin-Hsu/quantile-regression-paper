\documentclass[twoside]{article} \usepackage{aistats2017}

% If your paper is accepted, change the options for the package
% aistats2017 as follows:
%
%\usepackage[accepted]{aistats2017}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathbbol}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{vector}
\usepackage[usenames, dvipsnames]{color}
\usepackage{cleveref}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{identity}[theorem]{Identity}
\newtheorem{example}[theorem]{Example}

\newcommand{\eq}[1]{\begin{equation} \begin{aligned} #1 \end{aligned} \end{equation}}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\rv}[1]{{#1}}
\newcommand{\ds}[1]{\tilde{#1}}
\newcommand{\warn}[1]{{\color{red} #1}}
\newcommand{\extra}[1]{{\color{ForestGreen} #1}}
\newcommand{\qpi}{QPI }

\newcommand{\expect}[1]{{\mathbb{E}[#1]}}
\newcommand{\inner}[2]{{\langle #1, #2 \rangle}}

\newcommand{\Hk}{\mathcal{H}_{k}}
\newcommand{\Hl}{\mathcal{H}_{l}}
\newcommand{\muX}{\mu_{\rv{X}}}
\newcommand{\muY}{\mu_{\rv{Y}}}
\newcommand{\muYx}{\mu_{\rv{Y} | \rv{X} = x}}
\newcommand{\muXy}{\mu_{\rv{X} | \rv{Y} = y}}
\newcommand{\phiX}{\phi_{\rv{X}}}
\newcommand{\psiY}{\psi_{\rv{Y}}}
\newcommand{\Cxy}{C_{\rv{X} \rv{Y}}}
\newcommand{\Cyx}{C_{\rv{Y} \rv{X}}}
\newcommand{\Cxx}{C_{\rv{X} \rv{X}}}
\newcommand{\Cyy}{C_{\rv{Y} \rv{Y}}}
\newcommand{\Cylx}{C_{\rv{Y} | \rv{X}}}
\newcommand{\Cxly}{C_{\rv{X} | \rv{Y}}}

\newcommand{\hatmuX}{\hat{\mu}_{\rv{X}}}
\newcommand{\hatmuY}{\hat{\mu}_{\rv{Y}}}
\newcommand{\hatmuYx}{\hat{\mu}_{\rv{Y} | \rv{X} = x}}
\newcommand{\hatmuXy}{\hat{\mu}_{\rv{X} | \rv{Y} = y}}
\newcommand{\hatCxy}{\hat{C}_{\rv{X} \rv{Y}}}
\newcommand{\hatCyx}{\hat{C}_{\rv{Y} \rv{X}}}
\newcommand{\hatCxx}{\hat{C}_{\rv{X} \rv{X}}}
\newcommand{\hatCyy}{\hat{C}_{\rv{Y} \rv{Y}}}
\newcommand{\hatCylx}{\hat{C}_{\rv{Y} | \rv{X}}}
\newcommand{\hatCxly}{\hat{C}_{\rv{X} | \rv{Y}}}
\newcommand{\cardX}{\Vert \mathcal{X} \Vert}
\newcommand{\cardY}{\Vert \mathcal{Y} \Vert}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Quantile Regression with Kernel Embeddings : \\ Supplementary Material}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}

\end{abstract}

\section{Notation}
\label{sec:notation}

	\textbf{Vectors.} The only quantities that are bold in notation are vectors. Scalars, matrices, and functions are not denoted in bold. Keep in mind that this convention is chosen even after considering the fact that functions operate as uncountably infinite dimensional vectors in many settings within this paper.
	
	A vector $\bvec{v}$ with components $v_{1}, \dots, v_{n}$ can be written in multiple ways.
	
	\begin{equation}
		\bvec{v} = {\begin{bmatrix} v_{1} & \dots & v_{n} \end{bmatrix}}^{T} = \begin{bmatrix} v_{1} \\ \vdots \\ v_{n} \end{bmatrix} = \{v_{i}\}_{i = 1}^{n}
	\end{equation}
	
	\textbf{Indices.} The $j$-th element of a vector $\bvec{a}$ is denoted as $a_{j}$. The $(i, j)$-th element of a matrix $A$ is denoted as $a_{i, j}$.
	
	\textbf{Random Variables and Vectors.} A random variable (RV) is denoted with an upper case letter, as in $\rv{X}$. A random vector (also RV) is then denoted in bold, as in $\bvec{\rv{X}}$. Semantically, while random variables and vectors are so named, they are formally a mapping from the sample space $\Omega$ to some other Borel measurable space, such as $\mathcal{X} = \mathbb{R}$ or $\mathcal{X} = \mathbb{R}^{d}$. This is where the notations in probability theory and statistics differ slightly from those in measure theory. In general, while the notations in measure theory are more precise (making clear that RVs are mappings), the notations in probability theory and statistics are easier to read and interpret (treating RVs as what it sounds like - random variables and vectors).
	
	For events, a common shorthand used in both probability theory and measure theory is
	
	\begin{equation}
		\{\rv{X} \in A\} := \rv{X}^{-1}[A] := \{\omega \in \Omega : \rv{X}(\omega) \in A\} \in \mathcal{W}
	\end{equation}
	
	\textbf{Measures, Functions, and Functionals.} While both measures and regular functions are mappings, this paper makes a distinction between mappings whose domain is a $\sigma$-algebra (measures) and mappings whose domain is not (other functions). This distinction is emphasised by the notation of how a measure $\nu$ or a regular function $f$ takes its arguments. A measure takes its arguments with square brackets, as in $\nu[\cdot]$. A regular function takes its arguments with round brackets, as in $f(\cdot)$. For functionals $\mathbb{F}$, or mappings that take regular functions as an input, square brackets are also used, such as $\mathbb{F}[\cdot]$.
	
	Typical examples of measures are the probability measure $\mathbb{P} : \mathcal{W}_{\Omega} \mapsto [0, 1]$ on the sample space $\Omega$ and the Lebesgue measure $m_{N} : \mathcal{A}_{\mathbb{R}^{N}} \mapsto [0, \infty)$ on Euclidean space $\mathbb{R}^{N}$, where $\mathcal{W}_{\Omega}$ and $\mathcal{A}_{\mathbb{R}^{N}}$ are $\sigma$-algebras of their respective subscripts.
	
	Typical examples of regular functions involve functions $f : \mathcal{X} \mapsto \mathbb{R}$ that form a Hilbert space (HS) or reproducing kernel Hilbert space (RKHS) and random variables $\rv{X} : \Omega \mapsto \mathcal{X}$.
	
	Typical examples of functionals are the expectance operator $\mathbb{E}$ and variance operator $\mathbb{V}$ that take in a function or function composition whose domain is a $\sigma$-algebra of the sample space $\Omega$. For example, it takes in a function that maps from $\Omega$ to $\mathbb{R}$ (for instance, a random variable), and outputs a real number in $\mathbb{R}$ (see also the next point). An inner product on an uncountably infinite dimensional Hilbert space $\langle \cdot, \cdot, \rangle$ is a functional of two arguments, recalling that a function can be seen as an uncountably infinite dimensional vector. A Lebesgue integral $\int_{B} \cdot d\nu$ (as opposed to a Riemann integral) with respect to a measure $\nu$ on some set $B$ is also a functional.
	
	\textbf{Expectance and Variance Operators.} For a random variable $\rv{X} : \Omega \mapsto \mathcal{X}$ and a function $f : \mathcal{X} \mapsto \mathbb{R}$. The notation in probability theory and statistics for the expectance of a function of a random variable would be $\mathbb{E}[f(\rv{X})]$, as if $\rv{X}$ is a variable and not a function, while in measure theory the more precise notation is $\mathbb{E}[f \circ \rv{X}]$, as $\mathbb{E}$ is a functional and should therefore take a function as the input. In practice, $\mathbb{E}[f(\rv{X})]$ becomes a shorthand for $\mathbb{E}[f \circ \rv{X}]$, and thus the former is used more often.
	
	\textbf{Indicator Measures and Functions.} Another example of a measure is the indicator measure $\mathbb{1} : \mathcal{W} \mapsto \{0, 1\}$. Using the definition $\mathbb{1}_{A}(x) := \mathbb{1}[x \in A]$, the indicator function is then a regular function $\mathbb{1}_{A} : \mathcal{X} \mapsto \{0, 1\}$. Note that the latter is an abuse of notation since the shorthand for $x \in A$ is only defined if $x$ is a random variable (and thus a mapping). Formally, we define the indicator function as below \eqref{eq:indicator_function}.
	
	\begin{equation}
		\mathbb{1}_{A}(x) := \begin{cases}
			1 & : x \in A \\
			0 & : x \notin A
		\end{cases}
	\label{eq:indicator_function}
	\end{equation}
	
	\textbf{Datasets.} A dataset consisting of $n$ observations of $d$-dimensional vector quantities will be denoted using an upper case with a curly tilde on top, such as $\ds{X}$, as it is also a matrix of size $n \times d$ so that $\ds{X} \in \mathbb{R}^{n \times d}$. We call the dimension from $1$ to $n$ the observation dimension, and the dimension from $1$ to $d$ the covariate dimension, noting that the covariates themselves are not always the same as the features. As such, we have $\ds{X} = \{x_{i, j}\}_{i = 1, j = 1}^{n, d}$, where $x_{i, j}$ is the $j$-th covariate of the $i$-th observation. We also denote those observations equivalently by

	\begin{equation}
	\bvec{x}_{i} = \{x_{i, j}\}_{j = 1}^{d} \equiv \{x_{i, 1}, \dots, x_{i, d}\} \equiv \begin{bmatrix}
	x_{i, 1} & \cdots & x_{i, d}
	\end{bmatrix}^{T}
	\label{eq:data_vector_notation}
	\end{equation}
	
	for all $i \in \{1, \dots, n\}$, and thus

	\begin{equation}
		\ds{X} = \{\bvec{x}_{i}\}_{i = 1}^{n} \equiv \{\bvec{x}_{1}, \dots, \bvec{x}_{n}\} \equiv \begin{bmatrix}
			\bvec{x}_{1} & \cdots & \bvec{x}_{n}
		\end{bmatrix}^{T}
	\label{eq:data_matrix_notation}
	\end{equation}
		
	Since $\{ \cdot \}$ stacks the components in the first axis, stacking $d$ scalars would turn it into a $\mathbb{R}^{d}$ vector \eqref{eq:data_vector_notation}, while stacking $n$ $\mathbb{R}^{d}$ vectors will turn it into a $\mathbb{R}^{n \times d}$ matrix \eqref{eq:data_matrix_notation}.
	
	\textbf{Vectorisation.} For a function $f : \mathcal{X} \mapsto \mathbb{R}$ for some Borel measurable space $\mathcal{X}$. Suppose $x_{i} \in \mathcal{X}$, $i \in \{1, \dots, n\}$, and the function $f$ is to be applied on each of those elements. Suppose further, as \eqref{eq:data_matrix_notation} we denote $X = \{x_{i}\}_{i = 1}^{n}$. Note that here we do not bold the elements $x_{i}$ as they may or may not be vectors, or even elements of a Euclidean space, in order to illustrate the general case. We define the following shorthand notation for the vector containing $f(x_{1}), \dots, f(x_{n})$. We choose the curly brackets \eqref{eq:function_vectorisation} to remind readers of the shorthand notation for the vector $\{f(x_{i})\}_{i = 1}^{n}$.
	
	\begin{equation}
		f\{\ds{X}\} := \{f(x_{i})\}_{i = 1}^{n} = {\begin{bmatrix} f(x_{1}) & \dots & f(x_{n}) \end{bmatrix}}^{T}
	\label{eq:function_vectorisation}
	\end{equation}
	
	\textbf{Kernel Embeddings.} In the kernel embeddings setting, the partially applied kernel function, $k(x, \cdot)$, acts like a feature vector for the RKHS it spans over $x \in \mathcal{X}$. However, instead of an usual finite dimensional vector, this is an (usually uncountably, depending on the cardinality of $\mathcal{X}$) infinite dimensional vector as it is a function of one variable in $\mathcal{X}$. In many literature, the notation $\phi(x) := k(x, \cdot)$ is used. This is because in most kernel based algorithms, such as support vector machines, the feature vector is a finite dimensional vector so that $\phi(x) \in \mathbb{R}^{p}$. However, in the kernel embedding context, the dimensionality of the feature vector is the cardinality of the domain of interest $\mathcal{X}$. In many cases, $\mathcal{X}$ is the euclidean space $\mathbb{R}^{d}$ such that the dimensionality of the feature vector is uncountably infinite. As such, the feature vector represents an enumerated function, indexed by its inputs. To emphasise this, this paper will use $\phi_{x} := k(x, \cdot)$ so that it is clearer that $\phi_{x}$ is a function of one variable such that $\phi_{x}(x') = k(x, x')$. It would be awkward to write $\phi(x)(x')$ otherwise. Also, as an uncountably infinite dimensional vector, when an appropriate Hilbert-Schmidt operator comes, say $C_{\rv{Y} | \rv{X}}$, it would look less awkward if we write $C_{\rv{Y} | \rv{X}} \phi_{x}$ as in $\phi_{x}$ is an element of some vector space where linear operations can be applied onto, instead of $C_{\rv{Y} | \rv{X}} \phi(x)$ where it looks like this is $C_{\rv{Y} | \rv{X}}$ multiples of some functional value $\phi(x)$.
	
	In this way, we emphasise that, for continuous euclidean spaces $\mathcal{X}$, $\phi_{x}$ is an uncountably infinite dimensional vector, or a matrix with size $\cardX \times 1$. We can then write tensor products and inner products with matrix notation as usual, as outlined in \cref{sec:kernel_embeddings:empirical_representation}.
	
\section{Probability Theory and Measure Theory}
\label{sec:probability_theory}

	Let $(\Omega, \mathcal{W}, \mathbb{P})$ be a probability space, where $\Omega$ is the sample space, $\mathcal{W}$ is a $\sigma$-algebra on $\Omega$, and $\mathbb{P} : \mathcal{W} \mapsto [0, 1]$ a probability measure. The elements $W$ of $\mathcal{W}$, that is, the measurable sets, are called events, which are by definition subsets of $\Omega$.
	
	\begin{definition} \label{def:distribution}
		(Distribution)
		The \textit{distribution} of a random variable $\rv{X} : \Omega \mapsto \mathcal{X}$ is the Borel measure $\mathbb{P}_{\rv{X}} : \mathcal{A} \mapsto [0, 1]$ defined by
		\begin{equation}
		\mathbb{P}_{\rv{X}}[A] := \mathbb{P}[\rv{X} \in A]
		\label{eq:distribution}
		\end{equation}
		where the event $\{\rv{X} \in A\} := \rv{X}^{-1}[A] := \{\omega \in \Omega : \rv{X}(\omega) \in A\} \in \mathcal{W}$ is the inverse image (pre-image) of $A \subseteq \mathcal{X}$ under the random variable $\rv{X}$, and $\mathcal{A}$ is a $\sigma$-algebra of subsets on $\mathcal{X}$.
		
		Recall that a random variable (in this case, $\rv{X}$) is formally a map from the sample space (in this case, $\Omega$) to some Borel measurable domain (in this case, $\mathcal{X}$) of interest. The \textit{distribution} is also a probability measure itself, as it satisfies the defining properties of a probability measure, such that $(\mathcal{X}, \mathcal{A}, \mathbb{P}_{\rv{X}})$ is also a probability space. It differs from $\mathbb{P}$ in that it operates in a different domain - not directly on the events, but on the values the random variable takes for those events.
	\end{definition}
	
	\begin{definition} \label{def:cdf}
		(Cumulative Distribution Function)
		The \textit{cumulative distribution function} (CDF) $P_{\rv{X}} : \mathbb{R} \mapsto [0, 1]$ of a \textit{real-valued} random variable $\rv{X} : \Omega \mapsto \mathbb{R}$ and random vector $\bvec{\rv{X}} : \Omega \mapsto \mathbb{R}^{d}$ is defined as follows \eqref{eq:cdf}.
		\begin{equation}
		\begin{aligned}
		P_{\rv{X}}(x) &:= \mathbb{P}_{\rv{X}}[(-\infty, x]] = \mathbb{P}[\rv{X} \leq x] \\
		P_{\bvec{\rv{X}}}(\bvec{x}) &:= \mathbb{P}_{\bvec{\rv{X}}}[(-\infty, x_{1}] \times \dots \times (-\infty, x_{d}]] = \mathbb{P}[\bvec{\rv{X}} \leq \bvec{x}] \\
		\end{aligned}
		\label{eq:cdf}
		\end{equation}
		
		For conciseness, we define the following shorthand \eqref{eq:shorthand}. 
		\begin{equation}
		\begin{aligned}
		\{\rv{X} \leq x\} &:= \{\rv{X} \in (-\infty, x]\} \\
		\{\bvec{\rv{X}} \leq \bvec{x}\} &:= \bigcap\limits_{j = 1}^{d} \{\rv{X}_{j} \in (-\infty, x_{j}]\} \in \mathcal{W} \\
		(-\bvec{\infty}, \bvec{x}] &:= (-\infty, x_{1}] \times \dots \times (-\infty, x_{d}] \\
		\end{aligned}
		\label{eq:shorthand}
		\end{equation}
	\end{definition}
	
	\begin{theorem} \label{thm:radon_nikodym}
		(Radon-Nikodym)
		If the distribution $\mathbb{P}_{\rv{X}}$ is absolutely continuous with respect to the Lebesgue measure on $\mathcal{X} = \mathbb{R}^{d}$, then the Radon-Nikodym theorem implies the existence of a density for $\mathbb{P}_{\rv{X}}$. We call this the distribution density of $\rv{X}$, or more commonly known as the \textit{probability density function} (PDF), denoted by $p_{\rv{X}}: \mathcal{X} \mapsto [0, \infty)$.
	\end{theorem}
	
	Let $f : \mathcal{X} \mapsto \mathbb{R}$ be some arbitrary function over the domain, then we have
	\begin{equation}
	\mathbb{E}[f(\rv{X})] = \int_{\Omega} f \circ \rv{X} d \mathbb{P} = \int_{\mathcal{X}} f d \mathbb{P}_{\rv{X}}
	\label{eq:functional_expectation}
	\end{equation}
	
	Recall that $\mathbb{P}[W] = \mathbb{E}[\mathbb{1}[W]] \;\; \forall W \in \mathcal{W}$, so that the CDF for a vector-valued random variable $\bvec{\rv{X}}$ is
	\begin{equation}
	\begin{aligned}
	P_{\bvec{\rv{X}}}(\bvec{x}) &= \mathbb{P}[\bvec{\rv{X}} \leq \bvec{x}] = \mathbb{E}[\mathbb{1}[\bvec{\rv{X}} \leq \bvec{x}]] = \mathbb{E}[\mathbb{1}_{(-\bvec{\infty}, \bvec{x}]}(\bvec{X})] \\
	&= \int_{\Omega} \mathbb{1}_{(-\bvec{\infty}, \bvec{x}]} \circ \bvec{\rv{X}} d \mathbb{P} = \int_{\mathbb{R}^{D}} \mathbb{1}_{(-\bvec{\infty}, \bvec{x}]} d \mathbb{P}_{\bvec{\rv{X}}} \\
	&= \int_{(-\bvec{\infty}, \bvec{x}]} d \mathbb{P}_{\bvec{\rv{X}}}
	\end{aligned}
	\label{eq:cdf_integral}
	\end{equation}
	where we use the common notation $\mathbb{1}_{A}(x) := \mathbb{1}[x \in A]$.
		
\section{Kernel Embeddings}
\label{sec:kernel_embeddings}
	
	This section summarises fundamental and important results in the field of kernel embeddings. The purpose is to present relevant results and theorems that form the basis for understanding works that involve kernel embeddings, as well as to provide a reference for its literature in both notation and concept. 
	
	\Cref{sec:kernel_embeddings:theoretical_representation} introduces and summarises the formulation and theory of kernel embeddings. \Cref{sec:kernel_embeddings:empirical_representation} then extends the theoretical work to the empirical case, focusing on representations of kernel embeddings and its structure given finite amounts of sample data.
	
	\subsection{Theoretical Representation}
	\label{sec:kernel_embeddings:theoretical_representation}
	
		In this paper we assume all kernels we deal with are positive definite and characteristic. That is, the kernel $k : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ defines a unique reproducing kernel Hilbert space (RKHS), which we will denote by $\mathcal{H}_{k}$. Note that in some literature, since $k$ takes elements $x$ of $\mathcal{X}$ as arguments, the corresponding reproducing kernel Hilbert space is denoted as $\mathcal{H}_{\mathcal{X}}$ instead. However, since the kernel $k$ also includes information about the space $\mathcal{X}$ it operates on, the notation $\mathcal{H}_{k}$ will be preferred.
		
		As always, let $\rv{X} : \Omega \mapsto \mathcal{X}$ be a random variable taking values in $\mathcal{X}$. Similarly, let $\rv{Y} : \Omega \mapsto \mathcal{Y}$ be a random variable taking values in $\mathcal{Y}$. Let $k$ and $l$ be characteristic positive definite kernels defined on $\mathcal{X}$ and $\mathcal{Y}$ respectively. Let $f \in \Hk$ and $g \in \Hl$ be real-valued functions.
			
		\begin{theorem} \label{thm:feature_functions}
			(Feature Functions)
			The feature functions that form the basis for the full RKHS are obtained by partially applying the respective kernel of that RKHS \eqref{eq:feature_functions}.
			\begin{equation}
			\begin{aligned}
				\phi_{x} &:= k(x, \cdot) \in \Hk \\
				\psi_{y} &:= l(y, \cdot) \in \Hl
			\label{eq:feature_functions}
			\end{aligned}
			\end{equation}
		\end{theorem}
	
		\begin{theorem} \label{thm:reproducing_property}
			(Reproducing Property)
			The evaluation operator of a function within the full RKHS is the inner product with the feature function in that RKHS \eqref{eq:reproducing_property}.
			\begin{equation}
			\begin{aligned}
				\inner{\phi_{x}}{f} &= f(x) \\
				\inner{\psi_{y}}{g} &= g(y)
			\label{eq:reproducing_property}
			\end{aligned}
			\end{equation}
			
			In particular, the kernel value between two points can be evaluated as the inner product between the feature functions at those points \eqref{eq:kernel_reproducing_property}.
			\begin{equation}
			\begin{aligned}
				\inner{\phi_{x}}{\phi_{x'}} &= k(x, x') \\
				\inner{\psi_{y}}{\psi_{y'}} &= l(y, y')
			\label{eq:kernel_reproducing_property}
			\end{aligned}
			\end{equation}
		\end{theorem}
	
		\begin{definition} \label{def:kernel_embedding}
			(Kernel Embedding)
			The kernel embedding (or mean embedding) of a distribution $\mathbb{P}_{\rv{X}}$ is defined as the expectation of the corresponding feature function in that RKHS \eqref{eq:kernel_embedding}.
			
			\begin{equation}
			\begin{aligned}
				\muX \equiv \muX(\cdot) &:= \expect{\phiX} = \expect{k(\rv{X}, \cdot)} \in \Hk \\
				\muY \equiv \muY(\cdot) &:= \expect{\psiY} = \expect{l(\rv{Y}, \cdot)} \in \Hl
			\label{eq:kernel_embedding}
			\end{aligned}
			\end{equation}	
			where it is understood that the $\cdot$ is a place-holder for an argument such that $\muX(\cdot) : \mathcal{X} \mapsto \mathbb{R}$ is a function and not just a value.

			Alternatively, this definition can be written as follows.
			\begin{equation}
			\begin{aligned}
				\muX(x) &:= \expect{k(\rv{X}, x)} = \int_{\mathcal{X}} k(\cdot, x) d \mathbb{P}_{\rv{X}} \\
				\muY(y) &:= \expect{l(\rv{Y}, y)} = \int_{\mathcal{Y}} l(\cdot, y) d \mathbb{P}_{\rv{Y}}
			\label{eq:kernel_embedding_alternative}
			\end{aligned}
			\end{equation}
			where it is understood that the $\cdot$ here in the last equation is for a different argument than the previous equations.

			In some literature, to make explicit of the fact that the kernel embedding is an embedding of a distribution measure $\mathbb{P}_{\rv{X}}$, the kernel embedding is instead denoted by $\mu_{\mathbb{P}_{\rv{X}}}$. Nevertheless, since the specification of a random variable $\rv{X}$ and its distribution $\mathbb{P}_{\rv{X}}$ must come together, either notation is equally informative.

			Usually, the kernel $k$ comes from a family of kernel functions with a set of hyperparameters $\bm{\theta}$. To make this relationship explicit, a subscript to the notation may be attached as $k_{\bm{\theta}}$. Correspondingly, the kernel embedding would then be denoted as $\mu_{\rv{X}, \bm{\theta}}$.

			It would indeed be more precise to put a subscript $k$ on the kernel embedding, as $\mu_{\rv{X}, k}$ or $\mu_{\rv{X}, k_{\bm{\theta}}}$, to make clear that it was embedded with kernel $k$. However, as the kernel would not be changing much except up to some hyperparameter tuning, this will just be implicitly assumed.
		\end{definition}
	
		\begin{theorem} \label{thm:function_expectation}
			(Function Expectation)
			The expectation of a function of a random variable can be evaluated as the inner product between the corresponding kernel embedding and the function \eqref{eq:function_expectation}.
			\begin{equation}
				\inner{\muX}{f} = \expect{f(\rv{X})}
				\label{eq:function_expectation}
			\end{equation}
			
			\begin{proof}
				\begin{align*}
					\inner{\muX}{f} &= \inner{\expect{\phiX}}{f} \tag{\cref{def:kernel_embedding}} \\
					&= \expect{\inner{\phiX}{f}} \tag{bilinearity of $\inner{\cdot}{\cdot}$} \\
					&= \expect{f(\rv{X})} \tag{\cref{thm:reproducing_property}}
				\end{align*}
			\end{proof}
		\end{theorem}
		
		\begin{definition} \label{def:tensor_operator}
			(Tensor Product as an Operator)
			Suppose $a \in \mathcal{H}_{1}$ and $b \in \mathcal{H}_{2}$. Applying the tensor product $(a \otimes b) \in \mathcal{H}_{1} \otimes \mathcal{H}_{2}$ on $c \in \mathcal{H}_{2}$ results in an element in $\mathcal{H}_{1}$ according to \eqref{eq:tensor_operator}. Note that sometimes we may use $\cdot$ to emphasise this operation so that it is not confused with scalar multiplication. Also note that the result of the dot product $\langle b, c \rangle$ is a scalar.
			\begin{equation}
				(a \otimes b) c \equiv (a \otimes b) \cdot c := a \langle b, c \rangle = \langle b, c \rangle a \in \mathcal{H}_{1}
			\label{eq:tensor_operator}
			\end{equation}
		\end{definition}
		
		\begin{definition} \label{def:inner_of_tensor}
			(Inner Products of Tensor Products)
			Suppose $a, c \in \mathcal{H}_{1}$ and $b, d \in \mathcal{H}_{2}$. The inner product of the two tensor products are defined to be the product of the corresponding component inner products \eqref{eq:inner_of_tensor}.
			\begin{equation}
				\inner{a \otimes b}{c \otimes d} := \inner{a}{c} \inner{b}{d}
			\label{eq:inner_of_tensor}
			\end{equation}
		\end{definition}
		
		\begin{definition} \label{def:cross_cov}
			(Uncentred Cross-Covariance Operators)
			The uncentred cross-covariance operators are defined as the kernel embedding of the corresponding tensor-joint RKHS. \eqref{eq:cross_cov}.
			\begin{equation}
			\begin{aligned}
				\Cxx &:= \expect{\phiX \otimes \psiY} \in \Hk \otimes \Hk \\
				\Cxy &:= \expect{\phiX \otimes \psiY} \in \Hk \otimes \Hl \\
				\Cyx &:= \expect{\psiY \otimes \phiX} \in \Hl \otimes \Hk \\
				\Cyy &:= \expect{\psiY \otimes \psiY} \in \Hl \otimes \Hl 
			\label{eq:cross_cov}
			\end{aligned}
			\end{equation}		
		\end{definition}
	
		\begin{lemma} \label{thm:cxy_g}
			$\Cxy \in \mathcal{H}_{k} \otimes \mathcal{H}_{l}$ is also an operator from $\Hl$ to $\Hk$, such that $\Cxy : \Hl \mapsto \Hk$, with its operation given by the following identity \eqref{eq:cxy_g}.
			\begin{equation}
				\Cxy \cdot g = \expect{g(\rv{Y}) \phiX}
			\label{eq:cxy_g}
			\end{equation}
		
			\begin{proof}
			\begin{align*}
				\Cxy \cdot g &=  (\expect{\phiX \otimes \psiY}) \cdot g \tag{\cref{def:cross_cov}} \\
				&= \expect{(\phiX \otimes \psiY) \cdot g} \tag{linearity of $\expect{\cdot}$} \\
				&= \expect{\phiX \inner{\psiY}{g}} \tag{\cref{def:tensor_operator}} \\
				&= \expect{g(\rv{Y}) \phiX} \tag{\cref{thm:reproducing_property}}
			\end{align*}
			\end{proof}
		\end{lemma}
			
		\begin{lemma} \label{thm:cxy_g_f}
			Being the expectation of a scalar multiple of $\phiX$, this shows that $\Cxy \cdot g \in \mathcal{H}_{k}$, so that the inner product with $f \in \mathcal{H}_{k}$ can also be applied, resulting in the following identity \eqref{eq:cxy_g_f}.
			\begin{equation}
				\inner{\Cxy \cdot g}{f} = \expect{f(\rv{X}) g(\rv{Y})}
			\label{eq:cxy_g_f}
			\end{equation}
			\begin{proof}
			\begin{align*}
				\inner{\Cxy \cdot g}{f} &= \inner{\expect{g(\rv{Y}) \phiX}}{f} \tag{\cref{thm:cxy_g}}\\
				&= \expect{g(\rv{Y}) \inner{\phiX}{f}} \tag{bilinearity of $\inner{\cdot}{\cdot}$} \\
				&= \expect{f(\rv{X}) g(\rv{Y})} \tag{\cref{thm:reproducing_property}}
			\end{align*}
			\end{proof}
		\end{lemma}
			
	
		\begin{lemma} \label{thm:cxy_f_g}
			Alternatively, since $\Cxy \in \mathcal{H}_{k} \otimes \mathcal{H}_{l}$, the inner product of $\Cxy$ and a function $f \otimes g \in \mathcal{H}_{k} \otimes \mathcal{H}_{l}$ in the same space results in another identity \eqref{eq:cxy_f_g}.
			\begin{equation}
				\inner{\Cxy}{f \otimes g} = \expect{f(X) g(Y)}
			\label{eq:cxy_f_g}
			\end{equation}
			
			\begin{proof}
			\begin{align*}
				\inner{\Cxy}{f \otimes g} &= \inner{\expect{\phiX \otimes \psiY}}{f \otimes g} \tag{\cref{def:cross_cov}} \\
				&= \expect{\inner{\phiX \otimes \psiY}{f \otimes g}} \tag{bilinearity} \\
				&= \expect{\inner{\phiX}{f} \inner{\psiY}{g}} \tag{\cref{def:inner_of_tensor}}\\
				&= \expect{f(\rv{X}) g(\rv{Y})} \tag{\cref{thm:reproducing_property}}
			\end{align*}
			\end{proof}			
		\end{lemma}
		
		\begin{theorem}
			Together, \cref{thm:cxy_g_f} and \cref{thm:cxy_f_g} reveals an important identity for the cross-covariance operator $\Cxy$ \eqref{eq:cxy}.
			
			\begin{equation}
			\begin{aligned}
				 \inner{\Cxy g}{f} &= \inner{\Cxy}{f \otimes g} = \inner{\Cyx f}{g} \\
				 &= \expect{f(\rv{X}) g(\rv{Y})}
				 \label{eq:cxy}			
			\end{aligned}
			\end{equation}
			
			where the second equality is obtained through symmetry. \\
		\end{theorem}
	
		\begin{theorem} \label{thm:cxx_f_cxy_g}
			Define $f$ in relation to $g$ as follows \eqref{eq:f_def_by_g}.
			\begin{equation}
				f := \expect{g(Y) | X = \cdot}
			\label{eq:f_def_by_g}
			\end{equation}
			
			Then, the following result hold \eqref{eq:cxx_f_cxy_g}.
			\begin{equation}
				\Cxx \cdot f = \Cxy \cdot g
			\label{eq:cxx_f_cxy_g}
			\end{equation}
			
			Further, if the inverse of the operator $\Cxx \equiv \Cxx \cdot$ exists, then $f$ is uniquely defined by \eqref{eq:f_cxxinv_cxy_g}.
			
			\begin{equation}
				f := \expect{g(\rv{Y}) | \rv{X} = \cdot} = \Cxx^{-1} \Cxy g
			\label{eq:f_cxxinv_cxy_g}
			\end{equation}
	
			\begin{proof}
			Notice that $f$ is a function in $\mathcal{H}_{k}$ and the domain of $f$ is $\mathcal{X}$ such that $f(x) = \expect{g(\rv{Y}) | \rv{X} = x}$.
			
			\begin{align*}
				\Cxx \cdot f &= \expect{\phiX \otimes \phiX} \cdot f \tag{\cref{def:cross_cov}} \\
				&= \expect{(\phiX \otimes \phiX) \cdot f} \tag{linearity of $\expect{\cdot}$} \\
				&= \expect{\phiX \inner{\phiX}{f}} \tag{\cref{def:tensor_operator}} \\
				&= \expect{\phiX f(\rv{X})} \tag{\cref{thm:reproducing_property}} \\
				&= \expect{\phiX \expect{g(\rv{Y}) | \rv{X}}} \tag{equation \eqref{eq:f_def_by_g}} \\
				&= \expect{\expect{\phiX g(\rv{Y}) | \rv{X}}} \tag{$\rv{X}$ is given} \\
				&= \expect{\phiX g(\rv{Y})} \tag{tower property} \\
				&= \Cxy \cdot g \tag{\cref{thm:cxy_g}}
			\end{align*}
			
			If the inverse of the operator $\Cxx \equiv \Cxx \cdot$ exists, then we can write the following.
			
			\begin{align*}
				\Cxx f &= \Cxy g \tag{equation \eqref{eq:cxx_f_cxy_g}} \\
				f &= \Cxx^{-1} \Cxy g \tag{$\Cxx^{-1}$ exists} \\
				\expect{g(\rv{Y}) | \rv{X} = \cdot} &= \Cxx^{-1} \Cxy g \tag{equation \eqref{eq:f_def_by_g}}
			\end{align*}
			
			\end{proof}
		\end{theorem}
	
		\begin{definition} \label{def:conditional_embedding}
			(Conditional Embedding)
			The conditional embedding of $\rv{Y}$ given $\{\rv{X} = x\}$ is the kernel embedding of the distribution $\mathbb{P}_{\rv{Y} | \rv{X} = x}$ \eqref{eq:conditional_embedding}. Symmetrically, the conditional embedding of $\rv{X}$ given $\{\rv{Y} = y\}$ is the kernel embedding of the distribution $\mathbb{P}_{\rv{X} | \rv{Y} = y}$ \eqref{eq:conditional_embedding}.
			
			\begin{equation}
			\begin{aligned}
				\muYx &:= \expect{\psiY | \rv{X} = x} = \expect{l(\rv{Y}, \cdot) | \rv{X} = x} \\
				\muXy &:= \expect{\phiX | \rv{Y} = y} = \expect{k(\rv{X}, \cdot) | \rv{Y} = y}
			\label{eq:conditional_embedding}
			\end{aligned}
			\end{equation}
		\end{definition}
		
		\begin{theorem} \label{thm:conditional_function_expectation}
			(Conditional Function Expectation)
			The conditional expectation of a function can be evaluated as the inner product between the corresponding conditional embedding and the function \eqref{eq:conditional_function_expectation}.
			\begin{equation}
				\inner{\muYx}{g} = \expect{g(\rv{Y}) | \rv{X} = x}
			\label{eq:conditional_function_expectation}
			\end{equation}
			
			Furthermore, the conditional expectation of a function can also be obtained from applying cross-covariance operators to the function and then evaluating that function \eqref{eq:conditional_function_expectation_alt}.
			
			\begin{equation}
				\expect{g(\rv{Y}) | \rv{X} = x} = (\Cxx^{-1} \Cxy g)(x)
			\label{eq:conditional_function_expectation_alt}
			\end{equation}
			
			Thus, the following identity holds \eqref{eq:conditional_function_expectation_identity}
			
			\begin{equation}
				\inner{\muYx}{g} = \expect{g(\rv{Y}) | \rv{X} = x} = (\Cxx^{-1} \Cxy g)(x)
			\label{eq:conditional_function_expectation_identity}	
			\end{equation}
	
			\begin{proof}
				\begin{align*}
					\inner{\muYx}{g} &= \inner{\expect{\psiY | \rv{X} = x}}{g} \tag{\cref{def:conditional_embedding}} \\
					&= \expect{\inner{\psiY}{g} | \rv{X} = x} \tag{bilinearity of $\inner{\cdot}{\cdot}$} \\
					&= \expect{g(\rv{Y}) | \rv{X} = x} \tag{\cref{thm:reproducing_property}} \\
					&= (\expect{g(\rv{Y}) | \rv{X} = \cdot})(x) \tag{notation: $f \equiv f(\cdot)$} \\
					&= (\Cxx^{-1} \Cxy g)(x) \tag{\cref{thm:cxx_f_cxy_g}}
				\end{align*}
			\end{proof}
		\end{theorem}
		
		\begin{definition} \label{def:conditional_operator}
			(Conditional Operator)
			The conditional operator $\Cylx$ is defined by the operator that satisfies the following equality \eqref{eq:conditional_operator}.
			\begin{equation}
				\muYx = \Cylx \cdot \phi_{x} \equiv \Cylx \phi_{x}
			\label{eq:conditional_operator}
			\end{equation}
		\end{definition}
	
		\begin{theorem} \label{thm:conditional_operator}
			(Conditional Operator)
			The conditional operator $\Cylx$ can be expressed in terms of cross-covariance operators \eqref{eq:conditional_operator_cross_cov}, provided that $\Cxx^{-1}$ exists.
			\begin{equation}
				\Cylx = \Cyx \Cxx^{-1}
			\label{eq:conditional_operator_cross_cov}
			\end{equation}
			
			\begin{proof}
				\begin{align*}
				\Cyx :=& \expect{\psiY \otimes \phiX} \tag{\cref{def:cross_cov}}\\
				=& \expect{\expect{\psiY \otimes \phiX | \rv{X}}} \tag{tower property} \\
				=& \expect{\expect{\psiY| \rv{X}} \otimes \phiX} \tag{\rv{X} is given} \\
				=& \expect{\mu_{\rv{Y} | \rv{X}} \otimes \phiX} \tag{\cref{def:conditional_embedding}}\\
				=& \expect{(\Cylx \cdot \phiX) \otimes \phiX} \tag{\cref{def:conditional_operator}} \\
				=& \expect{\Cylx \cdot (\phiX \otimes \phiX)} \tag{tensor associativity} \\
				=& \Cylx \cdot \expect{\phiX \otimes \phiX} \tag{linearity of $\expect{\cdot}$} \\
				=& \Cylx \cdot \Cxx \equiv \Cylx \Cxx \tag{\cref{def:cross_cov}}
				\end{align*}
				
				Therefore, if $\Cxx^{-1}$ exists, then \eqref{eq:conditional_operator_cross_cov} holds.
			\end{proof}
		\end{theorem}
		
		\begin{theorem} \label{thm:kernel_sum_rule}
			(Kernel Sum Rule)
			The Kernel Sum Rule \eqref{eq:kernel_sum_rule} is the kernelised and non-parametric version of the basic sum rule of probability \eqref{eq:sum_rule}.
			\begin{equation}
			\begin{aligned}
				p_{\rv{X}}(x) &= \int_{\mathcal{Y}} p_{\rv{X} | \rv{Y}}(x | \cdot) d\mathbb{P}_{\rv{Y}} \quad & \mathrm{(Lebesgue)} \\
				&= \int_{\mathcal{Y}} p_{\rv{X} | \rv{Y}}(x | y) p_{\rv{Y}}(y)dy \quad & \mathrm{(Riemann)}
			\label{eq:sum_rule}
			\end{aligned}
			\end{equation}
			
			\begin{equation}
				\muX = \Cxly \muY
			\label{eq:kernel_sum_rule}
			\end{equation}
			
			\begin{proof}
				\begin{align*}
				\Cxly \muY &= \Cxly \expect{\psiY} \tag{\cref{def:kernel_embedding}} \\
				&= \expect{\Cxly \psiY} \tag{linearity of $\expect{\cdot}$} \\
				&= \expect{\mu_{\rv{X} | \rv{Y}}} \tag{\cref{def:conditional_operator}} \\
				&= \expect{\expect{\phiX | \rv{Y}}} \tag{\cref{def:conditional_embedding}} \\
				&= \expect{\phiX} \tag{tower property} \\
				&= \muX \tag{\cref{def:conditional_embedding}}
				\end{align*}
			\end{proof}
		\end{theorem}
		
		\begin{corollary} \label{thm:kernel_sum_rule_2}
			(Kernel Sum Rule Version 2)
			Another useful version of the kernel sum rule is the following \eqref{eq:kernel_sum_rule_2}, by simply replacing the distribution for $\rv{X}$ by the joint distribution of $\rv{X}$ and $\rv{X}$.
			
			\begin{equation}
				\Cxx = C_{\rv{X} \rv{X} | \rv{Y}} \muY
			\label{eq:kernel_sum_rule_2}
			\end{equation}
			
			This represents the following sum rule in usual probability space \eqref{eq:sum_rule_2}.
			
			\begin{equation}
				p_{\rv{X} \rv{X}}(x, x') = \int_{\mathcal{Y}} p_{\rv{X} \rv{X} | \rv{Y}}((x, x') | y) p_{\rv{y}}(y) dy
			\label{eq:sum_rule_2}
			\end{equation}
			
			where the joint probability of $\rv{X}$ with itself, conditioned or not, is degenerate in the sense that
			
			\begin{equation}
			\begin{aligned}
				p_{\rv{X} \rv{X}}(x, x') &= p_{\rv{X}}(x) \delta_{x}(x') \\
				p_{\rv{X} \rv{X} | \rv{Y}}((x, x') | y) &= p_{\rv{X} | \rv{Y}}(x | y) \delta_{x}(x')
			\end{aligned}
			\end{equation}
			
			where $\delta_{x}$ is the Dirac delta function centred at $x$. While this is essentially the same probability relation, the extra dimension allows one dimension to be kept even after marginalisation through a cross-covariance operator later.
		
		\end{corollary}
		
		\begin{corollary} \label{thm:kernel_sum_rule_3}
			(Kernel Sum Rule Version 3)
			Yet another useful version of the kernel sum rule is the following \eqref{eq:kernel_sum_rule_3}, by simply replacing the distribution for $\rv{X}$ by the joint distribution of $\rv{X}$ and $\rv{Y}$.
		
			\begin{equation}
				\Cxy = C_{\rv{X} \rv{Y} | \rv{Y}} \muY
			\label{eq:kernel_sum_rule_3}
			\end{equation}
			
			This represents the following sum rule in usual probability space \eqref{eq:sum_rule_3}.
			
			\begin{equation}
				p_{\rv{X} \rv{Y}}(x, y) = \int_{\mathcal{Y}} p_{\rv{X} \rv{Y} | \rv{Y}}((x, y) | y') p_{\rv{y}}(y') dy'
			\label{eq:sum_rule_3}
			\end{equation}
			
			where
			
			\begin{equation}
			p_{\rv{X} \rv{Y} | \rv{Y}}((x, y) | y') = p_{\rv{X} | \rv{Y}}(x| y) \delta_{y}(y')
			\end{equation}
		\end{corollary}
		
		This last operation \eqref{eq:kernel_sum_rule_3} represents a way to obtain joint distribution $p_{\rv{X} \rv{Y}}$ from a conditional distribution $p_{\rv{X} \rv{Y} | \rv{Y}}$, which contains the same information as $p_{\rv{X} | \rv{Y}}$, and a marginal distribution $p_{\rv{y}}$. This is in fact very close to what the chain rule of probability intends to do, which motivates the following.
	
		\begin{theorem} \label{thm:kernel_chain_rule}
			(Kernel Chain Rule)
			The Kernel Chain Rule \eqref{eq:kernel_chain_rule} is the kernelised and non-parametric version of the basic chain rule of probability \eqref{eq:chain_rule}.
			\begin{equation}
			\begin{aligned}
				p_{\rv{X} \rv{Y}}(x, y) &= p_{\rv{X} | \rv{Y}}(x | y) p_{\rv{Y}}(y)
			\label{eq:chain_rule}
			\end{aligned}
			\end{equation}	

			\begin{equation}
				\Cxy = \Cxly \Cyy
			\label{eq:kernel_chain_rule}
			\end{equation}
			
			\begin{proof}
				After switching the roles of $\rv{X}$ and $\rv{Y}$ in \cref{thm:conditional_operator}, left apply $\Cyy$ on both sides.
			\end{proof}
		\end{theorem}

		While the probability chain rule \eqref{eq:chain_rule} is the usual intended interpretation of kernel chain rule \eqref{eq:kernel_chain_rule}, in actuality the kernel chain rule \eqref{eq:kernel_chain_rule} stands for the following probability rule \eqref{eq:chain_rule_actual}, which is another application of the sum rule.

		\begin{equation}
		\begin{aligned}
			p_{\rv{X} \rv{Y}}(x, y) &= \int_{\mathcal{Y}} p_{\rv{X} | \rv{Y}}(x | y') p_{\rv{Y} \rv{Y}}(y', y) dy' \\
		\label{eq:chain_rule_actual}
		\end{aligned}
		\end{equation}	
					
		Notice that this is the case because cross-covariance operators necessarily marginalise away their last dimension (together with the first dimension of its operand) as an inner product due to how tensor product operators are defined in \cref{def:tensor_operator}. Therefore, the only effective rule of probability that can be represented through cross-covariance operations are the sum rules. The chain rule is only represented indirectly by the sum rule through introducing extra dimensions that stay after the marginalisation.

%		\begin{theorem} \label{thm:kernel_bayes_rule}
%			(Kernel Bayes Rule)
%			
%		\end{theorem}
	\subsection{Empirical Representation}
	\label{sec:kernel_embeddings:empirical_representation}
	
		In practice, the actual probability distributions of interest are not available in closed form. Instead, independent and identically distributed (\textit{iid}) samples from such probability distributions are available. Suppose that joint samples $\{x_{i}, y_{i}\}_{i = 1}^{n}$ are observed and collected in an \textit{iid} fashion from the joint distribution $\mathbb{P}_{\rv{X} \rv{Y}}$. Again, let $\ds{X} := \{x_{i}\}_{i = 1}^{n}$ and $\ds{Y} := \{y_{i}\}_{i = 1}^{n}$. It is possible to represent kernel embeddings empirically such that in the limit of infinite data, the empirical representations would converge to the true representations at an appropriate rate. Most significant, however, is the fact that important results for manipulating kernel embeddings also hold for their empirical representations.

		\begin{theorem} \label{thm:empirical_embedding}
			(Empirical Kernel Embedding)
			The empirical representation of a kernel embedding is obtained by simply replacing the expectation with an empirical average \eqref{eq:empirical_embedding}.
			\begin{equation}
			\begin{aligned}
				\hatmuX &= \frac{1}{n} \sum_{i = 1}^{n} \phi_{x_{i}} \\
				\hatmuY &= \frac{1}{n} \sum_{i = 1}^{n} \psi_{y_{i}}
			\label{eq:empirical_embedding}
			\end{aligned}
			\end{equation}
		\end{theorem}
	
		\begin{theorem} \label{thm:empirical_function_expectation}
			(Empirical Function Expectation)
			The empirical representation of a function expectation is simply the empirical average of the function, and can be obtained by the inner product of the function with the empirical embedding \eqref{eq:empirical_function_expectation}.
			\begin{equation}
				\inner{\hatmuX}{f} = \frac{1}{n} \sum_{i = 1}^{n} f(x_{i})
				\label{eq:empirical_function_expectation}
			\end{equation}
			
			\begin{proof}
				\begin{align*}
					\inner{\hatmuX}{f} &= \inner{\frac{1}{n} \sum_{i = 1}^{n} \phi_{x_{i}}}{f} \tag{\cref{thm:empirical_embedding}} \\
					&= \frac{1}{n} \sum_{i = 1}^{n} \inner{\phi_{x_{i}}}{f} \tag{bilinearity of $\inner{\cdot}{\cdot}$} \\
					&= \frac{1}{n} \sum_{i = 1}^{n} f(x_{i}) \tag{\cref{thm:reproducing_property}}
				\end{align*}
			\end{proof}
		\end{theorem}
		
		\begin{theorem} \label{thm:empirical_cross_cov}
			(Empirical Uncentred Cross-Covariance Operators)
			The empirical representation of a kernel embedding is obtained by simply replacing the expectation with an empirical average \eqref{eq:empirical_cross_cov}.
			\begin{equation}
			\begin{aligned}
				\hatCxx &= \frac{1}{n} \sum_{i = 1}^{n} \phi_{x_{i}} \otimes \phi_{x_{i}} \\
				\hatCxy &= \frac{1}{n} \sum_{i = 1}^{n} \phi_{x_{i}} \otimes \psi_{y_{i}} \\
				\hatCyx &= \frac{1}{n} \sum_{i = 1}^{n} \psi_{y_{i}} \otimes \phi_{x_{i}} \\
				\hatCyy &= \frac{1}{n} \sum_{i = 1}^{n} \psi_{y_{i}} \otimes \psi_{y_{i}}
			\label{eq:empirical_cross_cov}
			\end{aligned}
			\end{equation}
		\end{theorem}
		
		Since the samples $\ds{X} := \{x_{i}\}_{i = 1}^{n}$ and $\ds{Y} := \{y_{i}\}_{i = 1}^{n}$ are sampled from the joint distribution $\mathbb{P}_{\rv{X} \rv{Y}}$ such that marginal samples are simply $\ds{X}$ and $\ds{Y}$ themselves, the empirical representations for regular embeddings $\muX$ and $\muY$ are easy to obtain, as per \cref{thm:empirical_embedding}.
		
		However, instead of a uniform average, a empirical representations of the conditional embedding would require a weighted average of the following sform \eqref{eq:empirical_conditional_embedding_form} instead.
		
		\begin{equation}
		\begin{aligned}
			\hatmuYx &= \sum_{i = 1}^{n} \alpha_{i} \psi_{y_{i}} \\
			\hatmuXy &= \sum_{i = 1}^{n} \beta_{i} \phi_{x_{i}}
		\label{eq:empirical_conditional_embedding_form}
		\end{aligned}
		\end{equation}
		
		where $\bm{\alpha} := \{\alpha_{i}\}_{i = 1}^{n}$ is to be determined from the samples $\ds{X}$ and $\bm{\beta} := \{\beta_{i}\}_{i = 1}^{n}$ is to be determined from the samples $\ds{Y}$.
		
		Before deriving the expressions for $\bm{\alpha}$ and $\bm{\beta}$ however, it is useful to introduce matrix notation for representing empirical embeddings.
		
		The feature functions can be viewed as an feature vector of the same dimension as the cardinality of the its domain. For example, if the domain $\mathcal{X} = \mathbb{R}^{d}$ is the $d$ dimensional euclidean space whose cardinality is uncountably infinite, then the feature function can be viewed as an uncountably infinite dimensional feature vector, indexed by the elements of $\mathcal{X} = \mathbb{R}^{d}$.
		
		Denote the cardinality of $\mathcal{X}$ as $\cardX$, and similarly the cardinality of $\mathcal{Y}$ as $\cardY$. With the observations $\{x_{i}, y_{i}\}_{i = 1}^{n}$ sampled from $\mathbb{P}_{\rv{X} \rv{Y}}$, there are $n$ feature vectors for each RKHS, $\{\phi_{x_{i}}\}_{i = 1}^{n}$ for $\Hk$ and $\{\psi_{y_{i}}\}_{i = 1}^{n}$ for $\Hl$. As elements within their respective RKHS for which Hilbert-Schmidt operators can operate on, $\phi_{x_{i}}$ has an effective dimension of $\cardX \times 1$ and $\psi_{y_{i}}$ has an effective dimension of $\cardY \times 1$. Since this can be said about the feature functions, which form the basis for the RKHS, this also applies to general functions within the RKHS. As such, the inner product between functions $f_{1}, f_{2} \in \Hk$ can also be written in matrix notation \eqref{eq:inner_dot_notation}.
		
		\begin{equation}
			\inner{f_{1}}{f_{2}} = f_{1}^{T} f_{2}
		\label{eq:inner_dot_notation}
		\end{equation}
		
		It can be conceptually helpful to check that $f_{1}^{T}$ is of size $1 \times \cardX$ and $f_{2}$ is of size $\cardX \times 1$ so that the result is a scalar of size $1 \times 1$.

		Recall that a matrix $A := \begin{bmatrix} \bvec{a}_{1} & \cdots & \bvec{a}_{n} \end{bmatrix} \in \mathbb{R}^{n \times n}$, $\bvec{a}_{i} \in \mathbb{R}^{n} \; \forall i \in \{1, \dots, n\}$, operated on a vector $\bvec{v} \in \mathbb{R}^{n}$ results in a vector that is the linear combination of the columns of $A$ with coefficients given by the components of $\bvec{v}$ \eqref{eq:matrix_operation}.
		
		\begin{equation}
			A \bvec{v} = \begin{bmatrix} \bvec{a}_{1} & \cdots & \bvec{a}_{n} \end{bmatrix} \begin{bmatrix} v_{1} \\ \vdots \\ v_{n} \end{bmatrix} = \sum_{i = 1}^{n} v_{i} \bvec{a}_{i}
		\label{eq:matrix_operation}
		\end{equation}
		
		Similarly, a feature matrix can be defined in the same way such that empirical representations of kernel embeddings can be reduced down to linear algebraic operations.
		
		\begin{definition} \label{def:feature_matrix}
			(Feature Matrix)
			A feature matrix is formed by stacking the corresponding feature vectors \textit{horizontally}, where each feature vector represents a column of that matrix.
			
			\begin{equation}
			\begin{aligned}
				\Phi \equiv \Phi_{\ds{X}} &:= \begin{bmatrix} \phi_{x_{1}} & \cdots & \phi_{x_{n}} \end{bmatrix} \\
				\Psi \equiv \Psi_{\ds{Y}} &:= \begin{bmatrix} \psi_{y_{1}} & \cdots & \psi_{y_{n}} \end{bmatrix}
			\label{eq:feature_matrix}
			\end{aligned}
			\end{equation}
			
			Here, the subscripts of $\Phi$ and $\Psi$ are dropped whenever it is clear from context what dataset the feature matrix is representing.
			
			In this way, $\Phi$ has effective size $\cardX \times n$ and $\Psi$ has effective size $\cardY \times n$.			
		\end{definition} 
		
		\begin{definition}
		\label{def:gram_matrix}
			(Kernel Gram Matrix)
			The gram matrix $K_{\ds{X} \ds{X}}$ of a kernel $k$ for some dataset $\ds{X}$ is the matrix of all paired kernel evaluations between the data points \eqref{eq:gram_matrix}.
			\begin{equation}
			\begin{aligned}
				K \equiv K_{\ds{X} \ds{X}} &:= \{k(x_{i}, x_{j})\}_{i = 1, j = 1}^{n, n} \\
				L \equiv L_{\ds{Y} \ds{Y}} &:= \{l(y_{i}, y_{j})\}_{i = 1, j = 1}^{n, n}
			\label{eq:gram_matrix}
			\end{aligned}
			\end{equation}
			
			Similarly, the subscripts of $K$ and $L$ are dropped whenever it is clear from context what dataset the feature matrix is representing.
			
			Notationally, the subscripts can be very helpful when the kernel evaluations are not taken between all data points. Suppose there are two sets of collection of of points $\ds{X}^{1} := \{x_{i}^{1}\}_{i = 1}^{n}$ and $\ds{X}^{2} := \{x_{j}^{2}\}_{i = 1}^{m}$, where the superscripts differentiates between the datasets instead of the denoting the usual exponentiation. Then the kernel matrix between the two datasets is written as follows \eqref{eq:kernel_matrix_general}.
			
			\begin{equation}
				K_{\ds{X}^{1} \ds{X}^{2}} := \{k(x_{i}^{1}, x_{j}^{2})\}_{i = 1, j = 1}^{n, m} \in \mathbb{R}^{n \times m}
			\label{eq:kernel_matrix_general}
			\end{equation}
			
			If instead one of the datasets only contain one element, then the matrix reduces to either a column or row vector \eqref{eq:kernel_vector}.
	
			\begin{equation}
			\begin{aligned}
				K_{\ds{X}^{1} x} &:= \{k(x_{i}^{1}, x)\}_{i = 1}^{n} \in \mathbb{R}^{n \times 1} \\
				K_{x \ds{X}^{2}} &:= (\{k(x, x_{j}^{2})\}_{j = 1}^{m})^{T} \in \mathbb{R}^{1 \times m}
			\label{eq:kernel_vector}
			\end{aligned}
			\end{equation}
		\end{definition}
			
		\begin{theorem} \label{thm:kernel_matrix}
			(Kernel Gram Matrix)
			The gram matrices $K$ and $L$ for datasets $\ds{X}$ and $\ds{Y}$ can be found by their corresponding feature matrices \eqref{eq:kernel_matrix}.
			\begin{equation}
			\begin{aligned}
				\Phi_{\ds{X}}^{T} \Phi_{\ds{X}} = K_{\ds{X} \ds{X}} &\leftrightarrows \Phi^{T} \Phi = K \\
				\Psi_{\ds{Y}}^{T} \Psi_{\ds{Y}} = L_{\ds{Y} \ds{Y}} &\leftrightarrows \Psi^{T} \Psi = L
			\label{eq:kernel_matrix}
			\end{aligned}
			\end{equation}
			


			\begin{proof}
				Below is an informal but illustrative proof for the case for the kernel $k$. The proof for the kernel $l$ is exactly the same.
				\begin{align*}
					\Phi^{T} \Phi &= \begin{bmatrix} \phi_{x_{1}}^{T} \\ \vdots \\ \phi_{x_{n}}^{T} \end{bmatrix} \begin{bmatrix} \phi_{x_{1}} & \cdots & \phi_{x_{n}} \end{bmatrix} \tag{\cref{def:feature_matrix}}\\ 
					&= 	\begin{bmatrix}
							\phi_{x_{1}}^{T} \phi_{x_{1}} & \phi_{x_{1}}^{T} \phi_{x_{2}} & \cdots & \phi_{x_{1}}^{T} \phi_{x_{n}} \\
							\phi_{x_{2}}^{T} \phi_{x_{1}} & \phi_{x_{2}}^{T} \phi_{x_{2}} & \cdots & \phi_{x_{2}}^{T} \phi_{x_{n}} \\
							\vdots & \vdots & \ddots & \vdots \\
							\phi_{x_{n}}^{T} \phi_{x_{1}} & \phi_{x_{n}}^{T} \phi_{x_{2}} & \cdots & \phi_{x_{n}}^{T} \phi_{x_{n}} \\
						\end{bmatrix} \\
					&= 	\begin{bmatrix}
							k(x_{1}, x_{1}) & k(x_{1}, x_{2}) & \cdots & k(x_{1}, x_{n}) \\
							k(x_{2}, x_{1}) & k(x_{2}, x_{2}) & \cdots & k(x_{2}, x_{n}) \\
							\vdots & \vdots & \ddots & \vdots \\
							k(x_{n}, x_{1}) & k(x_{n}, x_{2}) & \cdots & k(x_{n}, x_{n}) \\
						\end{bmatrix} \tag{\cref{thm:reproducing_property}} \\
					&=: K
				\end{align*}
			\end{proof}
		\end{theorem}
		
		In this way, the embeddings can be written with the following notation.
		
		\begin{theorem} \label{thm:empirical_embedding_matrices}
		(Matrix Notation for Empirical Embeddings)
		Empirical embeddings can be written with matrix notation as the following \eqref{eq:empirical_conditional_embedding_matrices}.
		\begin{equation}
		\begin{aligned}
			\hatmuYx &= \sum_{i = 1}^{n} \alpha_{i} \psi_{y_{i}} = \Psi \bm{\alpha} \\
			\hatmuXy &= \sum_{i = 1}^{n} \beta_{i} \phi_{x_{i}} = \Phi \bm{\beta} \\
		\label{eq:empirical_conditional_embedding_matrices}
		\end{aligned}
		\end{equation}
		
		Note that for non-conditional, straight embeddings where the available samples are sampled from the corresponding distribution, the weights become uniform such that $\alpha_{i} = \frac{1}{n}$ and $\beta_{i} = \frac{1}{n}$ for $i \in \{1, \dots, n\}$ so that $\bm{\alpha} = \frac{1}{n} \bvec{1}$ and $\bm{\beta} = \frac{1}{n} \bvec{1}$, where $\bvec{1} := \{1\}_{i = 1}^{n}$ is a vector of ones \eqref{eq:empirical_embedding_matrices}.
		
		\begin{equation}
		\begin{aligned}
			\hatmuY &= \sum_{i = 1}^{n} \alpha_{i} \psi_{y_{i}} = \Psi \bm{\alpha} = \frac{1}{n} \Psi \bvec{1} \\
			\hatmuX &= \sum_{i = 1}^{n} \beta_{i} \phi_{x_{i}} = \Phi \bm{\beta} = \frac{1}{n} \Phi \bvec{1}
		\label{eq:empirical_embedding_matrices}
		\end{aligned}
		\end{equation}
		
		\begin{proof}
			Below is an informal but illustrative proof for $\hatmuXy$. The proof for $\hatmuYx$ is exactly the same.
			\begin{align*}
				\Phi \bm{\beta} &= \begin{bmatrix} \phi_{x_{1}} & \cdots & \phi_{x_{n}} \end{bmatrix} \begin{bmatrix} \beta_{1} \\ \vdots \\ \beta_{n} \end{bmatrix} \tag{\cref{def:feature_matrix}} \\
				&= \sum_{i = 1}^{n} \beta_{i} \phi_{x_{i}}
			\end{align*}
		\end{proof}
		\end{theorem}
		
		\begin{example} \label{ex:embedding_evaluation}
			(Embedding Evaluation)
			As an example of why this notation is useful, consider the evaluation of the conditional embedding of $\hatmuXy$ at $x$ directly.
			\begin{align*}
				\hatmuXy(x) &= \sum_{i = 1}^{n} \beta_{i} \phi_{x_{i}}(x) \tag{\cref{eq:empirical_conditional_embedding_form}} \\
				&= \sum_{i = 1}^{n} \beta_{i} k(x_{i}, x) \tag{\cref{thm:feature_functions}} \\
				&= K_{x \ds{X}} \bm{\beta} \tag{matrix multiplication}
			\end{align*}
			where $K_{x \ds{X}} := \{k(x, x_{i})\}_{i}^{n}$ is a row vector of size $1 \times n$.
			
			By the reproducing property from \cref{thm:reproducing_property}, this evaluation can also be written in the form of an inner product.
			\begin{align*}
				\hatmuXy(x) &= \inner{\phi_{x}}{\hatmuXy} \tag{\cref{thm:reproducing_property}}\\
				&= \inner{\phi_{x}}{\Phi \bm{\beta}} \tag{\cref{thm:empirical_embedding_matrices}}\\
				&= \phi_{x}^{T} \Phi \bm{\beta} \tag{matrix notation} \\
				&= K_{x \ds{X}} \bm{\beta} \tag{\cref{thm:kernel_matrix}}
			\end{align*}
			
			As such, empirical embeddings can be purely manipulated through linear algebra.
		\end{example}
		
		\begin{theorem} \label{thm:empirical_cross_cov_matrices}
			(Matrix Notation for Empirical Uncentred Cross-Covariance Operators)
			By writing tensor products of vectors as outer products, the uncentred cross-covariance operators can be written as follows.
			\begin{equation}
			\begin{aligned}
				\hatCxx &= \frac{1}{n} \Phi \Phi^{T} \\
				\hatCxy &= \frac{1}{n} \Phi \Psi^{T} \\
				\hatCyx &= \frac{1}{n} \Psi \Phi^{T} \\
				\hatCyy &= \frac{1}{n} \Psi \Psi^{T}
			\label{eq:empirical_cross_cov_matrices}
			\end{aligned}
			\end{equation}
		\end{theorem}
		
		\begin{definition} \label{def:empirical_conditional_operator}
			(Empirical Conditional Operator)
			The empirical conditional operator is defined by the same structure as that of the full conditional operator.
			\begin{equation}
				\hatCylx := \hatCyx (\hatCxx + \frac{1}{n} \lambda I_{\cardX})^{-1}
			\label{eq:empirical_conditional_operator_cross_cov}
			\end{equation}
			where $\lambda$ is a regularisation parameter and $I_{\cardX}$ is the identity operator, with the same effective size $\cardX \times \cardX$ as $\hatCxx$. There are multiple theoretical reasons for the introduction of this regularisation parameter in the literature. It is also unclear what type of regularisation technique is best suited in this case. In practice, however, this regularisation form is chosen to prevent overfitting on the dataset.
		\end{definition}
		
		\begin{theorem} \label{thm:woodbury}
			(Woodbury Theorem)
			A special case of the Woodbury Theorem states that for any linear operators $B \in \mathbb{R}^{n \times m}$ and $C \in \mathbb{R}^{m \times n}$ and identity operators $I_{n}$ and $I_{m}$, the following identity holds
			\begin{equation}
				B (CB + I_{m})^{-1} = (BC + I_{n})^{-1} B
			\label{eq:woodbury}
			\end{equation}
			Informally, this results also holds if one of $n$ and $m$ is not finite.
		\end{theorem}
		
		\begin{theorem} \label{thm:empirical_conditional_operator}
			(Empirical Conditional Operator)
			The empirical conditional operator is given by the following form \eqref{eq:empirical_conditional_operator}.
			\begin{equation}
				\hatCylx = \Psi (K + \lambda I_{n})^{-1} \Phi^{T}
			\label{eq:empirical_conditional_operator}
			\end{equation}
			
			It can be conceptually helpful to note that $\Psi$ has size $\cardY \times n$, $(K + \lambda I_{n})^{-1}$ has size $n \times n$, and $\Phi^{T}$ has size $n \times \cardX$, resulting in the appropriate size $\cardY \times \cardX$ for $\hatCylx$.

			\begin{proof}
				\begin{align*}
					\hatCylx &= \hatCyx \hatCxx^{-1} \tag{\cref{def:empirical_conditional_operator}}\\
					&= \frac{1}{n} \Psi \Phi^{T} (\frac{1}{n} \Phi \Phi^{T} + \frac{1}{n} \lambda I_{\cardX})^{-1} \tag{\cref{{thm:empirical_cross_cov_matrices}}} \\
					&= \Psi \Phi^{T} (\Phi \Phi^{T} + \lambda I_{\cardX})^{-1} \tag{$\frac{1}{n}$ cancels} \\
					&= \Psi (\Phi^{T} \Phi + \lambda I_{n})^{-1} \Phi^{T} \tag{\cref{thm:woodbury}} \\
					&= \Psi (K + \lambda I_{n})^{-1} \Phi^{T} \tag{\cref{thm:kernel_matrix}} \\
				\end{align*}
			\end{proof}
		\end{theorem}
		
		It can be helpful to view $\hatCylx$ as a representation of the conditional field. 
		
		\begin{theorem} \label{thm:empirical_conditional_embedding}
			(Empirical Conditional Embedding)
			To fix the conditioned variable to $x$, the empirical analogue of \cref{def:conditional_operator} is used.
			\begin{equation}
				\hatmuYx = \hatCylx \phi_{x} = \Psi (K + \lambda I_{n})^{-1} K_{\ds{X} x}
			\label{eq:empirical_conditional_embedding}
			\end{equation}
			where $K_{\ds{X} x} := \{k(x_{i}, x)\}_{i = 1}^{n}$ is a column vector of size $n \times 1$.
			To evaluate the empirical conditional embedding, the reproducing property is used once again.
			\begin{equation}
				\hatmuYx(y) = \psi_{y}^{T} \hatCylx \phi_{x} = L_{y \ds{Y}} (K + \lambda I_{n})^{-1} K_{\ds{X} x}
			\label{eq:empirical_conditional_embedding_evaluated}
			\end{equation}
			where $L_{y \ds{Y}} := \{l(y, y_{i})\}_{i = 1}^{n}$ is a row vector of size $1 \times n$.
		\end{theorem}
		
		\extra{
		\begin{theorem} \label{thm:empirical_kernel_sum_rule}
			(Empirical Kernel Sum Rule)
			The Empirical Kernel Sum Rule holds in an analogous form to the full Kernel Sum Rule in \cref{thm:kernel_sum_rule}.
			
			\begin{equation}
				\hatmuX = \hatCxly \hatmuY
			\label{eq:empirical_kernel_sum_rule}
			\end{equation}
			
			\begin{proof}
				For this proof, the regularisation parameter $\lambda$ from \cref{thm:empirical_conditional_operator} is omitted as it was mainly introduced for numerical and practical reasons.
				\begin{align*}
					\hatCxly \hatmuY &= \Phi L^{-1} \Psi^{T} \hatmuY \tag{\cref{thm:empirical_conditional_operator}} \\
					&= \hatCxly \hatmuY = \Phi L^{-1} \Psi^{T} \frac{1}{n} \Psi \bvec{1} \tag{\cref{thm:empirical_embedding}} \\
					&= \hatCxly \hatmuY =  \frac{1}{n} \Phi L^{-1} L \bvec{1} \tag{\cref{thm:kernel_matrix}} \\
					&=  \frac{1}{n} \Phi \bvec{1} \\
					&= \hatmuX \tag{\cref{thm:empirical_embedding}}
				\end{align*}
			\end{proof}
		\end{theorem}
		
		\begin{theorem} \label{thm:empirical_kernel_sum_rule_2}
			(Empirical Kernel Sume Rule Version 2) The empirical version of \cref{thm:kernel_sum_rule_2} is important for deriving kernel Bayes rule later on.
		\end{theorem}
		}
		\warn{Finish Later.}

\section{Derivations}

	\begin{theorem} \label{thm:function_projection_solution}
		(Optimal Function Projection)
		The function $f \in \mathcal{H}_{k}$ can be approximated with $\hat{f} \in \mathcal{H}_{k, \ds{X}}$ \eqref{eq:function_projection} so that they are close as measured by the norm induced by the full RKHS $\mathcal{H}_{k}$, by tuning the embedding weights $\bm{\alpha} = \{\alpha_{i}\}_{i = 1}^{n}$ \eqref{eq:function_projection_optimisation}.
		
		\begin{equation}
			\hat{f} = \sum_{i = 1}^{n} \alpha_{i} k(\bvec{x}_{i}, \cdot) = \sum_{i = 1}^{n} \alpha_{i} \phi_{\bvec{x}_{i}} = \Phi_{\ds{X}} \bm{\alpha}
		\label{eq:function_projection}
		\end{equation}

		The optimisation problem \eqref{eq:function_projection_optimisation} has a closed form solution \eqref{eq:function_projection_solution}, where $K \equiv K_{\ds{X} \ds{X}} := \{k(\bvec{x}_{i}, \bvec{x}_{j})\}_{i = 1, j = 1}^{n, n}$ is the gram matrix and $\bvec{f} := f\{\ds{X}\} := \{f(\bvec{x}_{i})\}_{i = 1}^{n}$ is the function evaluated at the observed points.
			
		\begin{equation}
			\bm{\alpha}^{\star} = \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert
		\label{eq:function_projection_optimisation}
		\end{equation}
		
		\begin{equation}
		\begin{aligned}
			\bm{\alpha}^{\star} &= K^{-1} \bvec{f} \\
			\hat{f} &= \Phi \bm{\alpha}^{\star} = \Phi K^{-1} \bvec{f}
		\label{eq:function_projection_solution}
		\end{aligned}
		\end{equation}
			
		\begin{proof}
			Since the square is a monotonic function for positive inputs, the objective function can be rewritten as follows.
			
			\begin{align*}
				\bm{\alpha}^{\star} &= \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert \tag{\cref{eq:function_projection_optimisation}} \\
				&= \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert^{2} \tag{monotonic transformation}
			\end{align*}
			
			Since this is a positive quadratic form, it is known from quadratic programming that this optimisation has one solution that is obtained by setting the objective's gradient to zero. There is no need to check for the Hessian to distinguish between minimum and maximum, as the quadratic is convex and not concave. Expanding the norm as an inner product through its definition in the Hilbert space results in the following.

			\begin{align*}
				\Vert \hat{f} - f \Vert^{2} &= \langle \hat{f} - f, \hat{f} - f \rangle \tag{Hilbert norm} \\
				&= \langle \hat{f}, \hat{f} \rangle - 2 \langle \hat{f}, f \rangle + \langle f, f \rangle \tag{bilinearity of $\inner{\cdot}{\cdot}$} \\
				&=  \hat{f}^{T} \hat{f} - 2 \hat{f}^{T} f + f^{T} f \tag{\cref{eq:inner_dot_notation}} \\
				&= (\Phi_{\ds{X}} \bm{\alpha})^{T} \Phi_{\ds{X}}  \bm{\alpha} - 2 (\Phi_{\ds{X}}  \bm{\alpha})^{T} f + f^{T} f \tag{\cref{eq:function_projection}}\\
				&= \bm{\alpha}^{T} \Phi_{\ds{X}} ^{T} \Phi_{\ds{X}}  \bm{\alpha} - 2 \bm{\alpha}^{T} \Phi_{\ds{X}} ^{T} f + f^{T} f \\
				&= \bm{\alpha}^{T} K_{\ds{X} \ds{X}}  \bm{\alpha} - 2 \bm{\alpha}^{T} \bvec{f}\{\ds{X}\} + f^{T} f \tag{\cref{thm:kernel_matrix}}\\
			\end{align*}
			
			Differentiating the above results in the following.
			
			\begin{align*}
				\frac{d}{d \bm{\alpha}} \Vert \hat{f} - f \Vert^{2} &= \frac{d}{d \bm{\alpha}} 
				\bigg[ \bm{\alpha}^{T} K_{\ds{X} \ds{X}}  \bm{\alpha} - 2 \bm{\alpha}^{T} \bvec{f}\{\ds{X}\} + f^{T} f \bigg]\\
				&= 2 K_{\ds{X} \ds{X}}  \bm{\alpha} - 2 \bvec{f}\{\ds{X}\}
			\end{align*}
			
			Setting the above to zero reveals the solution.
			
			\begin{align*}
				\frac{d}{d \bm{\alpha}} \Vert \hat{f} - f \Vert^{2} &= 0 \\
				2 K_{\ds{X} \ds{X}}  \bm{\alpha}^{\star} - 2 \bvec{f}\{\ds{X}\} &= 0 \\
				K_{\ds{X} \ds{X}}  \bm{\alpha}^{\star} = \bvec{f}\{\ds{X}\} \\
				\bm{\alpha}^{\star} = K_{\ds{X} \ds{X}}^{-1} \bvec{f}\{\ds{X}\}
			\end{align*}			
		\end{proof}
	\end{theorem}
		\warn{Finish Later.}

\end{document}
