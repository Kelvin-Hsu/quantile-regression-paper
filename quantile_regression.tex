\documentclass[twoside]{article} \usepackage{aistats2017}

% If your paper is accepted, change the options for the package
% aistats2017 as follows:
%
%\usepackage[accepted]{aistats2017}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

\bibliographystyle{apalike}  % Use the "unsrtnat" BibTeX style for formatting the Bibliography
\usepackage[square, authoryear, comma, sort&compress]{natbib}  % Use the "Natbib" style for the references in the Bibliography

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathbbol}
\usepackage{mathtools}
\usepackage{mathrsfs}     
\usepackage{bm}           
\usepackage{vector}       
\usepackage[usenames, dvipsnames]{color}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
%\newcommand{\rv}[1]{\underline{\overline{#1}}}
\newcommand{\rv}[1]{\underline{#1}}
\newcommand{\ds}[1]{{#1}}
\newcommand{\warn}[1]{{\color{RedOrange} #1}}
\newcommand{\extra}[1]{{\color{ForestGreen} #1}}
\newcommand{\qpi}{QPI }

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Quantile Regression with Kernel Embeddings}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
  The Abstract paragraph should be indented 0.25 inch (1.5 picas) on
  both left and right-hand margins. Use 10~point type, with a vertical
  spacing of 11~points. The {\bf Abstract} heading must be centered,
  bold, and in point size 12. Two line spaces precede the
  Abstract. The Abstract must be limited to one paragraph.
\end{abstract}

\warn{Red lines are notes to pay attention to. They are not part of the paper.}

\extra{Green lines are sections that we are considering to delete. They are part of the paper.}

\section{Introduction}
		
	
	\warn{Why quantile regression? What are the advantages of quantile regression over regular regression?}
	
	Understanding risk is critical to making decisions based on uncertain information. Computing uncertainty through the use of probability, and then bounding that uncertainty with quantile estimation is one approach to this problem. Building from our work in chapter three, we now aim to extend our non-parametric, multi-modal techniques for regression to include cumulative and quantile estimation. With these additions, our work can be applied to problems with unknown prior and likelihood and produce not only probabilistic estimations, but also quantify the risk associated with decisions that rely on them.
	
	\warn{Not done yet. Come back later.}
	
\section{Related Work}

	Quantile regression was first introduced by \cite{Koenker1978}. This work considered the regression problem of inferring values of an unknown linea model given noisy samples of inputs and outputs. Rather than using the mean as an estimator of the function, Koenker chose the 0.5 quantile (the median), on the basis that it was less sensitive to outliers, and hence effective in cases with non-Gaussian or heavy tailed noise. A general regression quantile was then defined as the function value minimising the pinball loss \citep{Koenker1978}. For linear models, the minimisation was formulated as a linear programming problem and so solved efficiently.

	The pursuit of conditional quantiles quickly divided the field into so-called discriminative methods, that attempt to directly compute the quantile from the training data through minimisation of the pinball loss, and generative methods, that attempt to first model the conditional cumulative distribution, and from there derive quantile estimates \citep{Koenker2005}.

	The discriminative initially demonstrated the advantage of flexibility --- non-parametric function estimation techniques could be applied that made no assumptions about the underlying distribution. Examples include locally-constant and locally-linear approximations \citep{Chaudhuri1991, Yu1998}. However, this flexibility also led to problems such as quantile crossing; in which a data point might be considered to be below the 0.5 quantile but above the 0.6 quantile\cite{Koenker2005}. 

	An important development was the addition of non-crossing constraints to discriminative quantile regression solutions \citep{He1997}. This ensured that different quantile estimates obeyed a strict ordering, with no two quantiles having the same function value. Such a strict ordering was achieved by adding a penalty term to the optimisation \citep{Cole1992}, or by reducing the class of possible functions used to represent the quantiles, for instance to classes of location-scale models \citep{Koenker1984, He1997}. Unfortunately, such models implicitly restricted the underlying distribution, causing reduced applicability to data that did not fit the assumptions of the model \citep{Koenker2005}.

	Standard machine learning techniques have also been co-opted for conditional quantile estimation, including the support vector machine (SVM), which finds the decision boundary associated with the minimisation of the pinball loss, with additional non-crossing constraints \citep{Takeuchi2006}. However, in this algorithm, the non-crossing constraints may cause the resulting estimates to violate the (empirical) quantile definition.

	More recently, Chernozhukov et. al. developed a monotisation procedure based on function re-arrangement to remove crossing from quantile estimates generated by other algorithms. The resulting quantiles were guaranteed to be more accurate, and no assumptions were made about the underlying distribution \citep{Chernozhukov2010}.

	The `quanting' algorithm \citep{Langford2006} introduced an important development in discriminative estimation of conditional quantiles, which was to re-cast quantile regression as a classification problem. By placing a set of classifiers over the range of the regression, each classifier can be trained on whether the quantile is above or below it. The quantile estimate is then the expectation of this assignment, over all the classifiers.  A set of importance weights were learned to minimise the error of the estimation.

	Generative models for conditional quantile regression have also been examined in the semi-parametric and non-parametric setting. Linear models for Bayesian quantile estimation were examined in \cite{Yu2001}, using Laplacian likelihood functions and uniform priors. These methods require Markov chain Monte Carlo (MCMC) integration to perform inference, as well as making the uni-modal assumption. Similar semi-parametric methods such as \cite{Hjort2006, Hjort2009} use Dirichlet process priors, which again required costly inference approximations.

	Kernel conditional quantiles using Gaussian process regression to explicitly compute the cumulative distribution were examined by \cite{Quadrianto2009}. The underlying PDF was estimated using a Gaussian process, which enforced the non-crossing constraint and allowed for efficient inference. Additionally, heteroskedastic covariance functions were employed to account for input-dependent noise in the data. One limitation of the work was that the underlying Gaussianity assumption restricts applicability to uni-modal distributions.

	A more general formulation of Bayesian quantile regression was developed in \cite{Taddy2010}, which used Dirichlet process mixture models to represent the underlying joint distribution. This allowed a very general class of distributions to be represented, but inference required expensive MCMC integration.

% Indicator co-Kriging in a non-parametric approach in the geo-statistics
% literature based on explicit modelling of the underlying distribution through
% Kriging~\cite{Pardo2005}.

% check chen and muller 2012 for a nonparametric CDF
% approach~\cite{Chen2012} 

	Our work aims to overcome limitations of both these methods-- giving the flexibility of a mixture distribution representation for modelling multi-modal behaviour, with the efficient inference inherent in the MKBR algorithm. It is to this new algorithm we now turn our attention.

	\warn{Need to talk about recent advances in quantile regression since 2014. Need to go back to this literature review and update it too.}
	
\section{Background}
	
		Let $(\Omega, \mathcal{W}, \mathbb{P})$ be a probability space, where $\Omega$ is the sample space, $\mathcal{W}$ is a $\sigma$-algebra on $\Omega$, and $\mathbb{P} : \mathcal{W} \mapsto [0, 1]$ a probability measure. The elements $W$ of $\mathcal{W}$, that is, the measurable sets, are called events, which are by definition subsets of $\Omega$.
		
		The \textit{distribution} of a random variable $\rv{X} : \Omega \mapsto \mathcal{X}$ is the Borel measure $\mathbb{P}_{\rv{X}} : \mathcal{A} \mapsto [0, 1]$ defined by
		\begin{equation}
			\mathbb{P}_{\rv{X}}[A] := \mathbb{P}[\rv{X} \in A]
		\label{eq:distribution}
		\end{equation}
		where the event $\{\rv{X} \in A\} := \rv{X}^{-1}[A] := \{\omega \in \Omega : \rv{X}(\omega) \in A\} \in \mathcal{W}$ is the inverse image (pre-image) of $A \subseteq \mathcal{X}$ under the random variable $\rv{X}$, and $\mathcal{A}$ is a $\sigma$-algebra of subsets on $\mathcal{X}$. \extra{Recall that a random variable (in this case, $\rv{X}$) is formally a map from the sample space (in this case, $\Omega$) to some Borel measurable domain (in this case, $\mathcal{X}$) of interest. The \textit{distribution} is also a probability measure itself, as it satisfies the defining properties of a probability measure, such that $(\mathcal{X}, \mathcal{A}, \mathbb{P}_{\rv{X}})$ is also a probability space. It differs from $\mathbb{P}$ in that it operates in a different domain - not directly on the events, but on the values the random variable takes for those events.}
		
		The \textit{cumulative distribution function} (CDF) $P_{\rv{X}} : \mathbb{R} \mapsto [0, 1]$ of a \textit{real-valued} random variable $\rv{X} : \Omega \mapsto \mathbb{R}$ (that is, take $\mathcal{X}$ to be $\mathbb{R}$) is defined by
		\begin{equation}
			P_{\rv{X}}(x) := \mathbb{P}_{\rv{X}}[(-\infty, x]] = \mathbb{P}[\rv{X} \leq x]
		\label{eq:cdf}
		\end{equation}
		where $\{\rv{X} \leq x\} := \{\rv{X} \in (-\infty, x]\}$ is a shorthand.

		Similarly, if we take a \textit{vectored-valued} random variable $\bvec{\rv{X}} : \Omega \mapsto \mathbb{R}^{d}$, then 
		\begin{equation}
			P_{\bvec{\rv{X}}}(\bvec{x}) := \mathbb{P}_{\bvec{\rv{X}}}[(-\infty, x_{1}] \times \dots \times (-\infty, x_{d}]] = \mathbb{P}[\bvec{\rv{X}} \leq \bvec{x}]
		\label{eq:cdf_vector}
		\end{equation}
		where $\{\bvec{\rv{X}} \leq \bvec{x}\} := \bigcap\limits_{j = 1}^{d} \{\rv{X}_{j} \in (-\infty, x_{j}]\} \in \mathcal{W}$ is a shorthand. From here on we also define a shorthand $(-\bvec{\infty}, \bvec{x}] := (-\infty, x_{1}] \times \dots \times (-\infty, x_{d}]$ for conciseness.
				
%		Note that in most literature the cumulative distribution function would be denoted by $F_{X}$ and not $P_{X}$. However, with a lot of different spaces and variables to introduce later on, we will reserve $F$ and consequently $f$ for other quantities later.
		
		If the distribution $\mathbb{P}_{\rv{X}}$ is absolutely continuous with respect to the Lebesgue measure on $\mathcal{X} = \mathbb{R}^{d}$, then the Radon-Nikodym theorem implies the existence of a density for $\mathbb{P}_{\rv{X}}$. We call this the distribution density of $\rv{X}$, or more commonly known as the \textit{probability density function} (PDF), denoted by $p_{\rv{X}}: \mathcal{X} \mapsto [0, \infty)$.
		
		Let $f : \mathcal{X} \mapsto \mathbb{R}$ be some arbitrary function over the domain, then we have
		\begin{equation}
			\mathbb{E}[f(\rv{X})] = \int_{\Omega} f \circ \rv{X} d \mathbb{P} = \int_{\mathcal{X}} f d \mathbb{P}_{\rv{X}}
		\label{eq:functional_expectation}
		\end{equation}
		
		Recall that $\mathbb{P}[W] = \mathbb{E}[\mathbb{1}[W]] \;\; \forall W \in \mathcal{W}$, so that the CDF for a vector-valued random variable $\bvec{\rv{X}}$ is
		\begin{equation}
			\begin{aligned}
				P_{\bvec{\rv{X}}}(\bvec{x}) &= \mathbb{P}[\bvec{\rv{X}} \leq \bvec{x}] = \mathbb{E}[\mathbb{1}[\bvec{\rv{X}} \leq \bvec{x}]] = \mathbb{E}[\mathbb{1}_{(-\bvec{\infty}, \bvec{x}]}(\bvec{X})] \\
				&= \int_{\Omega} \mathbb{1}_{(-\bvec{\infty}, \bvec{x}]} \circ \bvec{\rv{X}} d \mathbb{P} = \int_{\mathbb{R}^{D}} \mathbb{1}_{(-\bvec{\infty}, \bvec{x}]} d \mathbb{P}_{\bvec{\rv{X}}} \\
				&= \int_{(-\bvec{\infty}, \bvec{x}]} d \mathbb{P}_{\bvec{\rv{X}}}
			\end{aligned}
		\label{eq:cdf_integral}
		\end{equation}
		where we use the common notation $\mathbb{1}_{A}(x) := \mathbb{1}[x \in A]$.
		
	\subsection{Kernel Embedding}
	
		In this paper we assume all kernels we deal with are positive definite and characteristic. That is, the kernel $k : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ defines a unique reproducing kernel Hilbert space (RKHS), which we will denote by $\mathcal{H}_{k}$. \extra{Note that in some literature, since $k$ takes elements $x$ of $\mathcal{X}$ as arguments, the corresponding reproducing kernel Hilbert space is denoted as $\mathcal{H}_{\mathcal{X}}$ instead. However, since the kernel $k$ also includes information about the space $\mathcal{X}$ it operates on, the notation $\mathcal{H}_{k}$ will be preferred.}
		 
		The kernel embedding $\mu_{\rv{X}}$ of a distribution measure $\mathbb{P}_{\rv{X}}$ under kernel $k$ is defined by
		\begin{equation}
			\mu_{\rv{X}}(x) := \mathbb{E}[k(\rv{X}, x)] = \int_{\mathcal{X}} k(\cdot, x) d \mathbb{P}_{\rv{X}}
		\label{eq:basic_embedding}
		\end{equation}
		\extra{where it is understood that the $\cdot$ is a place-holder for an argument such that $k(\cdot, x) : \mathcal{X} \mapsto \mathbb{R}$ is a function and not just a value.}
		
		We can also write the definition equivalently as
		\begin{equation}
			\mu_{\rv{X}} \equiv \mu_{\rv{X}}(\cdot) := \mathbb{E}[k(\rv{X}, \cdot)]
		\label{eq:basic_embedding_alternative}
		\end{equation}
		\extra{where it is understood that the $\cdot$ here in the last equation is for a different argument than the previous equations.}
		 
		\extra{In some literature, to make explicit of the fact that the kernel embedding is an embedding of a distribution measure $\mathbb{P}_{\rv{X}}$, the kernel embedding is instead denoted by $\mu_{\mathbb{P}_{\rv{X}}}$. We will not do this here in general here. Since the specification of a random variable $\rv{X}$ and its distribution $\mathbb{P}_{\rv{X}}$ must come together, either notation is equally informative.}
		 
		Usually, the kernel $k$ comes from a family of kernel functions with a set of hyperparameters $\bm{\theta}$. To make this relationship explicit, we may attach a subscript to the notation as $k_{\bm{\theta}}$. Correspondingly, the kernel embedding would then be denoted as $\mu_{\rv{X}, \bm{\theta}}$.

		\extra{It would indeed be more precise to put a subscript $k$ on the kernel embedding, as $\mu_{\rv{X}, k}$ or $\mu_{\rv{X}, k_{\bm{\theta}}}$, to make clear that it was embedded with kernel $k$. However, as we will not be changing the kernel much except up to some hyperparameter tuning, this will just be implicitly assumed.}
			 
		\extra{The kernel embeddings of Cartesian products of random variables over the same event space $\Omega$ define uncentred covariance and cross-covariance operators. Note that most of the important operations regarding uncertred versions also hold for centred versions.}
		 
		\warn{This could be more detailed but space is really limited. Perhaps just a short paragraph introducing cross-covariance operators and also conditional operators and reference a previous paper. However, in the supplementary material we will have to make consistent the notation.}

		
	\subsection{Quantiles and Quantile Regression}
	
		Consider a real-valued random variable $\rv{X} : \Omega \mapsto \mathbb{R}$ with distribution $\mathbb{P}_{X}$ and cumulative distribution function $P_{\rv{X}}$. The $\tau$-quantile, $\tau \in (0, 1)$, of $\mathbb{P}_{\rv{X}}$ is defined as
		\begin{equation}
			q_{\rv{X}}(\tau) = \inf\{x : P_{\rv{X}}(x) \geq \tau\} \equiv \inf_{P_{\rv{X}}(x) \geq \tau} x
		\label{eq:quantile}
		\end{equation}
		
		A quantile of a conditional distribution $P_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}$ of is defined similarly. This paper considers regression problems with multiple inputs and single output, such that $\mathcal{X} = \mathbb{R}^{d}$ and $\mathcal{Y} = \mathbb{R}$.
		
		\begin{equation}
			q_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(\tau) = \inf\{y : P_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(y) \geq \tau\} \equiv \inf_{P_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(y) \geq \tau} y
		\label{eq:conditional_quantile}
		\end{equation}
		
		Clearly, finding the conditional quantile is the exact same problem as finding a regular quantile, except with the CDF replaced by a conditional CDF. As such, for brevity and simpleness, in this paper we will present CDF estimation from a general embedding with general weights, whose form both conditional embeddings and regular embeddings share.
		
		Quantile regression is then the problem of finding an empirical estimate $\hat{q}_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(\tau)$ for $q_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(\tau)$ given a set of paired observations $\{(\bvec{x}_{i}, y_{i})\}_{i = 1}^{n}$ and a query point $\bvec{x}$.
		
		In practice, it is sufficient to replace the infimum with the minimum if the CDF estimate is smooth. Since the CDF is always non-decreasing, the inequality becomes an equality, and obtaining quantiles becomes a root finding procedure once an estimate of the CDF is obtained \eqref{eq:quantile_root_finding}. In this paper, we present methods for estimating the CDF from a dataset. 
		
		\begin{equation}
			q_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(\tau) = y : P_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(y) = \tau
		\label{eq:quantile_root_finding}
		\end{equation}	
	
		\warn{Do we need to introduce the pinball loss and the basics of quantile regression?}
	
\section{Discriminative Quantile Regression}

	The discriminative approach to quantile regression refers to techniques which computes an estimate of the CDF directly, without needing to compute the PDF first.
	
	The central idea here revolves around the fact that the probability measure of a set is the expected indicator value of that set for a given random vector $\bvec{\rv{X}}$, and the expected value of a function can be written as the inner product between the kernel embedding and the function \eqref{eq:distribution_inner}.
	
	\begin{equation}
		\mathbb{P}_{\bvec{\rv{X}}}[A] = \mathbb{E}[\mathbb{1}_{A}(\bvec{\rv{X}})] = \langle \mu_{\rv{X}}, \mathbb{1}_{A} \rangle
	\label{eq:distribution_inner}
	\end{equation}
	
	As the CDF $P_{\rv{X}}$ is defined through the distribution $\mathbb{P}_{\rv{X}}$ \eqref{eq:cdf}, it becomes straightforward to obtain the CDF as an inner product between the kernel embedding and the corresponding indicator function. \eqref{eq:cdf_inner}.
	
	\begin{equation}
		P_{\bvec{\rv{X}}}(\bvec{x}) = \mathbb{P}_{\bvec{\rv{X}}}[(-\bvec{\infty}, \bvec{x}]] = \langle \mu_{\bvec{\rv{X}}}, \mathbb{1}_{(-\bvec{\infty}, \bvec{x}]} \rangle
	\label{eq:cdf_inner}
	\end{equation}
	
	We will now focus our attention on how to estimate the distribution and CDF from a given dataset.
	
	\subsection{Optimal Empirical Projection for Function Approximation}

		In order to estimate the distribution and CDF empirically, we would need both the empirical kernel embedding $\hat{\mu}_{\rv{X}}$ and a projection $\hat{\mathbb{1}}_{A}$ of the indicator function $\mathbb{1}_{A}$ into the same RKHS as $\hat{\mu}_{\rv{X}}$ so that a proper inner product within that RKHS can be evaluated.

		Suppose $\ds{X} := \{\bvec{x}_{1}, \dots, \bvec{x}_{n}\} \in \mathbb{R}^{n \times d}$ is set of observations from $\mathbb{P}_{\rv{X}}$. The embeddings that can be represented by these samples are linear combinations of the feature functions $\phi_{\bvec{x}_{i}}$ at the observations \eqref{eq:function_projection}. The span obtained by those basis $\{\phi_{\bvec{x}_{i}}\}_{i = 1}^{n}$ also forms a RKHS, denoted by $\mathcal{H}_{k, \ds{X}}$, and is a finite vector subspace of the full RKHS $\mathcal{H}_{k}$.
		
		\begin{equation}
			\hat{f} = \sum_{i = 1}^{n} \alpha_{i} k(\bvec{x}_{i}, \cdot) = \sum_{i = 1}^{n} \alpha_{i} \phi_{\bvec{x}_{i}} = \Phi_{\ds{X}} \bm{\alpha}
		\label{eq:function_projection}
		\end{equation}
	
		We are interested in approximating the function $f \in \mathcal{H}_{k}$ with $\hat{f} \in \mathcal{H}_{k, \ds{X}}$ so that they are close as measured by the norm induced by the full RKHS $\mathcal{H}_{k}$, by tuning the embedding weights $\bm{\alpha} = \{\alpha_{i}\}_{i = 1}^{n}$ \eqref{eq:function_projection_optimisation}.
			
		\begin{equation}
			\bm{\alpha}^{\star} = \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert
		\label{eq:function_projection_optimisation}
		\end{equation}
		
		This optimisation problem has a closed form solution \eqref{eq:function_projection_solution}, where $K_{\ds{X} \ds{X}} := \{k(\bvec{x}_{i}, \bvec{x}_{j})\}_{i = 1, j = 1}^{n, n}$ is the gram matrix and $f\{\ds{X}\} := \{f(\bvec{x}_{i})\}_{i = 1}^{n}$ is the function evaluated at the observed points (see supplementary material).
		
		\begin{equation}
			\begin{aligned}
				\bm{\alpha}^{\star} &= K_{\ds{X} \ds{X}}^{-1} f\{\ds{X}\} \\
				\hat{f} = \Phi_{\ds{X}} \bm{\alpha}^{\star} &= \Phi_{\ds{X}} K_{\ds{X} \ds{X}}^{-1} f\{\ds{X}\}
			\end{aligned}
		\label{eq:function_projection_solution}
		\end{equation}
		
	\subsection{Optimal Empirical Distribution}
				
		Letting $f = \mathbb{1}_{A}$, we can find the projection of an indicator function into $\mathcal{H}_{k, \ds{X}}$ \eqref{eq:indicator_projection}.
		
		\begin{equation}
			\hat{\mathbb{1}}_{A} = \Phi_{\ds{X}} K_{\ds{X} \ds{X}}^{-1} \mathbb{1}_{A}\{\ds{X}\}
		\label{eq:indicator_projection}
		\end{equation}
		
		The empirical distribution \eqref{eq:empirical_distribution} and CDF \eqref{eq:empirical_cdf} can then be derived.
		
		\begin{equation}
		\begin{aligned}
			\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] &= \langle \hat{\mu}_{\bvec{\rv{X}}}, \hat{\mathbb{1}}_{A} \rangle = \hat{\mu}_{\bvec{\rv{X}}}^{T}\hat{\mathbb{1}}_{A} = (\Phi_{\ds{X}} \bm{\beta})^{T} \Phi_{\ds{X}} K_{\ds{X} \ds{X}}^{-1} \mathbb{1}_{A}\{\ds{X}\} \\
			&= \bm{\beta}^{T} \Phi_{\ds{X}}^{T} \Phi_{\ds{X}} K_{\ds{X} \ds{X}}^{-1} \mathbb{1}_{A}\{\ds{X}\} = \bm{\beta}^{T} K_{\ds{X} \ds{X}} K_{\ds{X} \ds{X}}^{-1} \mathbb{1}_{A}\{\ds{X}\} \\
			&= \bm{\beta}^{T} \mathbb{1}_{A}\{\ds{X}\} = \sum_{i = 1}^{n} \beta_{i} \mathbb{1}_{A}(\bvec{x}_{i}) = \sum_{\bvec{x}_{i} \in A} \beta_{i}
		\end{aligned}
		\label{eq:empirical_distribution}
		\end{equation}
		
		\begin{equation}
			\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) := \hat{\mathbb{P}}_{\bvec{\rv{X}}}[(-\bvec{\infty}, \bvec{x}]] = \sum_{\bvec{x}_{i} \in (-\bvec{\infty}, \bvec{x}]} \beta_{i} = \sum_{\bvec{x}_{i} \leq \bvec{x}} \beta_{i}
		\label{eq:empirical_cdf}
		\end{equation}
		
		There is however a subtlety in this derivation. It is not guaranteed that the indicator function is an element of the full RKHS $\mathcal{H}_{k}$. Nevertheless, \cite{Kanagawa:RecoveringDistributions} showed that the estimator $\langle \hat{\mu}_{\bvec{\rv{X}}}, \hat{f} \rangle$ is consistent for $\mathbb{E}[f(\bvec{\rv{X}})]$ if $f$ is in the Besov space, which is a superset of the full RKHS $\mathcal{H}_{k}$. While indicator functions are included in the Besov space only under certain assumptions, the estimators we derived for its expectation, by assuming that it is contained in the RKHS, empirically converges to the actual expectation.
		
		\begin{figure}
			\begin{center}
				\includegraphics[width=\columnwidth]{figures/cumulativeexampleragged}
			\end{center}
			\caption{\small (Left) A MKBR estimate of the PDF from the Motorcycle dataset. The blue line indicates the PDF, and the red bars indicate the weights of training points at those $X$ locations. (Right) The resulting CDF estimate using \eqref{eq:empirical_cdf}. Note that the estimate jumps at locations corresponding to training points. \warn{Figure not referenced in text. Caption not updated.}}
			\label{fig:cumulativeragged} 
		\end{figure}
			
	\subsection{Smooth Empirical Distribution}
	
%		\warn{The good news for this section is that the function we are taking an expectation of is now in the RKHS. The bad news is that it is no longer a proper indicator function. However, empirically, it works well. Also, unlike the previous part where the resulting estimator coincides with the Motonobu's paper, this part is completely novel.}
		
		By embedding indicator functions, the resulting CDF estimate \eqref{eq:empirical_cdf} is discontinuous at the jumps located at each data point. Directly differentiating this would result in a superposition of Dirac deltas for the PDF. However, in many applications, the PDF is rather smooth, and thus so is the CDF. Many characteristic kernels are also smooth, and thus so are the functions within their RKHS. Therefore, instead of taking the inner product between the embedding and the indicator function, we propose to take the inner product with a function $\tilde{\mathbb{1}}_{A}$ \eqref{eq:indicator_smooth} that is within the full RKHS $\mathcal{H}_{k}$ and converges to the indicator function $\mathbb{1}_{A}$ as the kernel bandwidth goes to zero.
		
		\begin{equation}
			\tilde{\mathbb{1}}_{A} := \int_{A} k(\bvec{x}, \cdot) d\bvec{x} = \int_{A} \phi_{\bvec{x}} d\bvec{x}
		\label{eq:indicator_smooth}
		\end{equation}
		
		This results in a distribution estimate \eqref{eq:smooth_empirical_distribution} and CDF estimate \eqref{eq:smooth_empirical_cdf} that are smooth.

		\begin{equation}
		\begin{aligned}
			\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] &= \langle \hat{\mu}_{\bvec{\rv{X}}}, \tilde{\mathbb{1}}_{A} \rangle = \hat{\mu}_{\bvec{\rv{X}}}^{T} \tilde{\mathbb{1}}_{A} = (\Phi_{\ds{X}} \bm{\beta})^{T} \int_{A} \phi_{\bvec{x}} d\bvec{x} \\
			&= \bm{\beta}^{T} \Phi_{\ds{X}}^{T} \int_{A} \phi_{\bvec{x}} d\bvec{x} = \int_{A} \bm{\beta}^{T} \Phi_{\ds{X}}^{T} \phi_{\bvec{x}} d\bvec{x} \\
			&= \int_{A} \bm{\beta}^{T} K_{\ds{X} \bvec{x}} d\bvec{x} = \int_{A} \sum_{i = 1}^{n} \beta_{i} k(\bvec{x}_{i}, \bvec{x}) d\bvec{x} \\
			&= \sum_{i = 1}^{n} \beta_{i} \int_{A}  k(\bvec{x}_{i}, \bvec{x}) d\bvec{x} \\
		\end{aligned}
		\label{eq:smooth_empirical_distribution}
		\end{equation}	
		
		\begin{equation}
		\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) := \hat{\mathbb{P}}_{\bvec{\rv{X}}}[(-\bvec{\infty}, \bvec{x}]] = \sum_{i = 1}^{n} \beta_{i} \int_{(-\bvec{\infty}, \bvec{x}]}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}'
		\label{eq:smooth_empirical_cdf}
		\end{equation}
		
		The estimates are expressed as a summation of the kernel integrals, which would need to be analytically derived or otherwise estimated. For a Gaussian kernel, analytical form can be derived \eqref{eq:smooth_empirical_cdf_gaussian_kernel}. Even if analytical form cannot be derived for some kernels $k$, since the integral is independent of the embedding weights, they can be approximated and pre-computed before inference begins.

		\begin{equation}
			\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) = \frac{1}{2} \sum_{i = 1}^{n} \beta_{i} \prod_{j = 1}^{d} \Bigg[1 + \mathrm{erf}\bigg(\frac{x_{j} - x_{i, j}}{\sigma_{j} \sqrt{2}}\bigg)\Bigg]
		\label{eq:smooth_empirical_cdf_gaussian_kernel}
		\end{equation}
		
		With this approach, CDF estimates are now smooth and can be shown to converge to the true CDF in the limit of $n \rightarrow \infty$ and vanishing kernel bandwidth. However, due to the possibility of negative weights $\beta_{i}$, both discriminative approaches may produce CDF estimates that are not strictly non-decreasing. To rectify this, we propose a generative approach to estimating the CDF.

		\begin{figure}
			\begin{center}
				\includegraphics[width=\columnwidth]{figures/cumulativeexamplesmooth}
			\end{center}
			\caption{\small (Left) A MKBR estimate of the PDF from the Motorcycle dataset. The blue line indicates the PDF, and the red bars indicate the weights of training points at those $X$ locations. (Right) The resulting smooth CDF estimate using \eqref{eq:smooth_empirical_cdf_gaussian_kernel} overlaid on the previous (non-smooth) estimate. \warn{Figure not referenced in text. Caption not updated.}}
			\label{fig:cumulativeexamplesmooth}
		\end{figure}

\section{Generative Quantile Regression}

%	\warn{So we need to cite MKBR before we talk about integrating it. The integration part is relatively short and straight forward, so the section would comprise of deriving and discussing MKBR with examples, and then discuss integration briefly, and then present example figures.}
	
	Given a particular embedding, the empirical PDF can be recovered through the quadratic programming pre-image (\qpi) method proposed by \cite{McCalman2013}. This method represents the PDF as a mixture of the kernel $k$ centred at each of the data points, and thus has as many components as the number of data points. Through minimising the RKHS distance between the true PDF and the PDF estimate, the resulting algorithm reduces to a tractable quadratic programming optimisation problem, which finds the optimal weights $\{w_{i}\}_{i = 1}^{n}$ for which $\hat{p}_{\bvec{\rv{X}}}$ is closest to the true PDF \eqref{eq:qpi}. 
	
	\begin{equation}
		\hat{p}_{\bvec{\rv{X}}}(\bvec{x}) = \sum_{i = 1}^{n} w_{i} k(\bvec{x}_{i}, \bvec{x})
	\label{eq:qpi}
	\end{equation}

	Since \qpi recovers a valid PDF that is as smooth as the kernel $k$, integrating this PDF would result a smooth, proper distribution $\hat{\mathbb{P}}_{\bvec{\rv{X}}}$ \eqref{eq:qpi_empirical_distribution} and CDF $\hat{P}_{\bvec{\rv{X}}}$ \eqref{eq:qpi_empirical_cdf} that is always non-decreasing.
	
	\begin{equation}
		\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] = \int_{A} \hat{p}_{\bvec{\rv{X}}}(\bvec{x}) d\bvec{x} = \sum_{i = 1}^{n} w_{i} \int_{A} k(\bvec{x}_{i}, \bvec{x}) d\bvec{x}
	\label{eq:qpi_empirical_distribution}
	\end{equation}
	
	\begin{equation}
		\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) := \hat{\mathbb{P}}_{\bvec{\rv{X}}}[(-\bvec{\infty}, \bvec{x}]] = \sum_{i = 1}^{n} w_{i} \int_{(-\bvec{\infty}, \bvec{x}]} k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}'
	\label{eq:qpi_empirical_cdf}
	\end{equation}
	
	Similar to the smooth distribution estimate \eqref{eq:smooth_empirical_distribution} and CDF estimate \eqref{eq:smooth_empirical_cdf}, the estimates \eqref{eq:qpi_empirical_distribution} and \eqref{eq:qpi_empirical_cdf} require the integral of the kernel $k$, which can either be analytically derived or at least approximated and pre-computed beforehand. Again, analytical form is available for Gaussian kernels \eqref{eq:qpi_empirical_cdf_gaussian_kernel}.
	
	\begin{equation}
		\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) = \frac{1}{2} \sum_{i = 1}^{n} w_{i} \prod_{j = 1}^{d} \Bigg[1 + \mathrm{erf}\bigg(\frac{x_{j} - x_{i, j}}{\sigma_{j} \sqrt{2}}\bigg)\Bigg]
	\label{eq:qpi_empirical_cdf_gaussian_kernel}
	\end{equation}
	
	This CDF estimate is essentially of the same form as the smooth discriminative estimate \eqref{eq:smooth_empirical_cdf_gaussian_kernel}, but with the embedding weights $\bm{\beta}$ replaced by the recovered density weights $\bvec{w}$. Since the density weights $\bvec{w}$ are by construction of \qpi always positive, unlike the embedding weights $\bm{\beta}$, the resulting CDF is guaranteed to be non-decreasing. This form equivalence also highlights the fact that the smooth CDF estimate is simply the integral of the normalised embedding.
	
		\begin{figure}
			\begin{center}
				\includegraphics[width=\columnwidth]{figures/cumulativeexampleerf}
			\end{center}
			\caption{\small CDF estimate of the Motorcycle data with a Gaussian kernel. The individual components from equation \eqref{eq:qpi_empirical_cdf_gaussian_kernel} are also plotted. Just as the posterior density is a sum of Gaussians, the posterior cumulative is a sum of error functions. Note, negative weights mean that some of the contributions from the terms in the sum are negative. The resultant CDF estimate may not be strictly non-decreasing. \warn{Figure not referenced in text. Caption not updated.}}
			\label{fig:cumulativeexampleerf} 
		\end{figure}
		
\section{Hyperparameter Learning}

	\warn{There are two major methods for hyperparameter tuning: Directly optimising for error for a particular quantile, and using the negative log probability from MKBR.}
	
	With all kernel based techniques, the kernel $k = k_{\bm{\theta}}$ itself is usually specified from a family of kernels parametrised by some hyperparameters $\bm{\theta}$. For example, a Gaussian kernel would be specified by its length scale parameters. The number of parameters would depend on whether the kernel is isotropic ($1$ parameter), anisotropic axis-aligned ($d$ parameters), or general ($d(d - 1)/2$ parameters). Whichever family we choose, we would have to find the optimal hyperparameter setting that gives the best performance in terms of inference accuracy.
	
	We considered two approaches to hyperparameter learning for the purpose of quantile regression.
	
\section{Experiments}

\section{Conclusion}

	\warn{Future work would include a better learning method for the hyperparameters, and also a better density recovery method for the generative approach besides MKBR. We perhaps needs to justify our estimators a bit more for the discriminative approach. To me, the learning algorithm is the major thing waiting to be solved right now.}
	

\section*{Acknowledgements}

\bibliographystyle{named}
\bibliography{kernel_embeddings}

\end{document}
