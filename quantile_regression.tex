\documentclass[twoside]{article} \usepackage{aistats2017}

% If your paper is accepted, change the options for the package
% aistats2017 as follows:
%
%\usepackage[accepted]{aistats2017}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathbbol}
\usepackage{mathtools}
\usepackage{mathrsfs}     
\usepackage{bm}           
\usepackage{vector}       
\usepackage{color}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
%\newcommand{\rv}[1]{\underline{\overline{#1}}}
\newcommand{\rv}[1]{\underline{#1}}
\newcommand{\ds}[1]{{#1}}
\newcommand{\warn}[1]{{\color{red} #1}}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Quantile Regression with Kernel Embeddings}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
  The Abstract paragraph should be indented 0.25 inch (1.5 picas) on
  both left and right-hand margins. Use 10~point type, with a vertical
  spacing of 11~points. The {\bf Abstract} heading must be centered,
  bold, and in point size 12. Two line spaces precede the
  Abstract. The Abstract must be limited to one paragraph.
\end{abstract}

	\warn{Meeting 15/09/2016:
		
		\begin{itemize}
			\item Milestones
			\item Notations
			\item Work distribution
			\item What to include in supplementary
			\item How to share and collaborate
		\end{itemize}}

\section{Introduction}
		
	\warn{Why quantile regression? What are the advantages of quantile regression over regular regression?}
	
	
\section{Related Work}

	\warn{Discuss things like non-parameteric quantile regression (I think it's Smola's paper) and then also quantile regression with GP, and why we are in search for an even more flexible model. Follow Lachlan's related work section on this.}
	
\section{Background}

	\subsection{Measure Theory Notation and Setup} % DON'T NEED TO DO THIS: ASSUME HILBERT SPACE STUFF IS KNOWN (ASK FABIO)
	
	% SWITCH RELATED WORKS AND BACKGROUND
	
		Let $(\Omega, \mathcal{W}, \mathbb{P})$ be a probability space, where $\Omega$ is the sample space, $\mathcal{W}$ is a $\sigma$-algebra on $\Omega$, and $\mathbb{P} : \mathcal{W} \mapsto [0, 1]$ a probability measure. The elements $W$ of $\mathcal{W}$, that is, the measurable sets, are called events, which are by definition subsets of $\Omega$.
		
		The \textit{distribution} of a random variable $\rv{X} : \Omega \mapsto \mathcal{X}$ is the Borel measure $\mathbb{P}_{\rv{X}} : \mathcal{A} \mapsto [0, 1]$ defined by
		\begin{equation}
			\mathbb{P}_{\rv{X}}[A] := \mathbb{P}[\rv{X} \in A]
		\label{eq:distribution}
		\end{equation}
		where the event $\{\rv{X} \in A\} := \rv{X}^{-1}[A] := \{\omega \in \Omega : \rv{X}(\omega) \in A\} \in \mathcal{W}$ is the inverse image (pre-image) of $A \subseteq \mathcal{X}$ under the random variable $\rv{X}$. Here $\mathcal{A}$ is a $\sigma$-algebra of subsets on $\mathcal{X}$. Recall that a random variable (in this case, $\rv{X}$) is formally a map from the sample space (in this case, $\Omega$) to some Borel measurable domain (in this case, $\mathcal{X}$) of interest. The \textit{distribution} is also a probability measure itself, as it satisfies the defining properties of a probability measure, such that $(\mathcal{X}, \mathcal{A}, \mathbb{P}_{\rv{X}})$ is also a probability space. It differs from $\mathbb{P}$ in that it operates in a different domain - not directly on the events, but on the values the random variable takes for those events. 
		
		The \textit{cumulative distribution function} (CDF) $P_{\rv{X}} : \mathcal{X} \mapsto [0, 1]$ of a \textit{real-valued} random variable $\rv{X} : \Omega \mapsto \mathcal{X}$ (that is, take $\mathcal{X}$ to be $\mathbb{R}$ or a subset thereof) is defined by
		\begin{equation}
			P_{\rv{X}}(x) := \mathbb{P}_{\rv{X}}[(-\infty, x]] = \mathbb{P}[\rv{X} \leq x]
		\label{eq:cdf}
		\end{equation}
		where $\{\rv{X} \leq x\} := \{\rv{X} \in (-\infty, x]\}$ is a shorthand.

		Similarly, if we take a \textit{vectored-valued} random variable $\bvec{\rv{X}} : \Omega \mapsto \mathbb{R}^{d}$, then 
		\begin{equation}
			P_{\bvec{\rv{X}}}(\bvec{x}) := \mathbb{P}_{\bvec{\rv{X}}}[(-\infty, x_{1}] \times \dots \times (-\infty, x_{d}]] = \mathbb{P}[\bvec{\rv{X}} \leq \bvec{x}]
		\label{eq:cdf_vector}
		\end{equation}
		where $\{\bvec{\rv{X}} \leq \bvec{x}\} := \bigcap\limits_{j = 1}^{d} \{\rv{X}_{j} \in (-\infty, x_{j}]\} \in \mathcal{W}$ is a shorthand. From here on we also define a shorthand $(-\bvec{\infty}, \bvec{x}] := (-\infty, x_{1}] \times \dots \times (-\infty, x_{d}]$ for conciseness.
				
%		Note that in most literature the cumulative distribution function would be denoted by $F_{X}$ and not $P_{X}$. However, with a lot of different spaces and variables to introduce later on, we will reserve $F$ and consequently $f$ for other quantities later.
		
		If the distribution $\mathbb{P}_{\rv{X}}$ is absolutely continuous with respect to the Lebesgue measure on $\mathbb{R}$, then the Radon-Nikodym theorem implies the existence of a density for $\mathbb{P}_{\rv{X}}$. We call this the distribution density of $\rv{X}$, or more commonly known as the \textit{probability density function} (PDF), denoted by $p_{\rv{X}}$. Note that $p_{\rv{X}} : \mathcal{X} \mapsto [0, \infty)$ is a mapping from the Borel measurable domain to the positive real numbers.
		
		Let $f : \mathcal{X} \mapsto \mathbb{R}$ be some arbitrary function over the domain, then we have
		\begin{equation}
			\mathbb{E}[f(\rv{X})] = \int_{\Omega} f \circ \rv{X} d \mathbb{P} = \int_{\mathcal{X}} f d \mathbb{P}_{\rv{X}}
		\label{eq:functional_expectation}
		\end{equation}
		
		Recall that $\mathbb{P}[W] = \mathbb{E}[\mathbb{1}[W]] \;\; \forall W \in \mathcal{W}$, so that the cumulative distribution function for a vector-valued random variable $\bvec{\rv{X}}$ is
		\begin{equation}
			\begin{aligned}
				P_{\bvec{\rv{X}}}(\bvec{x}) &= \mathbb{P}[\bvec{\rv{X}} \leq \bvec{x}] = \mathbb{E}[\mathbb{1}[\bvec{\rv{X}} \leq \bvec{x}]] = \mathbb{E}[\mathbb{1}_{(-\bvec{\infty}, \bvec{x}]}(\bvec{X})] \\
				&= \int_{\Omega} \mathbb{1}_{(-\bvec{\infty}, \bvec{x}]} \circ \bvec{\rv{X}} d \mathbb{P} = \int_{\mathbb{R}^{D}} \mathbb{1}_{(-\bvec{\infty}, \bvec{x}]} d \mathbb{P}_{\bvec{\rv{X}}} \\
				&= \int_{(-\bvec{\infty}, \bvec{x}]} d \mathbb{P}_{\bvec{\rv{X}}}
			\end{aligned}
		\label{eq:cdf_integral}
		\end{equation}
		where we use the common notation $\mathbb{1}_{A}(x) := \mathbb{1}[x \in A]$.
		
		\warn{I am already detailing the notations in the supplementary material. Is it appropriate if we just put this whole section into the supplementary material so we have more space actually talking about Quantile Regression?}
		
	\subsection{Kernel Embeddings Notation}
	
		In this section we assume all kernels we deal with are characteristic. That is, a characteristic kernel $k : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ defines a unique reproducing kernel Hilbert space (RKHS), which we will denote by $\mathcal{H}_{k}$. Note that in some literature, since $k$ takes elements $x$ of $\mathcal{X}$ as arguments, the corresponding reproducing kernel Hilbert space is denoted as $\mathcal{H}_{\mathcal{X}}$ instead. However, since the kernel $k$ also includes information about the space $\mathcal{X}$ it operates on, the notation $\mathcal{H}_{k}$ will be preferred.
		 
		The kernel embedding $\mu_{\rv{X}}$ of a distribution measure $\mathbb{P}_{\rv{X}}$ under kernel $k$ is defined by
		\begin{equation}
			\mu_{\rv{X}}(x) := \mathbb{E}[k(\rv{X}, x)] = \int_{\mathcal{X}} k(\cdot, x) d \mathbb{P}_{\rv{X}}
		\label{eq:basic_embedding}
		\end{equation}
		where it is understood that the $\cdot$ is a place-holder for an argument such that $k(\cdot, x) : \mathcal{X} \mapsto \mathbb{R}$ is a function and not just a value.
		
		We can also write the definition equivalently as
		\begin{equation}
			\mu_{\rv{X}} \equiv \mu_{\rv{X}}(\cdot) := \mathbb{E}[k(\rv{X}, \cdot)]
		\label{eq:basic_embedding_alternative}
		\end{equation}
		where it is understood that the $\cdot$ here in the last equation is for a different argument than the previous equations.
		 
		In some literature, to make explicit of the fact that the kernel embedding is an embedding of a distribution measure $\mathbb{P}_{\rv{X}}$, the kernel embedding is instead denoted by $\mu_{\mathbb{P}_{\rv{X}}}$. We will not do this here in general here. Since the specification of a random variable $\rv{X}$ and its distribution $\mathbb{P}_{\rv{X}}$ must come together, either notation is equally informative.
		 
		Usually, the kernel $k$ comes from a family of kernel functions with a set of hyperparameters $\theta$. To make this relationship explicit, we may attach a subscript to the notation as $k_{\theta}$. Correspondingly, the kernel embedding would then be denoted as $\mu_{\rv{X}, \theta}$.

		It would indeed be more precise to put a subscript $k$ on the kernel embedding, as $\mu_{\rv{X}, k}$ or $\mu_{\rv{X}, k_{\theta}}$, to make clear that it was embedded with kernel $k$. However, as we will not be changing the kernel much except up to some hyperparameter tuning, this will just be implicitly assumed.
			 
		The kernel embeddings of Cartesian products of random variables over the same event space $\Omega$ define uncentred covariance and cross-covariance operators. Note that most of the important operations regarding uncertred versions also hold for centred versions.
		 
		\warn{This could be more detailed but space is really limited. Perhaps just a short paragraph introducing cross-covariance operators and also conditional operators and reference a previous paper. However, in the supplementary material we will have to make consistent the notation.}

		
	\subsection{Quantiles and Quantile Regression}
	
		Consider a real-valued random variable $\rv{X} : \Omega \mapsto \mathbb{R}$ with distribution $\mathbb{P}_{X}$ and cumulative distribution function $P_{\rv{X}}$. The $\tau$-quantile, $\tau \in (0, 1)$, of $\mathbb{P}_{\rv{X}}$ is defined as
		\begin{equation}
			q_{\rv{X}}(\tau) = \inf\{x : P_{\rv{X}}(x) \geq \tau\} \equiv \inf_{P_{\rv{X}}(x) \geq \tau} x
		\label{eq:quantile}
		\end{equation}
		
		A quantile of a conditional distribution $P_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}$ of is defined similarly. This paper considers regression problems with multiple inputs and single output, such that $\mathcal{X} = \mathbb{R}^{d}$ and $\mathcal{Y} = \mathbb{R}$.
		
		\begin{equation}
			q_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(\tau) = \inf\{y : P_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(y) \geq \tau\} \equiv \inf_{P_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(y) \geq \tau} y
		\label{eq:conditional_quantile}
		\end{equation}
		
		Quantile regression is then the problem of finding an empirical estimate $\hat{q}_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(\tau)$ for $q_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(\tau)$ given a set of paired observations $\{(\bvec{x}_{i}, y_{i})\}_{i = 1}^{n}$ and a query point $\bvec{x}$.
		
		In practice, it is sufficient to replace the infimum with the minimum if the CDF estimate is smooth. Since the CDF is always non-decreasing, the inequality becomes an equality, and obtaining quantiles becomes a root finding procedure once an estimate of the CDF is obtained \eqref{eq:quantile_root_finding}. In this paper, we present methods for estimating the CDF from a dataset.
		
		\begin{equation}
			q_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(\tau) = y : P_{\rv{Y} | \bvec{\rv{X}} = \bvec{x}}(y) = \tau
		\label{eq:quantile_root_finding}
		\end{equation}	
	
		\warn{This is where we introduce the pinball loss and the basics of quantile regression}
				
		\warn{Need perhaps illustrative examples through description}
		
\section{Discriminative Quantile Regression}

	The discriminative approach to quantile regression refers to techniques which computes an estimate of the CDF directly, without needing to compute the PDF first.
	
	The central idea here revolves around the fact that the probability measure of a set is the expected indicator value of that set for a given random vector $\bvec{\rv{X}}$, and the expected value of a function can be written as the inner product between the kernel embedding and the function \eqref{eq:distribution_inner}.
	
	\begin{equation}
		\mathbb{P}_{\bvec{\rv{X}}}[A] = \mathbb{E}[\mathbb{1}_{A}(\bvec{\rv{X}})] = \langle \mu_{\rv{X}}, \mathbb{1}_{A} \rangle
	\label{eq:distribution_inner}
	\end{equation}
	
	As the CDF $P_{\rv{X}}$ is defined through the distribution $\mathbb{P}_{\rv{X}}$ \eqref{eq:cdf}, it becomes straightforward to obtain the CDF as an inner product between the kernel embedding and the corresponding indicator function. \eqref{eq:cdf_inner}.
	
	\begin{equation}
		P_{\bvec{\rv{X}}}(\bvec{x}) = \mathbb{P}_{\bvec{\rv{X}}}[(-\bvec{\infty}, \bvec{x}]] = \langle \mu_{\bvec{\rv{X}}}, \mathbb{1}_{(-\bvec{\infty}, \bvec{x}]} \rangle
	\label{eq:cdf_inner}
	\end{equation}
	
	We will now focus our attention on how to estimate the distribution and CDF from a given dataset.
	
	\subsection{Optimal Empirical Projection for Function Approximation}

		In order to estimate the distribution and CDF empirically, we would need both the empirical kernel embedding $\hat{\mu}_{\rv{X}}$ and a projection $\hat{\mathbb{1}}_{A}$ of the indicator function $\mathbb{1}_{A}$ into the same RKHS as $\hat{\mu}_{\rv{X}}$ so that a proper inner product within that RKHS can be evaluated.

		Suppose $\ds{X} := \{\bvec{x}_{1}, \dots, \bvec{x}_{n}\} \in \mathbb{R}^{n \times d}$ is set of observations from $\mathbb{P}_{\rv{X}}$. The embeddings that can be represented by these samples are linear combinations of the feature functions $\phi_{\bvec{x}_{i}}$ at the observations \eqref{eq:function_projection}. The span obtained by those basis $\{\phi_{\bvec{x}_{i}}\}_{i = 1}^{n}$ also forms a RKHS, denoted by $\mathcal{H}_{k, \ds{X}}$, and is a finite vector subspace of the full RKHS $\mathcal{H}_{k}$.
		
		\begin{equation}
			\hat{f} = \sum_{i = 1}^{n} \alpha_{i} k(\bvec{x}_{i}, \cdot) = \sum_{i = 1}^{n} \alpha_{i} \phi_{\bvec{x}_{i}} = \Phi_{\ds{X}} \bm{\alpha}
		\label{eq:function_projection}
		\end{equation}
	
		We are interested in approximating the function $f \in \mathcal{H}_{k}$ with $\hat{f} \in \mathcal{H}_{k, \ds{X}}$ so that they are close as measured by the norm induced by the full RKHS $\mathcal{H}_{k}$, by tuning the embedding weights $\bm{\alpha} = \{\alpha_{i}\}_{i = 1}^{n}$ \eqref{eq:function_projection_optimisation}.
			
		\begin{equation}
			\bm{\alpha}^{\star} = \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert
		\label{eq:function_projection_optimisation}
		\end{equation}
		
		This optimisation problem has a closed form solution \eqref{eq:function_projection_solution}, where $K_{\ds{X} \ds{X}} := \{k(\bvec{x}_{i}, \bvec{x}_{j})\}_{i = 1, j = 1}^{n, n}$ is the gram matrix and $f\{\ds{X}\} := \{f(\bvec{x}_{i})\}_{i = 1}^{n}$ is the function evaluated at the observed points (see supplementary material).
		
		\begin{equation}
			\begin{aligned}
				\bm{\alpha}^{\star} &= K_{\ds{X} \ds{X}}^{-1} f\{\ds{X}\} \\
				\hat{f} = \Phi_{\ds{X}} \bm{\alpha}^{\star} &= \Phi_{\ds{X}} K_{\ds{X} \ds{X}}^{-1} f\{\ds{X}\}
			\end{aligned}
		\label{eq:function_projection_solution}
		\end{equation}
		
	\subsection{Optimal Empirical Distribution}

		\warn{There is something the reviewers might pick up here: We do not know if the indicator function is in the full RKHS $\mathcal{H}_{k}$ in general. In fact, the paper ``Recovering Distributions from Gaussian RKHS Embeddings'' said that polynomial functions and indicator functions are included in Besov spaces under certain assumptions. The paper does continue to derive a bound and also empirically show that the estimator we derived does converge to the true value. I guess we should probably mention this: That while indicator functions are not generally contained in the RKHS, the estimator we derived for its expectation, by assuming that it is contained in the RKHS, converges to the actual expectation.}
				
		Letting $f = \mathbb{1}_{A}$, we can find the projection of an indicator function into $\mathcal{H}_{k, \ds{X}}$ \eqref{eq:indicator_projection}.
		
		\begin{equation}
			\hat{\mathbb{1}}_{A} = \Phi_{\ds{X}} K_{\ds{X} \ds{X}}^{-1} \mathbb{1}_{A}\{\ds{X}\}
		\label{eq:indicator_projection}
		\end{equation}
		
		The empirical distribution \eqref{eq:empirical_distribution} and CDF \eqref{eq:empirical_cdf} can then be derived.
		
		\begin{equation}
		\begin{aligned}
			\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] &= \langle \hat{\mu}_{\bvec{\rv{X}}}, \hat{\mathbb{1}}_{A} \rangle = \hat{\mu}_{\bvec{\rv{X}}}^{T}\hat{\mathbb{1}}_{A} \\
			&= (\Phi_{\ds{X}} \bm{\beta})^{T} \Phi_{\ds{X}} K_{\ds{X} \ds{X}}^{-1} \mathbb{1}_{A}\{\ds{X}\} \\
			&= \bm{\beta}^{T} \Phi_{\ds{X}}^{T} \Phi_{\ds{X}} K_{\ds{X} \ds{X}}^{-1} \mathbb{1}_{A}\{\ds{X}\} \\
			&= \bm{\beta}^{T} K_{\ds{X} \ds{X}} K_{\ds{X} \ds{X}}^{-1} \mathbb{1}_{A}\{\ds{X}\} \\
			&= \bm{\beta}^{T} \mathbb{1}_{A}\{\ds{X}\} \\
			&= \sum_{i = 1}^{n} \beta_{i} \mathbb{1}_{A}(\bvec{x}_{i}) \\
			&= \sum_{\bvec{x}_{i} \in A} \beta_{i}
		\end{aligned}
		\label{eq:empirical_distribution}
		\end{equation}
		
		
		\begin{equation}
			\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) := \hat{\mathbb{P}}_{\bvec{\rv{X}}}[(-\bvec{\infty}, \bvec{x}]] = \sum_{\bvec{x}_{i} \in (-\bvec{\infty}, \bvec{x}]} \beta_{i} = \sum_{\bvec{x}_{i} \leq \bvec{x}} \beta_{i}
		\label{eq:empirical_cdf}
		\end{equation}
		
		\warn{Add examples figures}
	
	\subsection{Smooth Empirical Distribution}
	
		\warn{The good news for this section is that the function we are taking an expectation of is now in the RKHS. The bad news is that it is no longer a proper indicator function. However, empirically, it works well. Also, unlike the previous part where the resulting estimator coincides with the Motonobu's paper, this part is completely novel.}
		
		\begin{equation}
		\hat{\mathbb{1}}_{A} := \int_{A} k(\bvec{x}, \cdot) d\bvec{x} = \int_{A} \phi_{\bvec{x}} d\bvec{x}
		\end{equation}
		
		\begin{equation}
		\begin{aligned}
		\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] &= \langle \hat{\mu}_{\bvec{\rv{X}}}, \hat{\mathbb{1}}_{A} \rangle = \hat{\mu}_{\bvec{\rv{X}}}^{T}\hat{\mathbb{1}}_{A} \\
		&= (\Phi_{\ds{X}} \bm{\beta})^{T} \int_{A} \phi_{\bvec{x}} d\bvec{x} \\
		&= \bm{\beta}^{T} \Phi_{\ds{X}}^{T} \int_{A} \phi_{\bvec{x}} d\bvec{x} \\
		&= \int_{A} \bm{\beta}^{T} \Phi_{\ds{X}}^{T} \phi_{\bvec{x}} d\bvec{x} \\
		&= \int_{A} \bm{\beta}^{T} K_{\ds{X} \bvec{x}} d\bvec{x} \\
		&= \int_{A} \sum_{i = 1}^{n} \beta_{i} k(\bvec{x}_{i}, \bvec{x}) d\bvec{x} \\
		&= \sum_{i = 1}^{n} \beta_{i} \int_{A}  k(\bvec{x}_{i}, \bvec{x}) d\bvec{x} \\
		\end{aligned}
		\label{eq:smooth_empirical_distribution}
		\end{equation}	
		
		\begin{equation}
		\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) := \hat{\mathbb{P}}_{\bvec{\rv{X}}}[(-\bvec{\infty}, \bvec{x}]] = \sum_{i = 1}^{n} \beta_{i} \int_{(-\bvec{\infty}, \bvec{x}]}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}'
		\label{eq:smooth_empirical_cdf}
		\end{equation}
		
		For a Gaussian kernel:
		
		\begin{equation}
		\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) = \frac{1}{2} \sum_{i = 1}^{n} \beta_{i} \prod_{j = 1}^{d} \Bigg[1 + \mathrm{erf}\bigg(\frac{x_{j} - x_{i, j}}{\sigma_{j} \sqrt{2}}\bigg)\Bigg]
		\end{equation}
		
		\warn{Add examples figures}

\section{Generative Quantile Regression}

	\warn{It seems like the MKBR stuff also isn't published yet. So we need to talk about that before we talk about integrating it. The integration part is relatively short and straight forward, so the section would comprise of deriving and discussing MKBR with examples, and then discuss integration briefly, and then present example figures.}
	
\section{Hyperparameter Learning}

	\warn{There are two major methods for hyperparameter tuning: Directly optimising for error for a particular quantile, and using the negative log probability from MKBR.}
	
\section{Experiments}

\section{Conclusion}

	\warn{Future work would include a better learning method for the hyperparameters, and also a better density recovery method for the generative approach besides MKBR. We perhaps needs to justify our estimators a bit more for the discriminative approach. To me, the learning algorithm is the major thing waiting to be solved right now.}
	
\newpage

\section{Background}

First level headings are all caps, flush left, bold, and in point size
12. Use one line space before the first level heading and one-half line space
after the first level heading.

\subsection{Second Level Heading}

Second level headings are initial caps, flush left, bold, and in point
size 10. Use one line space before the second level heading and one-half line
space after the second level heading.

\subsubsection{Third Level Heading}

Third level headings are flush left, initial caps, bold, and in point
size 10. Use one line space before the third level heading and one-half line
space after the third level heading.

\paragraph{Fourth Level Heading}

Fourth level headings must be flush left, initial caps, bold, and
Roman type.  Use one line space before the fourth level heading, and
place the section text immediately after the heading with no line
break, but an 11 point horizontal space.

\subsection{CITATIONS, FIGURES, REFERENCES}


\subsubsection{Citations in Text}

Citations within the text should include the author's last name and
year, e.g., (Cheesman, 1985). References should follow any style that
you are used to using, as long as their style is consistent throughout
the paper.  Be sure that the sentence reads correctly if the citation
is deleted: e.g., instead of ``As described by (Cheesman, 1985), we
first frobulate the widgets,'' write ``As described by Cheesman
(1985), we first frobulate the widgets.''  Be sure to avoid
accidentally disclosing author identities through citations.

\subsubsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first
  footnote.} in the text. Use 8 point type for footnotes. Place the
footnotes at the bottom of the column in which their markers appear,
continuing to the next column if required. Precede the footnote
section of a column with a 0.5 point horizontal rule 1~inch (6~picas)
long.\footnote{Sample of the second footnote.}

\subsubsection{Figures}

All artwork must be centered, neat, clean, and legible.  All lines
should be very dark for purposes of reproduction, and art work should
not be hand-drawn.  Figures may appear at the top of a column, at the
top of a page spanning multiple columns, inline within a column, or
with text wrapped around them, but the figure number and caption
always appear immediately below the figure.  Leave 2 line spaces
between the figure and the caption. The figure caption is initial caps
and each figure should be numbered consecutively.

Make sure that the figure caption does not get separated from the
figure. Leave extra white space at the bottom of the page rather than
splitting the figure and figure caption.
\begin{figure}[h]
\vspace{.3in}
\centerline{\fbox{This figure intentionally left non-blank}}
\vspace{.3in}
\caption{Sample Figure Caption}
\end{figure}

\subsubsection{Tables}

All tables must be centered, neat, clean, and legible. Do not use hand-drawn tables. Table number and title always appear above the table.
See Table~\ref{sample-table}.

Use one line space before the table title, one line space after the table title, and one line space after the table. The table title must be
initial caps and each table numbered consecutively.

\begin{table}[h]
\caption{Sample Table Title} \label{sample-table}
\begin{center}
\begin{tabular}{ll}
{\bf PART}  &{\bf DESCRIPTION} \\
\hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{SUPPLEMENTARY MATERIAL}

If you need to include additional appendices during submission, you
can include them in the supplementary material file.


\newpage

\section{INSTRUCTIONS FOR CAMERA-READY PAPERS}

For the camera-ready paper, if you are using \LaTeX, please make sure
that you follow these instructions.  (If you are not using \LaTeX,
please make sure to achieve the same effect using your chosen
typesetting package.)

\begin{enumerate}
    \item Download \texttt{fancyhdr.sty} -- the
    \texttt{aistats2017.sty} file will make use of it.
    \item Begin your document with
    \begin{flushleft}
    \texttt{\textbackslash documentclass[twoside]\{article\}}\\
    \texttt{\textbackslash usepackage[accepted]\{aistats2017\}}
    \end{flushleft}
    The \texttt{twoside} option for the class article allows the
    package \texttt{fancyhdr.sty} to include headings for even and odd
    numbered pages. The option \texttt{accepted} for the package
    \texttt{aistats2017.sty} will write a copyright notice at the end of
    the first column of the first page. This option will also print
    headings for the paper.  For the \emph{even} pages, the title of
    the paper will be used as heading and for \emph{odd} pages the
    author names will be used as heading.  If the title of the paper
    is too long or the number of authors is too large, the style will
    print a warning message as heading. If this happens additional
    commands can be used to place as headings shorter versions of the
    title and the author names. This is explained in the next point.
    \item  If you get warning messages as described above, then
    immediately after $\texttt{\textbackslash
    begin\{document\}}$, write
    \begin{flushleft}
    \texttt{\textbackslash runningtitle\{Provide here an alternative shorter version of the title of your
    paper\}}\\
    \texttt{\textbackslash runningauthor\{Provide here the surnames of the authors of your paper, all separated by
    commas\}}
    \end{flushleft}
    Note that the text that appears as argument in \texttt{\textbackslash
      runningtitle} will be printed as a heading in the \emph{even}
    pages. The text that appears as argument in \texttt{\textbackslash
      runningauthor} will be printed as a heading in the \emph{odd}
    pages.  If even the author surnames do not fit, it is acceptable
    to give a subset of author names followed by ``et al.''

    \item Use the file sample\_paper.tex as an example.

    \item Both submitted and camera-ready versions of the paper are 8
      pages, plus any additional pages needed for references.

    \item If you need to include additional appendices,
      you can include them in the supplementary
      material file.

    \item Please, don't change the layout given by the above
      instructions and by the style file.

\end{enumerate}

\subsubsection*{Acknowledgements}

Use unnumbered third level headings for the acknowledgements.  All
acknowledgements go at the end of the paper.  Be sure to omit any
identifying information in the initial double-blind submission!


\subsubsection*{References}

References follow the acknowledgements.  Use an unnumbered third level
heading for the references section.  Any choice of citation style is
acceptable as long as you are consistent.  Please use the same font
size for references as for the body of the paper---remember that
references do not count against your page length total.

J.~Alspector, B.~Gupta, and R.~B.~Allen (1989). Performance of a
stochastic learning microchip.  In D. S. Touretzky (ed.), {\it
  Advances in Neural Information Processing Systems 1}, 748-760.  San
Mateo, Calif.: Morgan Kaufmann.

F.~Rosenblatt (1962). {\it Principles of Neurodynamics.} Washington,
D.C.: Spartan Books.

G.~Tesauro (1989). Neurogammon wins computer Olympiad.  {\it Neural
  Computation} {\bf 1}(3):321-323.

\end{document}
