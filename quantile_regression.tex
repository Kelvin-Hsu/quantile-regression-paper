\documentclass[twoside]{article} \usepackage{aistats2017}

% If your paper is accepted, change the options for the package
% aistats2017 as follows:
%
%\usepackage[accepted]{aistats2017}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

\bibliographystyle{apalike}  % Use the "unsrtnat" BibTeX style for formatting the Bibliography
\usepackage[square, authoryear, comma, sort&compress]{natbib}  % Use the "Natbib" style for the references in the Bibliography

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathbbol}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{vector}
\usepackage[usenames, dvipsnames]{color}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\rv}[1]{{#1}}
\newcommand{\ds}[1]{\tilde{#1}}
\newcommand{\warn}[1]{{\color{red} #1}}
\newcommand{\extra}[1]{{\color{ForestGreen} #1}}
\newcommand{\qpi}{QPI}

\newcommand{\expect}[1]{{\mathbb{E}[#1]}}
\newcommand{\inner}[2]{{\langle #1, #2 \rangle}}

\newcommand{\Hk}{\mathcal{H}_{k}}
\newcommand{\Hl}{\mathcal{H}_{l}}
\newcommand{\muX}{\mu_{\rv{X}}}
\newcommand{\muY}{\mu_{\rv{Y}}}
\newcommand{\muYx}{\mu_{\rv{Y} | \rv{X} = x}}
\newcommand{\muXy}{\mu_{\rv{X} | \rv{Y} = y}}
\newcommand{\phiX}{\phi_{\rv{X}}}
\newcommand{\psiY}{\psi_{\rv{Y}}}
\newcommand{\Cxy}{C_{\rv{X} \rv{Y}}}
\newcommand{\Cyx}{C_{\rv{Y} \rv{X}}}
\newcommand{\Cxx}{C_{\rv{X} \rv{X}}}
\newcommand{\Cyy}{C_{\rv{Y} \rv{Y}}}
\newcommand{\Cylx}{C_{\rv{Y} | \rv{X}}}
\newcommand{\Cxly}{C_{\rv{X} | \rv{Y}}}

\newcommand{\hatmuX}{\hat{\mu}_{\rv{X}}}
\newcommand{\hatmuY}{\hat{\mu}_{\rv{Y}}}
\newcommand{\hatmuYx}{\hat{\mu}_{\rv{Y} | \rv{X} = x}}
\newcommand{\hatmuXy}{\hat{\mu}_{\rv{X} | \rv{Y} = y}}
\newcommand{\hatCxy}{\hat{C}_{\rv{X} \rv{Y}}}
\newcommand{\hatCyx}{\hat{C}_{\rv{Y} \rv{X}}}
\newcommand{\hatCxx}{\hat{C}_{\rv{X} \rv{X}}}
\newcommand{\hatCyy}{\hat{C}_{\rv{Y} \rv{Y}}}
\newcommand{\hatCylx}{\hat{C}_{\rv{Y} | \rv{X}}}
\newcommand{\hatCxly}{\hat{C}_{\rv{X} | \rv{Y}}}
\newcommand{\cardX}{\Vert \mathcal{X} \Vert}
\newcommand{\cardY}{\Vert \mathcal{Y} \Vert}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Non-Parametric Quantile Regression with Kernel Embeddings}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}

	Quantile regression is concerned with the estimation of quantiles from a conditional distribution of a response variable, offering a more complete and robust inference framework than the mean prediction. This is especially important in decision making applications where a risk probability is used to bound the acceptable ranges of the response variable. In this paper, we present a non-parametric quantile regression framework by utilising the expressive power and flexibility provided by kernel embedding models. Through embedding probability distributions as unique mean embeddings that can be represented directly from data, an inference framework can be developed non-parametrically. We present a set of novel algorithms for computing the quantiles from such a non-parametric inference model, and compare it to state-of-the-art algorithms in quantile regression. On multiple standard datasets, our methods produces competitive or improved quantile estimates as compared to the literature.
	
\end{abstract}

\section{Introduction}
\label{sec:introduction}

	Understanding risk is critical in any discipline making decisions under uncertainty. Uncertainty can arise from both incomplete information and randomness in events, whose behaviour is often best captured by probabilistic methods. In many cases, such decisions depend on one or more quantities of interest, where the likelihood that such quantities will fall outside a nominal or acceptable range is used to quantify the corresponding risks involved.
	
	For example, the exterior toughness of a motorcycle can be designed to withstand a likely range of acceleration experienced under collision, and no more, in order to limit the development cost. The probability that the actual acceleration experienced falls outside the nominal range is seen as the level of risk involved in this decision.

	More naturally, however, instead of specifying an acceptable range of values and determining the level of risk involved, the risk probability can be specified instead, and the bounds of the acceptable range can be found. That is, we are interested in finding a quantile, which provides the bounds inside which a known mass of probability is contained.
	
%	Bayesian methods in the machine learning community has demonstrated the advantages that probabilistic techniques bring forth. By maintaining not only a point estimate of a phenomenon, but a probability distribution of our belief about that phenomenon, informed decisions and actions can be taken by taking into account the uncertainty involved.
	
% * <lachlan@mccalman.info> 2016-10-05T23:30:52.467Z:
%
% These three sentences, whilst each good, could probably be made to connect more. Ie perhaps clarify the relationship between risk, uncertainty, decision making and Bayesian techniques?
%
% ^.

	When the quantity of interest is a response variable conditionally dependent on a input variable that is also continuous, the relationship can be captured by a regression model. Typically, the response variable is predicted with the mean, and the uncertainty of this prediction can be captured by the standard deviation. However, this is only natural in the case of an output phenomenon that is uni-modal. Consider a bi-modal, symmetric distribution centred at zero with density peaks at $\pm1$ and a low density value at zero. The mean prediction of zero would not be a representative prediction of a likely sample or outcome, immediately also limiting the usefulness of the standard deviation which captures deviations from the mean. In this case, it is far more useful and natural to bound the uncertainty, or risk, through a quantile.
	
% * <lachlan@mccalman.info> 2016-10-05T23:33:18.326Z:
%
% Might be worth explicitly mentioning at the end here that a quantiles provide bounds inside which a known mass of probability is contained 
%
% ^.
	
%By embedding probability distributions as unique mean embeddings in a reproducing kernel Hilbert space, kernel embedding models reduce probabilistic marginalisation integrals into linear algebraic operations.

	In order to relax the uni-modality restriction, we propose a set of novel quantile regression algorithms using kernel embedding techniques. Through mapping general probability distributions into a reproducing kernel Hilbert space where marginalisation integrals become linear algebra operations, kernel embedding models are able to capture general, multi-modal phenomenon in a Bayesian and non-parametric way. In this work, probability distributions can be represented directly from data without assuming a parametric form, allowing quantile and risk inference on a variety of flexible distributions that is involved in decision making.

% * <lachlan@mccalman.info> 2016-10-05T23:36:20.099Z:
% 
% rather than describing them as six algorithms, might be better to say 'a set of' or a series of or something, because they're all variations on a theme.
% Consider finding different words for 'letting the data speak for itself' -- it's a little colloquial
% Done!
% 
% ^.

%	\begin{figure}
%		\begin{center}
%			\includegraphics[width=0.48\columnwidth]{figures/mcquantiles_1}
%			\includegraphics[width=0.48\columnwidth]{figures/mcquantiles_2}
%		\end{center}
%		\caption{\small Estimate of the cumulative embedding (left),
%			and the quantiles 0.1, 0.25, 0.5, 0.75, 0.9 overlaying a PDF estimate
%			(right)}
%		\label{fig:cembedding}
%	\end{figure}
	
\section{Background}
\label{sec:background}
	
% * <lachlan@mccalman.info> 2016-10-05T23:47:01.875Z:
%
% I'd add a sentence at the beginning explaining that you're using the measure-theoretic notation (and possibly why)
%
% ^.
	Let $(\Omega, \mathcal{W}, \mathbb{P})$ be a probability space, where $\Omega$ is the sample space, $\mathcal{W}$ is a $\sigma$-algebra on $\Omega$, and $\mathbb{P} : \mathcal{W} \to [0, 1]$ a probability measure. The elements $W$ of $\mathcal{W}$, that is, the measurable sets, are called events, which are by definition subsets of $\Omega$. Let $\mathcal{X}$ be some Borel measurable space of interest, endowed by a $\sigma$-algebra $\mathcal{A}$ on $\mathcal{X}$.
% * <lachlan@mccalman.info> 2016-10-05T23:45:45.159Z:
%
% I'd add a sentence here explaining why you're using the measure-theoretic approach. 
%
% ^ <lachlan@mccalman.info> 2016-10-05T23:46:46.888Z.
	
	\begin{definition} \label{def:distribution}
		\citep{bauer1981probability}
		The \textit{distribution} of a random variable $\rv{X} : \Omega \to \mathcal{X}$ is the Borel measure $\mathbb{P}_{\rv{X}} : \mathcal{A} \to [0, 1]$ defined by
		\begin{equation}
			\mathbb{P}_{\rv{X}}[A] := \mathbb{P}[\rv{X} \in A]
		\label{eq:distribution}
		\end{equation}
		where the event $\{\rv{X} \in A\} := \rv{X}^{-1}[A] := \{\omega \in \Omega : \rv{X}(\omega) \in A\} \in \mathcal{W}$ is the inverse image (pre-image) of $A \subseteq \mathcal{X}$ under the random variable $\rv{X}$.
	\end{definition}
	
	\begin{definition} \label{def:cdf}
		\citep{bauer1981probability}
		The \textit{cumulative distribution function} (CDF) $P_{\bvec{\rv{X}}} : \mathbb{R}^{d} \to [0, 1]$ of a \textit{real-valued} random vector $\bvec{\rv{X}} : \Omega \to \mathbb{R}^{d}$ is defined by
		
		\begin{equation}
				P_{\bvec{\rv{X}}}(\bvec{x}) := \mathbb{P}_{\bvec{\rv{X}}}[(-\bm{\infty}, \bvec{x}]] = \mathbb{P}[\bvec{\rv{X}} \leq \bvec{x}]
		\label{eq:cdf}
		\end{equation}

		For conciseness, we define the following shorthand \eqref{eq:shorthand}. 
		\begin{equation}
			\begin{aligned}
				\{\bvec{\rv{X}} \leq \bvec{x}\} &:= \bigcap\limits_{j = 1}^{d} \{\rv{X}_{j} \in (-\infty, x_{j}]\} \in \mathcal{W} \\
				(-\bm{\infty}, \bvec{x}] &:= (-\infty, x_{1}] \times \dots \times (-\infty, x_{d}] \\
			\end{aligned}
		\label{eq:shorthand}
		\end{equation}
	\end{definition}
		
	\subsection{Kernel Embedding}
	\label{sec:background:kernel_embedding}
	
		This section summarises fundamental and important results in the field of kernel embeddings that form the background of this paper. For more detail and proofs, see the supplementary material provided.

		In this paper we assume all kernels involved are positive definite and characteristic. That is, the kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ defines a unique reproducing kernel Hilbert space (RKHS), denoted by $\mathcal{H}_{k}$ \citep{fukumizu2004dimensionality}. Let $\rv{X} : \Omega \to \mathcal{X}$ and $\rv{Y} : \Omega \to \mathcal{Y}$ be random variables with distributions $\mathbb{P}_{\rv{X}}$ and $\mathbb{P}_{\rv{Y}}$, and let $k$ and $l$ be characteristic positive definite kernels defined on $\mathcal{X}$ and $\mathcal{Y}$ respectively. Let $f \in \Hk$ and $g \in \Hl$ be real-valued functions.
		
		The feature function of an RKHS can be obtained from the partially applied kernel \eqref{eq:feature_functions}, and forms the basis for the full RKHS \citep{muandet2016kernel}.
			
		\begin{equation}
			\phi_{x} := k(x, \cdot) \in \Hk ;\quad \psi_{y} := l(y, \cdot) \in \Hl
		\label{eq:feature_functions}
		\end{equation}

		An inner product in the RKHS satisfies the reproducing property \eqref{eq:reproducing_property}, where the function evaluation operator within the full RKHS is the inner product with the feature function in that RKHS \citep{muandet2016kernel}.

		\begin{equation}
			\inner{\phi_{x}}{f} = f(x) ;\quad \inner{\psi_{y}}{g} = g(y)
		\label{eq:reproducing_property}
		\end{equation}
		
%		\extra{
%		In particular, the kernel value between two points can be evaluated as the inner product between the feature functions at those points \eqref{eq:kernel_reproducing_property}.
%
%		\begin{equation}
%			\inner{\phi_{x}}{\phi_{x'}} = k(x, x') ;\quad \inner{\psi_{y}}{\psi_{y'}} = l(y, y')
%		\label{eq:kernel_reproducing_property}
%		\end{equation}
%		}
		
		\begin{definition} \label{def:kernel_embedding}
			\citep{smola2007hilbert}
			The \textit{kernel embedding} (or \textit{mean embedding}) of a distribution $\mathbb{P}_{\rv{X}}$ is defined as the expectation of the corresponding feature function in that RKHS \eqref{eq:kernel_embedding}.
			\begin{equation}
				\muX \equiv \muX(\cdot) := \expect{\phiX} = \expect{k(\rv{X}, \cdot)} \in \Hk
			\label{eq:kernel_embedding}
			\end{equation}
		\end{definition}
		
		The power of kernel embeddings comes from the fact that expectations of a function can be evaluated as the inner product \eqref{eq:function_expectation} between the kernel embedding and the function \citep{muandet2016kernel}.
		
		\begin{equation}
			\inner{\muX}{f} = \expect{f(\rv{X})} ;\quad \inner{\muY}{g} = \expect{g(\rv{Y})}
		\label{eq:function_expectation}
		\end{equation}

		
		\subsubsection{Empirical Representation}
		\label{sec:background:kernel_embeddings:empirical_representation}
			
			In practice, the actual probability distributions of interest are not available in closed form. Instead, independent and identically distributed (\textit{iid}) samples from such probability distributions are available. Suppose that joint samples $\{x_{i}, y_{i}\}_{i = 1}^{n}$ are observed and collected in an \textit{iid} fashion from the joint distribution $\mathbb{P}_{\rv{X} \rv{Y}}$. Let $\ds{X} := \{x_{i}\}_{i = 1}^{n}$ and $\ds{Y} := \{y_{i}\}_{i = 1}^{n}$. It is possible to represent kernel embeddings empirically such that in the limit of infinite data, the empirical representations would converge to the true representations at an appropriate rate. Most significant, however, is the fact that important results for manipulating kernel embeddings also hold for their empirical representations \citep{muandet2016kernel}.
			
			By replacing the expectation with an empirical average, the resulting estimator \eqref{eq:empirical_embedding} converges to the true embedding \eqref{eq:kernel_embedding} at rate $O(n^{-1/2})$ \citep{smola2007hilbert}

			\begin{equation}
				\hatmuX := \frac{1}{n} \sum_{i = 1}^{n} \phi_{x_{i}} ;\quad \hatmuY := \frac{1}{n} \sum_{i = 1}^{n} \psi_{y_{i}}
			\label{eq:empirical_embedding}
			\end{equation}

%			Correspondingly, the empirical representation of a function expectation is simply the empirical average of the function, and can be obtained by the inner product of the function with the empirical embedding \eqref{eq:empirical_function_expectation}.
%
%			\begin{equation}
%				\inner{\hatmuX}{f} = \frac{1}{n} \sum_{i = 1}^{n} f(x_{i}) ;\quad \inner{\hatmuY}{g} = \frac{1}{n} \sum_{i = 1}^{n} g(y_{i})
%			\label{eq:empirical_function_expectation}
%			\end{equation}

		
%			Since the samples $\ds{X} := \{x_{i}\}_{i = 1}^{n}$ and $\ds{Y} := \{y_{i}\}_{i = 1}^{n}$ are sampled from the joint distribution $\mathbb{P}_{\rv{X} \rv{Y}}$ such that marginal samples are simply $\ds{X}$ and $\ds{Y}$ themselves, the empirical representations for regular embeddings $\muX$ and $\muY$ are easy to obtain, as per \cref{thm:empirical_embedding}.
%			
%			However, instead of a uniform average, a empirical representations of the conditional embedding would require a weighted average of the following sform \eqref{eq:empirical_conditional_embedding_form} instead.
%			
%			\begin{equation}
%			\begin{aligned}
%			\hatmuYx &= \sum_{i = 1}^{n} \alpha_{i} \psi_{y_{i}} ;\quad \hatmuXy &= \sum_{i = 1}^{n} \beta_{i} \phi_{x_{i}}
%			\label{eq:empirical_conditional_embedding_form}
%			\end{aligned}
%			\end{equation}
%			
%			where $\bm{\alpha} := \{\alpha_{i}\}_{i = 1}^{n}$ is to be determined from the samples $\ds{X}$ and $\bm{\beta} := \{\beta_{i}\}_{i = 1}^{n}$ is to be determined from the samples $\ds{Y}$.
%			
%			Before deriving the expressions for $\bm{\alpha}$ and $\bm{\beta}$ however, it is useful to introduce matrix notation for representing empirical embeddings.
%			
%			The feature functions can be viewed as an feature vector of the same dimension as the cardinality of the its domain. For example, if the domain $\mathcal{X} = \mathbb{R}^{d}$ is the $d$ dimensional euclidean space whose cardinality is uncountably infinite, then the feature function can be viewed as an uncountably infinite dimensional feature vector, indexed by the elements of $\mathcal{X} = \mathbb{R}^{d}$.
%			
%			Denote the cardinality of $\mathcal{X}$ as $\cardX$, and similarly the cardinality of $\mathcal{Y}$ as $\cardY$. With the observations $\{x_{i}, y_{i}\}_{i = 1}^{n}$ sampled from $\mathbb{P}_{\rv{X} \rv{Y}}$, there are $n$ feature vectors for each RKHS, $\{\phi_{x_{i}}\}_{i = 1}^{n}$ for $\Hk$ and $\{\psi_{y_{i}}\}_{i = 1}^{n}$ for $\Hl$. As elements within their respective RKHS for which Hilbert-Schmidt operators can operate on, $\phi_{x_{i}}$ has an effective dimension of $\cardX \times 1$ and $\psi_{y_{i}}$ has an effective dimension of $\cardY \times 1$. Since this can be said about the feature functions, which form the basis for the RKHS, this also applies to general functions within the RKHS. As such, the inner product between functions $f_{1}, f_{2} \in \Hk$ can also be written in matrix notation as $\inner{f_{1}}{f_{2}} = f_{1}^{T} f_{2}$. It can be conceptually helpful to check that $f_{1}^{T}$ is of size $1 \times \cardX$ and $f_{2}$ is of size $\cardX \times 1$ so that the result is a scalar of size $1 \times 1$.
%			
%			Recall that a matrix $A := \begin{bmatrix} \bvec{a}_{1} & \cdots & \bvec{a}_{n} \end{bmatrix} \in \mathbb{R}^{n \times n}$, $\bvec{a}_{i} \in \mathbb{R}^{n} \; \forall i \in \{1, \dots, n\}$, operated on a vector $\bvec{v} \in \mathbb{R}^{n}$ results in a vector that is the linear combination of the columns of $A$ with coefficients given by the components of $\bvec{v}$. That is, $A \bvec{v} = \sum_{i = 1}^{n} v_{i} \bvec{a}_{i}$.
%			
%			Similarly, a feature matrix can be defined in the same way such that empirical representations of kernel embeddings can be reduced down to linear algebraic operations.
%			
			With an abuse of notation, a feature matrix $\Phi \equiv \Phi_{\ds{X}}$ is formed by stacking the corresponding feature vectors or functions \textit{horizontally} \eqref{eq:feature_matrix}, where each feature vector represents a column of that matrix  \citep{muandet2016kernel}. Here, the subscripts of $\Phi$ and $\Psi$ are dropped whenever it is clear from context the dataset involved. In this way, $\Phi$ has effective size $\cardX \times n$ and $\Psi$ has effective size $\cardY \times n$, where $\Vert \cdot \Vert$ denotes the cardinality of a set, which is uncountably infinite in the case for continuous Euclidean spaces.
				
			\begin{equation}
				\Phi := \begin{bmatrix} \phi_{x_{1}} & \cdots & \phi_{x_{n}} \end{bmatrix}; \Psi := \begin{bmatrix} \psi_{y_{1}} & \cdots & \psi_{y_{n}} \end{bmatrix}
			\label{eq:feature_matrix}
			\end{equation}
				

			With this notation, general empirical embeddings can be written with matrix notation \eqref{eq:empirical_conditional_embedding_matrices}, where $\bm{\gamma} := \{\gamma_{i}\}_{i = 1}^{n}$ depends on $\ds{X}$ and $\bm{\beta} := \{\beta_{i}\}_{i = 1}^{n}$ depends on $\ds{Y}$. For marginal (non-conditional) embeddings, the weights become uniform such that $\gamma_{i} = \frac{1}{n}$ and $\beta_{i} = \frac{1}{n}$ for $i \in \{1, \dots, n\}$.
			
			\begin{equation}
				\hatmuXy = \sum_{i = 1}^{n} \beta_{i} \phi_{x_{i}} = \Phi \bm{\beta} ;\quad \hatmuYx = \Psi \bm{\gamma}
			\label{eq:empirical_conditional_embedding_matrices}
			\end{equation}
		
	\subsection{Quantiles and Quantile Regression}
	\label{sec:background:quantiles}

		\begin{definition} \label{def:quantile}
			\citep{takeuchi2006nonparametric}
			Consider a real-valued random variable $\rv{X} : \Omega \to \mathbb{R}$. The \textit{$\tau$-quantiles}, $\tau \in (0, 1)$, of $\mathbb{P}_{\rv{X}}$ and $P_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}$ are defined as follows \eqref{eq:quantile}.
			\begin{equation}
				\begin{aligned}
					q_{\rv{X}}(\tau) &:= \inf\{x : P_{\rv{X}}(x) \geq \tau\} \equiv \inf_{P_{\rv{X}}(x) \geq \tau} x \\
					q_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}(\tau) &:= \inf\{x : P_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}(x) \geq \tau\} \equiv \inf_{P_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}(x) \geq \tau} x
				\end{aligned}
			\label{eq:quantile}
			\end{equation}
		\end{definition}

		Finding the conditional quantile is an identical problem to finding a regular quantile, except with the CDF replaced by a conditional CDF. As such, for brevity and simpleness, in this paper we will present CDF estimation from a general embedding with general weights, whose form both conditional embeddings and regular embeddings share.

		Quantile regression is then the problem of finding an empirical estimate $\hat{q}_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}(\tau)$ for $q_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}(\tau)$ given a set of paired observations $\{(\bvec{y}_{i}, x_{i})\}_{i = 1}^{n}$ and a query point $\bvec{y}$.
		
		In practice, the infimum is often replaced with the minimum. If the CDF estimate is smooth and always increasing, the inequality becomes an equality, and obtaining quantiles becomes a root finding procedure once an estimate of the CDF is obtained \eqref{eq:quantile_root_finding}. In this paper, we present methods for estimating the CDF from a dataset.
		
		\begin{equation}
			q_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}(\tau) = x : P_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}(x) = \tau
		\label{eq:quantile_root_finding}
		\end{equation}	
	
\section{Direct Quantile Regression}
\label{sec:direct_quantile_regression}

	The direct approach to quantile regression refers to techniques which computes an estimate of the CDF directly, without needing to compute the PDF first.
	
	Using \eqref{eq:function_expectation}, the distribution can be written as an inner product with the kernel embedding \eqref{eq:distribution_inner}.
		
	\begin{equation}
		\mathbb{P}_{\bvec{\rv{X}}}[A] = \mathbb{E}[\mathbb{1}_{A}(\bvec{\rv{X}})] = \langle \mu_{\rv{X}}, \mathbb{1}_{A} \rangle
	\label{eq:distribution_inner}
	\end{equation}

	The CDF  $P_{\rv{X}}$ reveals a similar structure to the distribution $\mathbb{P}_{\rv{X}}$ through \cref{def:cdf} \eqref{eq:cdf_inner}.
			
	\begin{equation}
		P_{\bvec{\rv{X}}}(\bvec{x}) := \mathbb{P}_{\bvec{\rv{X}}}[(-\bm{\infty}, \bvec{x}]] = \langle \mu_{\bvec{\rv{X}}}, \mathbb{1}_{(-\bm{\infty}, \bvec{x}]} \rangle
	\label{eq:cdf_inner}
	\end{equation}

	We will now focus our attention on how to estimate the distribution and CDF from a given dataset.
	
	\subsection{Optimal Function Projection}
	\label{sec:direct_quantile_regression:optimal_function_approximation}
	
		In order to estimate the distribution and CDF empirically, we would need both the empirical kernel embedding $\hat{\mu}_{\rv{X}}$ and a projection $\hat{\mathbb{1}}_{A}$ of the indicator function $\mathbb{1}_{A}$ into the same RKHS as $\hat{\mu}_{\rv{X}}$ so that a proper inner product within that RKHS can be evaluated.

		Suppose $\ds{X} := \{\bvec{x}_{1}, \dots, \bvec{x}_{n}\} \in \mathbb{R}^{n \times d}$ is set of observations from $\mathbb{P}_{\rv{X}}$. Functions that can be represented by these samples are linear combinations of the feature functions $\phi_{\bvec{x}_{i}}$ at the observations \eqref{eq:function_projection} \citep{muandet2016kernel}. The span obtained by those basis $\{\phi_{\bvec{x}_{i}}\}_{i = 1}^{n}$ also forms a RKHS, denoted by $\mathcal{H}_{k, \ds{X}}$, and is a finite vector subspace of the full RKHS $\mathcal{H}_{k}$.
		
		\begin{equation}
			\hat{f} = \sum_{i = 1}^{n} \alpha_{i} k(\bvec{x}_{i}, \cdot) = \sum_{i = 1}^{n} \alpha_{i} \phi_{\bvec{x}_{i}} = \Phi \bm{\alpha}
		\label{eq:function_projection}
		\end{equation}
	
		We are interested in approximating the function $f \in \mathcal{H}_{k}$ with $\hat{f} \in \mathcal{H}_{k, \ds{X}}$ so that they are close as measured by the norm induced by the full RKHS $\mathcal{H}_{k}$, by tuning the embedding weights $\bm{\alpha} = \{\alpha_{i}\}_{i = 1}^{n}$ \eqref{eq:function_projection_optimisation}.
			
		\begin{equation}
			\bm{\alpha}^{\star} = \argmin_{\bm{\alpha} \in \mathbb{R}^{n}} \Vert \hat{f} - f \Vert
		\label{eq:function_projection_optimisation}
		\end{equation}
		
		\begin{theorem} \label{thm:function_projection_solution}
			The optimisation problem \eqref{eq:function_projection_optimisation} has a closed form solution $\bm{\alpha}^{\star} = K^{-1} \bvec{f}$ \eqref{eq:function_projection_solution}, where $K \equiv K_{\ds{X} \ds{X}} := \{k(\bvec{x}_{i}, \bvec{x}_{j})\}_{i = 1, j = 1}^{n, n}$ is the gram matrix and $\bvec{f} := f\{\ds{X}\} := \{f(\bvec{x}_{i})\}_{i = 1}^{n}$ is the function evaluated at the observed points \eqref{eq:function_projection_solution}. See supplementary for proof.
			
			\begin{equation}
				\hat{f} = \Phi \bm{\alpha}^{\star} = \Phi K^{-1} \bvec{f}
			\label{eq:function_projection_solution}
			\end{equation}
		\end{theorem}
		
	\subsection{Direct Empirical Distribution}
	\label{sec:direct_quantile_regression:optimal_empirical_distribution}
	
		By \cref{thm:function_projection_solution} and letting $f = \mathbb{1}_{A}$,  the projection of an indicator function into $\mathcal{H}_{k, \ds{X}}$ \eqref{eq:indicator_projection} can be used to derive the direct empirical distribution.
			
		\begin{equation}
			\hat{\mathbb{1}}_{A} = \Phi K^{-1} \mathbb{1}_{A}\{\ds{X}\}
		\label{eq:indicator_projection}
		\end{equation}
			
		\begin{theorem} \label{thm:empirical_distribution_and_cdf}
			Assume that a kernel $k$ is chosen such that the indicator function is in $\mathcal{H}_{k}$. The direct empirical distribution measured on a set is simply the sum of weights on that set \eqref{eq:empirical_distribution}. See supplementary for proof.
			\begin{equation}
				\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] := \inner{\hat{\mu}_{\bvec{\rv{X}}}}{ \hat{\mathbb{1}}_{A}} = \sum_{\bvec{x}_{i} \in A} \beta_{i}
			\label{eq:empirical_distribution}
			\end{equation}
			By \cref{def:cdf}, the CDF shows a similar form \eqref{eq:empirical_cdf}.
			\begin{equation}
				\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) := \hat{\mathbb{P}}_{\bvec{\rv{X}}}[(-\bm{\infty}, \bvec{x}]] = \sum_{i : \bvec{x}_{i} \leq \bvec{x}} \beta_{i}
			\label{eq:empirical_cdf}
			\end{equation}
		\end{theorem}

		There is a subtlety in this derivation. \Cref{thm:empirical_distribution_and_cdf} restricts the class of available kernels further as not all RKHS $\mathcal{H}_{k}$ would contain the indicator function. \cite{kanagawa2014recovering} showed that the estimator $\langle \hat{\mu}_{\bvec{\rv{X}}}, \hat{f} \rangle$ is consistent for $\mathbb{E}[f(\bvec{\rv{X}})]$ if $f$ is in the Besov space, which is a superset of the full RKHS $\mathcal{H}_{k}$. While indicator functions are included in the Besov space only under certain assumptions, the estimators we derived for its expectation, by assuming that it is contained in the RKHS, empirically converges to the actual expectation.

	\subsection{Smooth Empirical Distribution}
	\label{sec:direct_quantile_regression:smooth_empirical_distribution}

		\begin{figure}
			\begin{center}
				\includegraphics[width=\columnwidth]{figures/cumulativeexamplesmooth}
				\includegraphics[width=\columnwidth]{figures/cumulativeexampleerf}
			\end{center}
			\caption{\small (Left) The PDF obtained through the pre-image method on the Motorcycle dataset (blue) and The embedding weights at the training points (red). (Right) The resulting CDF estimate using the direct estimate \eqref{eq:empirical_cdf} and smooth estimate \eqref{eq:smooth_empirical_cdf_gaussian_kernel}. (Bottom) The smooth CDF estimate again (green) with its individual error function components (red) overlayed \eqref{eq:qpi_empirical_cdf_gaussian_kernel}.}
			\label{fig:direct_quantile_regression}
		\end{figure}
		
		By embedding indicator functions, the resulting CDF estimate \eqref{eq:empirical_cdf} is discontinuous at the jumps located at each data point. Directly differentiating this would result in a superposition of Dirac deltas for the PDF. However, in many applications, the PDF is rather smooth, and thus so is the CDF. Many characteristic kernels are also smooth, and thus so are the functions within their RKHS. Therefore, instead of taking the inner product between the embedding and the indicator function, we propose to take the inner product with a function $\tilde{\mathbb{1}}_{A}$ \eqref{eq:indicator_smooth} that is within the full RKHS $\mathcal{H}_{k}$ and converges to the indicator function $\mathbb{1}_{A}$ as the kernel bandwidth goes to zero.
		
		\begin{equation}
			\tilde{\mathbb{1}}_{A} := \int_{A} k(\bvec{x}, \cdot) d\bvec{x} = \int_{A} \phi_{\bvec{x}} d\bvec{x}
		\label{eq:indicator_smooth}
		\end{equation}
		
		\begin{theorem} \label{thm:smooth_empirical_distribution_and_cdf}
			The smooth distribution \eqref{eq:smooth_empirical_distribution} and CDF \eqref{eq:smooth_empirical_cdf} estimate is simply the integral of the normalised embedding.
			\begin{equation}
				\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] := \inner{\hat{\mu}_{\bvec{\rv{X}}}}{\tilde{\mathbb{1}}_{A}} = \sum_{i = 1}^{n} \beta_{i} \int_{A}  k(\bvec{x}_{i}, \bvec{x}) d\bvec{x}
			\label{eq:smooth_empirical_distribution}
			\end{equation}	
			\begin{equation}
				\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) = \sum_{i = 1}^{n} \beta_{i} \int_{-\bm{\infty}}^{\bvec{x}}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' = \bm{\beta}^{T} \bvec{K}(\bvec{x})
			\label{eq:smooth_empirical_cdf}
			\end{equation}
			\begin{equation}
				\bvec{K}(\bvec{x}) := \Bigg\{ \int_{-\bm{\infty}}^{\bvec{x}}  k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' \Bigg\}_{i = 1}^{n}
			\label{eq:kernel_integral}
			\end{equation}
		\end{theorem}

		The estimates are expressed as a summation of the kernel integrals, which would need to be analytically derived or otherwise estimated. For a Gaussian kernel, analytical form can be derived \eqref{eq:smooth_empirical_cdf_gaussian_kernel}.

		\begin{equation}
			\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) = \frac{1}{2} \sum_{i = 1}^{n} \beta_{i} \prod_{j = 1}^{d} \Bigg[1 + \mathrm{erf}\bigg(\frac{x_{j} - x_{i, j}}{\sigma_{j} \sqrt{2}}\bigg)\Bigg]
		\label{eq:smooth_empirical_cdf_gaussian_kernel}
		\end{equation}

		Even if analytical form cannot be derived for some kernels $k$, since it is independent of the embedding weights, the integral $\bvec{K}(\bvec{x}) \equiv \bvec{K}_{\ds{X}}(\bvec{x})$ \eqref{eq:kernel_integral} can be approximated and pre-computed before inference begins.

		With this approach, CDF estimates are now smooth and can be shown to converge to the true CDF in the limit of $n \rightarrow \infty$ and vanishing kernel bandwidth. However, due to the possibility of negative weights $\beta_{i}$, both direct approaches may produce CDF estimates that are not strictly non-decreasing, as shown in \cref{fig:direct_quantile_regression}. Notice that the direct non-smooth direct CDF estimate jumps at locations corresponding to training points, and that, due to the negative weights, the smooth CDF estimate contain components that go below zero such that the resultant CDF estimate may not be strictly non-decreasing. Furthermore, the resulting CDF may not be in the range $[0, 1]$. To rectify this, we propose a Pre-Image approach to estimating the CDF.

\section{Pre-Image Quantile Regression}
\label{sec:pre_image_quantile_regression}

%	\warn{So we need to cite MKBR before we talk about integrating it. The integration part is relatively short and straight forward, so the section would comprise of deriving and discussing MKBR with examples, and then discuss integration briefly, and then present example figures.}
	
	Given a particular embedding, the empirical PDF can be recovered through the quadratic programming pre-image (\qpi) method proposed by \cite{mccalman2013multi}. This method represents the PDF as a mixture of the features $k(\bvec{x}_{i}, \cdot)$ centred at each of the data points, and thus has as many components as the number of data points. Through minimising the RKHS distance between the true PDF and the PDF estimate, the resulting algorithm reduces to a tractable quadratic programming optimisation problem, which finds the optimal weights $\bvec{w} = \{w_{i}\}_{i = 1}^{n}$ for which $\hat{p}_{\bvec{\rv{X}}}$ is closest to the true PDF \eqref{eq:qpi}. 
	
	\begin{equation}
		\hat{p}_{\bvec{\rv{X}}}(\bvec{x}) = \sum_{i = 1}^{n} w_{i} k(\bvec{x}_{i}, \bvec{x})
	\label{eq:qpi}
	\end{equation}

	Since \qpi\space recovers a valid PDF that is as smooth as the kernel $k$, integrating this PDF would result a smooth, proper distribution $\hat{\mathbb{P}}_{\bvec{\rv{X}}}$ \eqref{eq:qpi_empirical_distribution} and CDF $\hat{P}_{\bvec{\rv{X}}}$ \eqref{eq:qpi_empirical_cdf} that is always non-decreasing.
	
	\begin{equation}
		\hat{\mathbb{P}}_{\bvec{\rv{X}}}[A] = \int_{A} \hat{p}_{\bvec{\rv{X}}}(\bvec{x}) d\bvec{x} = \sum_{i = 1}^{n} w_{i} \int_{A} k(\bvec{x}_{i}, \bvec{x}) d\bvec{x}
	\label{eq:qpi_empirical_distribution}
	\end{equation}
	
	\begin{equation}
		\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) = \sum_{i = 1}^{n} w_{i} \int_{-\bm{\infty}}^{\bvec{x}} k(\bvec{x}_{i}, \bvec{x}') d\bvec{x}' =  \bvec{w}^{T} \bvec{K}(\bvec{x})
	\label{eq:qpi_empirical_cdf}
	\end{equation}
	
	Similar to the smooth distribution estimate \eqref{eq:smooth_empirical_distribution} and CDF estimate \eqref{eq:smooth_empirical_cdf}, the estimates \eqref{eq:qpi_empirical_distribution} and \eqref{eq:qpi_empirical_cdf} require the integral of the kernel $k$, which can either be analytically derived or at least approximated and pre-computed beforehand. Again, analytical form is available for Gaussian kernels \eqref{eq:qpi_empirical_cdf_gaussian_kernel}.
	
	\begin{equation}
		\hat{P}_{\bvec{\rv{X}}}(\bvec{x}) = \frac{1}{2} \sum_{i = 1}^{n} w_{i} \prod_{j = 1}^{d} \Bigg[1 + \mathrm{erf}\bigg(\frac{x_{j} - x_{i, j}}{\sigma_{j} \sqrt{2}}\bigg)\Bigg]
	\label{eq:qpi_empirical_cdf_gaussian_kernel}
	\end{equation}
	
	This CDF estimate is essentially of the same form as the smooth direct estimate \eqref{eq:smooth_empirical_cdf_gaussian_kernel}, but with the embedding weights $\bm{\beta}$ replaced by the recovered density weights $\bvec{w}$. Since the density weights $\bvec{w}$ are by construction of \qpi\space always positive, unlike the embedding weights $\bm{\beta}$, the resulting CDF is guaranteed to be non-decreasing.
 
\section{Hyperparameter Learning}
\label{sec:hyperparameter_learning}

	With all kernel based techniques, the kernel $k = k_{\bm{\theta}}$ itself is usually specified from a family of kernels parametrised by some hyperparameters $\bm{\theta}$. For example, a Gaussian kernel would be specified by its length scale parameters. Whichever family we choose, we would have to find the optimal hyperparameters that results in the best inference accuracy. %The number of parameters would depend on whether the kernel is isotropic ($1$ parameter), anisotropic axis-aligned ($d$ parameters), or general ($d(d - 1)/2$ parameters).
	
	We considered two approaches to hyperparameter learning for the purpose of quantile regression.
	
	The first approach involves minimising the expected pinball loss \eqref{eq:expected_pinball_loss} for a dataset $\ds{X} := \{x_{i}\}_{i = 1}^{n}$, where the pinball loss is defined by \eqref{eq:pinball_loss}. The minimiser of the expected pinball loss \eqref{eq:expected_pinball_loss} is the $\tau$-quantile $z = q_{\rv{X}}(\tau)$ of the distribution $\mathbb{P}_{\rv{X}}$ \citep{koenker1978regression}.
	
	\begin{equation}
		L_{\tau}(x, z) = \left\{ \begin{array}{lr}
			(x - z) \tau & : x \geq z \\
			(z - x) (1 - \tau) & : x < z
		\end{array} \right.
	\label{eq:pinball_loss}
	\end{equation}
	
	This expected pinball loss \eqref{eq:expected_pinball_loss} is evaluated over a leave-out validation set from the available training data obtained from a standard cross-validation procedure. We will call this approach \textit{pinball learning}.
	
	\begin{equation}
		\bar{L}_{\tau}(\ds{X}, z) := \frac{1}{n} \sum_{i = 1}^{n} L_{\tau}(x_{i}, z)
	\label{eq:expected_pinball_loss}
	\end{equation}
	
	The expected pinball loss is tolerant to CDF estimates which may not be strictly non-decreasing, such as \eqref{eq:empirical_cdf} and \eqref{eq:smooth_empirical_cdf}. This however also means that the loss function is specific to a particular quantile such that each quantile estimate requires separate optimisations despite using the same training set. The result is that quantile estimates may actually cross for certain query locations, a phenomenon known as quantile crossing.
	
	Instead, we can optimise the parameters with respect to the joint probability of observing the collected dataset \eqref{eq:lml}, also called the marginal likelihood. Evaluated over the whole training set, this is akin to method of log marginal likelihood optimisation in the Gaussian process context. However, here we again choose to only evaluate it over a leave-out validation set. We will call this approach \textit{marginal likelihood learning}. The advantage of this approach is that it is independent of the particular quantile estimate chosen. As such, a single, consistent CDF is generated for all quantile queries, eliminating the possibility of quantile crossing. 
	
	\begin{equation}
		p_{\rv{X}^{(n)}, \bm{\theta}}(x_{1}, \cdots, x_{n}) = \sum_{i = 1}^{n} \log(\hat{p}_{\rv{X}, \bm{\theta}}(x_{i}))
	\label{eq:lml}
	\end{equation}
	
	However, clearly this requires the PDF to be estimated first, even though direct approaches do not require them. Thus, this method is more natural in the Pre-Image approach. Nevertheless, this is only required during the learning stage. Once the optimal hyperparameters are obtained, they can be used in the direct approach for inference.
	
	\begin{figure*}[t]
		\centering
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{figures/Antigen_results}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{figures/Bone_Mass_Density_results}
		\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
			\includegraphics[width=\textwidth]{figures/Weather_results}
		\end{subfigure}
		\caption{Results from the three quantile experiments. The error bars in these figures represent $\pm 1$ standard deviation of the pinball loss over the testing set.}
		\label{fig:qfull}
	\end{figure*}

\section{Quantile Regression Algorithms}
\label{sec:quantile_regression_algorithms}
	
	Suppose some data $\{(\bvec{y}_{i}, x_{i})\}_{i = 1}^{n}$ is collected from a joint distribution $\mathbb{P}_{\rv{Y} \rv{X}}$. We can either find the standard empirical embedding $\hat{\mu}_{\rv{X}}$ or empirical conditional embedding $\hat{\mu}_{\rv{X} | \bvec{\rv{Y}} = \bvec{y}}$, both of which is of the form $\Phi \bm{\beta}$. This is also true for posterior embeddings obtained from KBR \citep{fukumizu2013kernel}. In this paper we present the following algorithms for quantile regression based on the methods proposed in this paper. Table \ref{table:quantile_regression_methods} lists the properties of each of these algorithms.

	\theoremstyle{definition}
	\begin{definition}
		Motivated by \cref{thm:empirical_distribution_and_cdf}, the \textit{direct embedding $\tau$-quantile estimate} (DR) is
		\begin{equation}
		q_{\rv{X}}(\tau) = \min\{x \in \mathbb{R} : \frac{1}{\sum_{i = 1}^{n} \beta_{i}} \sum_{i : x_{i} \leq x} \beta_{i} \geq \tau\}
		\end{equation}	
		The normalisation constant ensures that the CDF reaches 1. The hyperparameter learning is done by minimising the pinball loss using cross-validation.
	\end{definition}
	
	\theoremstyle{definition}
	\begin{definition}
		Motivated by \cref{thm:smooth_empirical_distribution_and_cdf}, the \textit{smooth embedding $\tau$-quantile estimate} (DS) is
		\begin{equation}
		q_{\rv{X}}(\tau) = x : \frac{1}{\int_{\mathbb{R}} k(0, x) dx \sum_{i = 1}^{n} \beta_{i}} \bm{\beta}^{T} \bvec{K}(x) = \tau
		\end{equation}	
		Similarly, we apply a normalisation constant, and pinball learning is used to learn the hyperparameters.
	\end{definition}
	
	\theoremstyle{definition}
	\begin{definition}
		Using \eqref{eq:qpi_empirical_cdf} directly, the \textit{pre-image $\tau$-quantile estimate} is
		\begin{equation}
			q_{\rv{X}}(\tau) = x : \bvec{w}^{T} \bvec{K}(x) = \tau
		\end{equation}	
		where $\bvec{w}$ is are the recovered density weights from QPI.
		Both pinball and marginal likelihood learning can be used. We denote this algorithm as JB with done with pinball learning, and JL when done with marginal likelihood learning.
	\end{definition}
	
	Since obtaining the density weights through \qpi\space requires a separate optimisation loop, this increases the time complexity of the algorithm significantly (see \cref{table:quantile_regression_methods}). At the expense of accuracy, the density weights can be instead obtained through clipping the embedding weights at zero and normalising them such that the resulting weights are valid PDF weights. For more detail, see the supplementary material. We then replace $\bvec{w}$ with these new and cheaply computable weights $\bvec{\tilde{w}}$ in \eqref{eq:qpi_empirical_distribution} and \eqref{eq:qpi_empirical_cdf}.
% * <lachlan@mccalman.info> 2016-10-06T00:07:22.658Z:
%
% There's a fukumizu paper I cite for this clipping process in my chapter 3 (can't remember now). Probably worth citing here.
%
% ^.

	\theoremstyle{definition}
	\begin{definition}
		The \textit{clip-normalised $\tau$-quantile estimate} is
		\begin{equation}
		q_{\rv{X}}(\tau) = x : \bvec{\tilde{w}}^{T} \bvec{K}(x) = \tau
		\end{equation}
		where $\bvec{\tilde{w}}$ is the clip-normalised weights of the relevant embedding.
		Both pinball learning and marginal likelihood learning can be used. We denote this algorithm as NB with done with pinball learning, and NL when done with marginal likelihood learning.
	\end{definition}	
	
	\begin{table}[t!]
% * <lachlan@mccalman.info> 2016-10-06T00:07:58.096Z:
% 
% If you want to double check these complexity bounds I'd appreciate it  =)
% 
% ^.
		\begin{center}
			\begin{tabular}{l|cccc}
				Algorithm & S & ND &   NC & Complexity \\ \hline
				DR  &              &                &                & $O(n \log(n))$    \\
				DS  & $\checkmark$ &                &                &
				$O(n \log(n))$  \\
				NB  & $\checkmark$ & $\checkmark$   &                &
				$O(n \log(n))$ \\
				JB  & $\checkmark$ & $\checkmark$   &                &
				$O(n^{3} \log(n))$ \\
				NL  & $\checkmark$ & $\checkmark$   & $\checkmark$   &
				$O(n \log(n))$ \\
				JL  & $\checkmark$ & $\checkmark$   & $\checkmark$   &   $O(n^{3} \log(n))$ 
			\end{tabular}
		\end{center}
		\caption{\small Comparison of Quantile estimation techniques. S stands for Smooth, ND for Non-Decreasing, NC for Non-Crossing, and $n$ is the number of training points.}
		\label{table:quantile_regression_methods}
	\end{table}
	
%	\subsection{Computational Complexity}
%	\label{sec:quantile_regression_algorithms:computational_complexity}
		
	The computational complexity of these algorithms varies from $O(n \log(n))$ to $O(n^{3} \log(n))$ in the number of training points $n$ (\cref{table:quantile_regression_methods}). The 1-D root-finding required to solve the quantile equations adds a factor of $\log(n)$ in all cases (using a binary search). The difference lies in the CDF estimation technique.
	
	The cheapest cumulative evaluate is the direct embedding estimator (DR) --- this is simply a (conditional) sum of the mixture weights and is therefore $O(n)$ in the number of training points. The smooth embedding estimator (DS) and the normed weights estimator (NL and NB) are also inexpensive to compute, provided an efficient estimate of the kernel integral exists. They are also $O(n)$, but with a larger constant due to the kernel integration. The most expensive are the \qpi\space-based pre-image estimators (JL and JB), which require solving a quadratic program to determine the mixture weights and are therefore $O(n^3)$. This is only relevant if a PDF estimate was not required, and the pre-image was computed only for the quantiles. If a pre-image estimate is already available then the JL and JB are also $O(n)$.
	
\section{Related Work}
\label{sec:related_work}

	Quantile regression was first introduced by \cite{koenker1978regression}, which considered inference based on an unknown linear model. Being more robust to noise and thus effective in modelling non-Gaussian behaviour, the median (0.5-quantile) is examined, rather than the mean. The Linear Quantile Estimator (Algorithm A) was constructed through minimising the pinball loss \eqref{eq:pinball_loss}, which, for a linear model, can be formulated as a linear programming problem with efficient solutions.
	
%	
%	The pursuit of conditional quantiles quickly divided the field into so-called direct methods, that attempt to directly compute the quantile from the training data through minimisation of the pinball loss, and Pre-Image methods, that attempt to first model the conditional cumulative distribution, and from there derive quantile estimates \citep{koenker2005quantile}.
%	
%	The direct initially demonstrated the advantage of flexibility --- non-parametric function estimation techniques could be applied that made no assumptions about the underlying distribution. Examples include locally-constant and locally-linear approximations \citep{Chaudhuri1991, Yu1998}. However, this flexibility also led to problems such as quantile crossing; in which a data point might be considered to be below the 0.5 quantile but above the 0.6 quantile \citep{koenker2005quantile}. 
%	
%	An important development was the addition of non-crossing constraints to direct quantile regression solutions \citep{He1997}. This ensured that different quantile estimates obeyed a strict ordering, with no two quantiles having the same function value. Such a strict ordering was achieved by adding a penalty term to the optimisation \citep{Cole1992}, or by reducing the class of possible functions used to represent the quantiles, for instance to classes of location-scale models \citep{Koenker1984, He1997}. Unfortunately, such models implicitly restricted the underlying distribution, causing reduced applicability to data that did not fit the assumptions of the model \citep{koenker2005quantile}.
%	
	\cite{takeuchi2006nonparametric} then examined Quantile SVM (Algorithm B), a conditional quantile estimation technique using support vector machines (SVM), which finds the decision boundary associated with the minimisation of the pinball loss and non-crossing constraints. However, such constraints may cause the resulting estimates to violate \cref{def:quantile} of a quantile.
%	
%	More recently, Chernozhukov et. al. developed a monotisation procedure based on function re-arrangement to remove crossing from quantile estimates generated by other algorithms. The resulting quantiles were guaranteed to be more accurate, and no assumptions were made about the underlying distribution \citep{Chernozhukov2010}.
%	

	Other direct approaches include the `quanting' algorithm \citep{langford2012predicting}, or Reduction to Classification (Algorithm C), which re-casts the quantile regression problem as a classification problem. By placing a set of classifiers over the range of the regression, each classifier can be trained on whether the quantile is above or below it. The quantile estimate is then the expectation of this assignment, over all the classifiers.

	Pre-Image models for conditional quantile regression have also been examined in the semi-parametric and non-parametric setting. Linear models for Bayesian quantile estimation were examined in \cite{yu2001bayesian}, using Laplacian likelihood functions and uniform priors. These methods require Markov chain Monte Carlo (MCMC) integration to perform inference, as well as making the uni-modal assumption. Similar semi-parametric methods such as \cite{hjort2007nonparametric, hjort2009quantile} use Dirichlet process priors, which again required costly inference approximations.
	
	In the non-parametric setting, \cite{quadrianto2009kernel} examined kernel conditional Quantile GP (Algorithm D) using Gaussian process regression (GPR) \citep{rasmussen2006gaussian} to explicitly compute the resulting Gaussian CDF, for which analytical forms exist. The Gaussian structure enforces non-crossing constraints and allows for efficient inference. Additionally, heteroskedastic covariance functions were employed to account for input-dependent noise in the data, leading to Heteroskedastic Quantile GP (Algorithm E). However, while non-parametric and Bayesian, the Gaussian structure restricts applicability to symmetric, uni-modal phenomenon.
	
	\cite{boukouvalas2012gaussian} then extends this GPR framework by instead minimising the expected pinball loss \eqref{eq:pinball_loss}, so that the technique is specifically optimised for obtaining quantiles. The resulting learning algorithm then loses its analytical tractability, and is computed by expectation propagation instead. With a similar goal, \cite{abeywardana2015variational} instead replaces the Gaussian likelihood involved in GPR with a scaled exponentiation of the pinball loss \eqref{eq:pinball_loss}, and uses variational inference to approximate the resulting intractable posterior, leading to a powerful and flixible quantile regression framework. However, while asymmetric, the likelihood is still uni-modal, again limiting its capability to model multi-modal phenomenon.
	 
	A more general formulation of Bayesian quantile regression was developed in \cite{taddy2012bayesian}, which used Dirichlet process mixture models to represent the underlying joint distribution. This allowed a very general class of distributions to be represented, but inference required expensive MCMC integration.
	
	To overcome the limitations of these methods, we apply kernel embedding techniques to represent general probability distributions non-parametrically and directly through the data. Without limiting the resulting distribution to any particular structure, quantile regression with kernel embeddings can model a variety of phenomenon which exhibits various structural forms.
	
% * <lachlan@mccalman.info> 2016-10-06T00:09:48.133Z:
% 
% Any luck finding a few recent citations for the related work section? Maybe check who has cited some of these papers cerently. Particalrly quadrianto2009kernel
% Kelvin: Yes! I should have included abeywardana2015variational, which is also one of Fabio's papers. I will do that now.
% Done.
% 
% ^.
	

	
\section{Experiments}
\label{sec:experiments}
		
	We now test the performance of our quantile estimators on a number of standard machine learning datasets. We compare our algorithms to state-of-the-art Pre-Image and direct techniques for conditional quantile estimation from the literature (Algorithms A, B, C, D, and E). % These include Linear Quantile Estimation (A) \citep{koenker1978regression}, Quantile SVM (B) \citep{takeuchi2006nonparametric}, Reduction to Classification (C) \citep{langford2012predicting}, Quantile GP (D) \citep{quadrianto2009kernel}, and Heteroskedastic Quantile GP (E) \citep{quadrianto2009kernel}.
	
	We evaluated the algorithms on four datasets commonly used in the literature for conditional quantile estimation tests: Antigen, Weather, Bone Mineral Density, and Motorcycle. Antigen, Weather and Motorcycle were taken from the UCI repository \citep{lichman2013uci} and Bone Mass Density from the ``ElemStatLearn'' R package \citep{hastie2005the}.
	
	For all experiments, performance was evaluated by five-fold cross validation and the average pinball loss over the hold-out set. The average and standard deviation of the performance over these five tests was used as the final score. Three quantiles were tested for each dataset; 0.1, 0.5 and 0.9. Any categorical variables in the data were ignored. All input and output variables were scaled to have zero mean and unit variance in each dimension.
	
	We place a Gaussian kernel for both the input variable $y$ and the output variable $x$. For the input variable $y$, we whiten the data to decorrelate each dimension using a form of automatic relevance determination \citep{rasmussen2006gaussian} such that an anisotropic but diagonal Gaussian kernel can be used. All our algorithms were trained with (nested) five-fold cross-validation.
% * <lachlan@mccalman.info> 2016-10-06T00:13:07.395Z:
%
% These were standard datasets when I did this work, hopefully someone has done something recently we can add here? 
%
% ^.
	\subsection{Results}
	\label{sec:experiments:results}
		
% * <lachlan@mccalman.info> 2016-10-06T00:12:22.660Z:
%
% I don't know what fabio would think about this, but could we put the full results table in the sup material? 
% Done.
		
		The average pinball loss for each algorithm in the three experiments is given in \cref{fig:qfull}. Overall, Pre-Image algorithms performed on par or better than algorithms from the literature. The two best performing algorithms were NB and JB, which are the normed-weights and \qpi\space methods, trained under the pinball loss. % It is unsurprising that pinball learning performed better than marginal likelihood learning, given that the training was optimising the same function that would evaluate the algorithm's performance.

		This demonstrates a trade-off between using pinball learning and receiving slightly better performance, or marginal likelihood learning (NL and JL) and ensuring the non-crossing constraint holds. The NL and JL algorithms were competitive, but kernel conditional Quantile GP (D) achieved the best performance.

		With both pinball and marginal likelihood learning, there was little difference on average between the Pre-Image estimator and the normed-weights estimator. This fact is worth noting when considering a real-time application of these algorithms, as the Pre-Image estimator has a much greater computational cost.
		
		Over the three experiments, we see that extremal quantiles 0.1 and 0.9 have significantly lower pinball loss compared to the median (0.5). There appear to be no algorithm that is especially capable at a particular quantile, although two direct methods (DR and DS) are much more competitive at the extremes than the median. Intuitively, an accurate estimate of the median is affected by nearby observations both above and below it. However, direct methods only sum contributions from points below the median, while pre-image methods considers all mixture components. Far away from the median, there are fewer points nearby, reducing this effect on the direct methods.
		
%		The likely explanation is that, in general, an accurate (empirical) estimate of the median is affected by nearby points both above and below it. These direct methods however only sum contributions from points lying below the median. The pre-image methods on the other hand, sum contributions from mixture components centred at all training points. Far away from the median, there are fewer points nearby and so the effect is reduced.
		
		\begin{figure}[t]
			\begin{center}
				\includegraphics[width=\columnwidth]{figures/mcquantilesall}\\
			\end{center}
			\caption{Examples of 0.1 (red), 0.5 (green) and 0.9 (blue) quantile estimates for the Motorcycle dataset, for the DR, JB, and NL algorithms.}
			\label{fig:motorcycleresults} 
		\end{figure}
		
		Figure \ref{fig:motorcycleresults} plots the quantiles estimated for the Motorcycle dataset for three of the algorithms. The piecewise-constant quantile estimates are clearly visible in the DR plot. The JB plot has each quantile trained separately, and as a result the 0.1 and 0.9 quantile both fit the boundary of the data very closely. The NL plot shows a much smoother representation, as these are quantiles for a single underlying cumulative distribution (trained with the marginal likelihood learning).

\section{Conclusion}
\label{sec:conclusion}
	
	This paper presented a set of novel algorithms for estimating quantiles non-parametrically from kernel embeddings of probability distributions given sampled observations. By projecting the indicator function into the RKHS, we first present a direct approach where the empirical CDF can be computed directly, but lack the the desired smoothness and non-decreasing property. We then impose smoothness by instead projecting a smooth function within the full RKHS that becomes the projected indicator function in the limit of vanishing bandwidth. At a trade off with computational complexity, the non-decreasing constraint is guaranteed by a Pre-Image approach, where the effective weights is to be recovered from a quadratic program.
	
	We then construct quantile estimators from these CDF estimators using simple numerical root-finding, and considered training on both the pinball loss and the negative log marginal likelihood. We then evaluated the resulting algorithms using a set of well-known machine learning datasets.
	
	Our Pre-Image quantile estimators in particular were comparable to, or better than, the state-of-the-art algorithms in the literature. These algorithms can be trained either using pinball loss, for a slight increase in performance, or with negative log marginal likelihood, to guarantee the non-crossing constraint. Additionally, all our algorithms explicitly represent multi-modal distributions, and can model datasets with strong heteroskedasticty.
	
	An interesting direction for future work would include the investigation of a Bayesian learning framework, where the latent phenomenon can be represented as a kernel embedding, and the learning objective is evaluated over the whole training set at once, similar to learning in the GPR framework.

\section*{Acknowledgements}

	This work is supported by anonymous organisations.

\bibliography{kernel_embeddings}

\end{document}
